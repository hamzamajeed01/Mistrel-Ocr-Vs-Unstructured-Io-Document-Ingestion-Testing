# Deep ReLU Neural Network Expression Rates for Data-to-QoI Maps in Bayesian PDE Inversion 

L. Herrmann and Ch. Schwab and J. Zech

Research Report No. 2020-02
January 2020

Seminar für Angewandte Mathematik
Eidgenössische Technische Hochschule
CH-8092 Zürich
Switzerland

# Deep ReLU Neural Network Expression Rates for Data-to-QoI Maps in Bayesian PDE Inversion* 

Lukas Herrmann ${ }^{\dagger}$, Christoph Schwab ${ }^{\dagger}$, and Jakob Zech ${ }^{\ddagger}$<br>${ }^{\dagger}$ Seminar for Applied Mathematics, ETH Zürich, Rämistrasse 101, CH-8092 Zürich, Switzerland. lukas.herrmann@sss.math.ethz.ch, christoph.schwab@sssmath.ethz.ch<br>${ }^{\ddagger}$ Department of Aeronautics and Astronautics, MIT, 02139 Cambridge, MA, USA. jzech@mit.edu

January 9, 2020


#### Abstract

For Bayesian inverse problems with input-to-response maps given by well-posed partial differential equations (PDEs) and subject to uncertain parametric or function space input, we establish (under rather weak conditions on the "forward", input-to-response maps) the parametric holomorphy of the data-to-QoI map relating observation data $\delta$ to the Bayesian estimate for an unknown quantity of interest (QoI). We prove exponential expression rate bounds for this data-to-QoI map by deep neural networks with rectified linear unit (ReLU) activation function, which are uniform with respect to the data $\delta$ taking values in a compact subset of $\mathbb{R}^{K}$. Similar convergence rates are verified for polynomial and rational approximations of the data-to-QoI map.


Key words: Deep ReLU neural networks, Bayesian inverse problems, approximation rates, exponential convergence, Uncertainty Quantification
Subject Classification: 41A25, 41A10, 41A46

## 1 Introduction

In recent years, computational Bayesian inversion of partial differential equations (PDEs) subject to uncertain inputs from function spaces ("distributed random inputs"), subject to various function space prior probability measures has received considerable attention. We refer for example to $[21,5,6]$ and to the references there. The currently most widely used computational method for numerical Bayesian inversion with assimilation of noisy observation data is the Markov Chain Monte Carlo (MCMC) algorithm, and its variants (e.g. [15, 14]). In practice, it is obstructed by the low Monte Carlo (MC) convergence rate (at most $1 / 2$ in terms of the number of MCMC proposals) and the need to numerically solve a forward PDE problem of each MCMC proposal, or also by a possibly extended burn-in phase of MCMC to reach asymptotic convergence.

These arguments remain valid, in part, also for multilevel variants of MCMC, see e.g. [15, 14] and the references there. Therefore, in recent years, alternative numerical methods have been

[^0]
[^0]:    *JZ is supported by the Swiss National Science Foundation under Early Postdoc.Mobility Fellowship 184530. CS acknowledges stimulating discussions at the RICAM WS on Optimization under uncertainty in November 2019 at RICAM, Linz, Austria, and at the WIAS WS on Deep Learning for PDEs at the Weierstrass Institute Berlin, Germany, 2-6 December 2019.

proposed which offer the possibility to circumvent the burn-in phase, and which afford potentially higher convergence rates than $1 / 2$; see, e.g. $[29,8,7,12]$ and the references there. Parametrization of the function space of uncertain PDE inputs, for example by means of a (Riesz- or Schauder) basis, and constructing a prior measure on the corresponding coordinate domain converts the Bayesian inverse problem (BIP) for the forward PDE with uncertain function space input into a parametric PDE inverse problem on high- or even infinite-dimensional parameter spaces, rendering the BIP amenable to deterministic numerical methods. The highdimensionality of the parameter spaces obstructs the use of standard numerical methods and has, classically, been addressed computationally by adopting MC-based numerical methods, such as MCMC and its variants, for the numerical solution of PDE BIPs.

# 1.1 Previous Work 

In recent years, efficient deterministic numerical methods capable of overcoming the mentioned curse of dimensionality in Bayesian PDE inversion and of providing higher (dimensionindependent) convergence rates than the rate $1 / 2$ afforded by MC-based methods have been developed. We mention in particular Quasi-Monte Carlo (QMC) (see, e.g., [8, 7]), and Sparsegrid, resp. (adaptive) Smolyak-type numerical integration schemes, see, e.g., [29, 9, 37] and the references there for an analysis of these methods in the presently considered forward and Bayesian inverse uncertainty quantification. The mentioned numerical methods do retain their significance in the context of training algorithms for deep neural network surrogates (DNNs) for data-to-QoI maps which are numerically approximated by "standard" schemes such as MCMC methods (see, e.g., [15] and the references there) as we will analyze in [11].

### 1.2 Contributions

In the present paper, we show that the data-to-QoI map which results from Bayesian inversion of a (well-posed) PDE with uncertain input data from function spaces and subject to additive, centered Gaussian observation noise can be expressed by deep neural networks (DNNs) with rectified linear unit (ReLU) activation function, and certain other, multivariate approximation methods, with exponential rate which is independent of the number of coordinates in the parametrization of the uncertain input from function spaces. These mathematical results are based on the strong, regularizing effect of the Gaussian weight in the high-dimensional integration in Bayesian posterior expectation. Due to Bayes' theorem, the appearance of the Gaussian in the Bayesian posterior expectation is a consequence of the (assumed) centered, Gaussian law of the observation noise in the data. As we show here, the strong smoothing property of a convolution with a Gaussian (or, equivalently, under a heat-flow) will imply exponential expression rates of the corresponding data-to-QoI maps in (Bayesian) inverse UQ. Importantly, this is valid under rather weak assumptions on the parameter-to-response map in forward UQ. Similar smoothing effects have, earlier, been identified by some of us to facilitate high approximation rates for statistical moments of in general discontinuous solution of nonlinear conservation laws [30]. As a "byproduct" of the present expression rate analysis, we also obtain quantitative bounds on the expression of the data-to-QoI maps by multivariate polynomial and rational surrogate maps in Section 5.2. These approximation rate bounds are of independent interest, as they also justify other approximations (different from the presently considered, DNN-based constructions of surrogates, such as tensor-structured surrogates) of these maps. The approximation of the map $x \mapsto 1 / x$ by ReLU NNs analyzed in Appendix C (needed in our analysis) could also be of independent interest.

The proven expression rate bounds by rational models will imply generalization error bounds in either the worst-case or in the mean-square sense. In "learning" data-to-QoI maps, there arises the practically significant question of how the DNN (or the mentioned alternative architectures) should be "trained". I.e., calibrated on a set of (possibly synthetic) "training data"

and observables of varying levels of fidelity.
Our analysis will yield, in particular, guidelines for in a sense minimal sets of synthetic training data which are sufficient for the calibration of the (polynomial, tensor-structured, or deep ReLU NN) surrogates. In the task of Bayesian PDE inversion considered here, "exact" Bayesian expectations for training the surrogate architectures are usually not available. As mentioned, reference values for surrogate training are rather assumed to be furnished by a numerical algorithm for Bayesian PDE inversion which, being based on PDE discretization and approximate posterior sampling, incurs modeling and discretization errors. Typically, then, several levels of accuracy (or "fidelity") of the reference values are accessible numerically, at corresponding cost. These extensions will be developed in [11].

# 1.3 Outline 

The outline of the present paper is as follows. In Section 2 we present the general setting for the presently considered class of Bayesian PDE constrained inverse problems. We recapitulate abstract results from [6] and from the references there to delineate sufficient conditions for its well-posedness. We distinguish uncertain inputs from finite and from infinite-dimensional spaces. In Section 3, we present examples of Bayesian inverse problems for two exemplary PDEs (elliptic, with level-set models for uncertain coefficient interfaces, and nonlinear hyperbolic PDEs with uncertain flux functions) which we show to fit into the abstract setting. The holomorphy of the mappings relating observation data $\delta$ to the Bayesian posterior expectation and to the normalization constant is shown, for nondegenerate Gaussian observation noise, in Section 4.2. In Section 5.1 we introduce the DNNs considered in the ensuing expressive power bounds. Section 5.2 discusses polynomial and rational approximation of data-to-QoI maps, and Section 5.3 contains the statement and the proof of our main result: exponential expression rate bounds for deep ReLU NNs for the data-to-QoI maps in Bayesian inverse UQ for partial differential equations. Section 6 contains some conclusions and straightforward generalizations of the present results, in particular an exponential expression rate bound for the finite-dimensional setting in Section 2.1 and additive noise distributed according to a Lipschitz density $\rho$ with respect to Lebesgue measure. In Appendix C, Lemma C.1, we prove a novel bound for the error of expressing the map $\left[x \mapsto \frac{1}{x}\right]$ by ReLU DNNs.

In [11], we will address bounds on the DNN generalization error for observables in forward UQ and for unobservable quantities of interest in Bayesian inverse UQ constrained by forward PDE models with uncertain inputs from function spaces. There, we also furnish an error analysis of DNN training based on generic, randomized "coaching" routines for Bayesian inversion, such as the mentioned multilevel MCMC algorithms (e.g., [15]) or QMC integration with randomly shifted lattice rules (e.g. [10]).

### 1.4 Notation

We adopt standard notation. Let $|\cdot|$ denote the Euclidean norm on $\mathbb{R}^{K}, K \in \mathbb{N}$. For $r>0$ we denote by $B_{r}(0)$ the closed ball with radius $r$ in either $\mathbb{R}^{K}$ (with respect to the Euclidean norm) or in a Banach space $X$ (which case is meant shall be clear from the context). By $\pi_{0}$ we shall generically denote a prior probability measure on a (assumed polish) space of uncertain PDE inputs $u$. Observation data will be denoted by the symbol $\delta$ and is assumed to take values in $\mathbb{R}^{K}$ for some finite value of $K$. The symbols $Z, Z^{\prime}$ shall denote certain quantities in the Bayesian estimate and, with various sub- and superscripts, bounds on these.

## 2 Bayesian Inverse Problems

In order to develop the holomorphic dependence of the Bayesian estimate on the observation data (vector) in some generality, we present an abstract setting of BIP, accommodating in

particular forward problems given by PDEs with random field ("distributed") uncertain input from function spaces. A reference on the mathematical setting and the well-posedness can be [6] and the references therein. We briefly recapitulate its mathematical setting, as our subsequent analysis will be based on its properties. For ease of presentation we first address the finitedimensional case, before generalizing to infinite dimensions, as required by PDE constrained Bayesian inversion with uncertain function space inputs.

# 2.1 Finite-Dimensional Case 

We wish to infer uncertain input data $u \in \mathbb{R}^{n}$ from noisy observation data $\delta \in \mathbb{R}^{K}$. We assume that the noiseless response $\delta$ is related to the uncertain input $u$ by a data-to-observable map $\delta=G(u)$. Assuming $\delta$ is only accessible up to additive, centered observation noise denoted by a mean-zero random variable $\eta \sim \mathbb{Q}_{0}$, we postulate

$$
\delta=G(u)+\eta, \quad \eta \sim \mathbb{Q}_{0}
$$

We model uncertainty in the input by furthermore assuming that $u \in \mathbb{R}^{n}$ is a random variable (RV) whose law admits a Borel measurable prior density $\rho_{0}$ w.r. to the Lebesgue measure $\lambda^{n}$, i.e.,

$$
u \sim \pi_{0}:=\rho_{0}(u) \lambda^{n}
$$

We assume in (1) that the forward map $\left[G: \mathbb{R}^{n} \rightarrow \mathbb{R}^{K}: u \mapsto \delta\right]$ is Borel measurable and that the observation noise $\eta \sim \mathbb{Q}_{0}$ is independent of the uncertain input $u$. In the case that the law $\mathbb{Q}_{0}$ of $\eta$ admits a density $\rho$ w.r. to $\lambda^{K}$, the pair $(u, \delta) \in \mathbb{R}^{n} \times \mathbb{R}^{K}$ is a RV with product density $\rho(\delta-G(u)) \rho_{0}(u)$. The distribution of the RV $u \mid \delta$ (read " $u$ given observation data $\delta$ ") is then given by the following result.

Theorem 2.1 (Bayes' Theorem) Assume that the data $\delta \in \mathbb{R}^{K}$ is such that

$$
Z=Z(\delta):=\int_{\mathbb{R}^{n}} \rho(\delta-G(u)) \rho_{0}(u) d u>0
$$

Then, $u \mid \delta$ is a $R V$ on $\mathbb{R}^{n}$ distributed according to the posterior $\pi^{\delta}$. The posterior $\pi^{\delta}$ admits the density

$$
\rho^{\delta}(u)=\frac{1}{Z} \rho(\delta-G(u)) \rho_{0}(u), \quad u \in \mathbb{R}^{n}
$$

with respect to the Lebesgue measure $\lambda^{n}$ on $\mathbb{R}^{n}$.
The expression $(u, \delta) \mapsto \rho(\delta-G(u))$ in (3), (4) is also referred to as the likelihood. The negative log-likelihood, denoted by $\Phi$, will be referred to as Bayesian potential, i.e.

$$
\Phi(u ; \delta):=-\log \rho(\delta-G(u))
$$

Denoting for $\delta \in \mathbb{R}^{K}$ the measure with density $\rho^{\delta}$ in (4) as $\pi^{\delta}$, we may write (3), (4) as

$$
\frac{d \pi^{\delta}}{d \pi_{0}}=\frac{1}{Z} \exp (-\Phi(u ; \delta)), \quad Z:=\int_{u \in \mathbb{R}^{n}} \exp (-\Phi(u ; \delta)) \pi_{0}(d u)
$$

Remark 2.2 The assumptions imply that the map $[\delta \mapsto Z(\delta)]$ is a probability density: the $R V$ $(\delta, u)$ admits the joint density $\rho(\delta-G(u)) \rho_{0}(u)$. The marginal density $[\delta \mapsto Z(\delta)]$ is given by $Z(\delta)=\int \rho(\delta-G(u)) \rho_{0}(u) \mathrm{d} u$.

Example 2.3 (Gaussian observation noise) Assume that in (1), the observation noise $\eta$ is centered, nondegenerate Gaussian observation noise on $\mathbb{R}^{K}$. Then $\mathbb{Q}_{0} \sim N(0, \Sigma)$ with symmetric, positive definite covariance matrix $\Sigma \in \mathbb{R}_{\text {sym }}^{K \times K}$, so that

$$
\rho:\left\{\begin{array}{l}
\mathbb{R}^{K} \rightarrow \mathbb{R} \\
\zeta \mapsto(2 \pi)^{-K / 2} \operatorname{det}(\Sigma)^{-1 / 2} \exp \left(-\zeta^{\top} \Sigma^{-1} \zeta / 2\right)
\end{array}\right.
$$

whence

$$
\Phi(u ; \delta)=-\log \rho(\delta-G(u))=\frac{1}{2}(\delta-G(u))^{\top} \Sigma^{-1}(\delta-G(u))+\frac{1}{2} \log \left((2 \pi)^{K} \operatorname{det}(\Sigma)\right)
$$

i.e. the Bayesian potential is the negative log-likelihood, respectively the (observation noise) covariance-weighted data-to-prediction misfit functional.

For any given, measurable QoI $\phi: \mathbb{R}^{n} \rightarrow \mathbb{R}$, the expected value under the posterior, given the data $\delta \in \mathbb{R}^{K}$, is

$$
\mathbb{E}^{\pi^{2}}[\phi]=\mathbb{E}^{\pi_{0}}\left[\frac{d \pi^{\delta}}{d \pi_{0}} \phi\right]=\frac{1}{Z} \int_{u \in \mathbb{R}^{n}} \exp (-\Phi(u ; \delta)) \phi(u) \pi_{0}(d u)
$$

# 2.2 Infinite-dimensional Case 

We denote by $X$ and $Y$ real, separable Banach spaces, equipped with the Borel sigma-algebra. In the finite-dimensional setting of the preceding section, $X=\mathbb{R}^{n}$ denotes the space of uncertain inputs and $Y=\mathbb{R}^{K}$ denotes the data space. Here, we retain $Y=\mathbb{R}^{K}$ finite-dimensional, but admit $X$ to be a real, separable Banach space corresponding to uncertain function space input for PDEs.

The forward ("input-to-observation") map will again be denoted by $G: X \rightarrow Y$. We assume $G$ be measurable and consider again the BIP: given noisy observation data $\delta \in Y$, find $u \in X$ such that

$$
\delta=G(u)+\eta
$$

Here, $\eta$ denotes a $Y$-valued RV which describes additive observation noise on the data $\delta$. Assumption (9) renders $(u, \delta) \in X \times Y$ a RV with respect to the product sigma algebra.

We are interested in the law of $u \mid \delta$. To calculate it, we place a (Bayesian) prior probability measure $\pi_{0}$ on $(X, \mathcal{B}(X))$, and a probability measure $\mathbb{Q}_{0}$ on $(Y, \mathcal{B}(Y))$ corresponding to the distribution of $\eta$. We assume $\mathbb{Q}_{0}$ to be centered, and the RVs $u$ and $\eta$ to be independent. Then, the product probability measure $\nu_{0}=\pi_{0} \otimes \mathbb{Q}_{0}$ is well-defined. To derive the law of $(u, \delta) \in X \times Y$, we observe that given $u \in X, \delta \mid u$ is a RV taking values in $Y$ with law $\mathbb{Q}_{u}$ being $\mathbb{Q}_{0}$ translated by $G(u)$. We assume

$$
\mathbb{Q}_{u} \ll \mathbb{Q}_{0} \quad \pi_{0} \text {-a.e. } u \in X
$$

This assumption implies that for $\pi_{0}$-a.e. $u \in X$ the nonnegative Radon-Nikodym derivative $\frac{d \mathbb{Q}_{u}}{d \mathbb{Q}_{0}}$ exists and we denote it by $\exp (-\Phi(u ; \delta))$ with the log-likelihood $-\Phi(\cdot ; \delta): X \rightarrow \mathbb{R}$, i.e.

$$
\frac{d \mathbb{Q}_{u}}{d \mathbb{Q}_{0}}=\exp (-\Phi(u ; \delta))
$$

Then, for $\pi_{0}$-a.e. $u \in X, \Phi(u, \cdot): Y \rightarrow \mathbb{R}$ is measurable with $\mathbb{E}^{\mathbb{Q}_{0}}[\exp (-\Phi(u ; \cdot))]=1$. Furthermore, the law of the $\operatorname{RV}(u, \delta)$ is $\nu=\pi_{0} \otimes \mathbb{Q}_{u}$ and $\nu \ll \nu_{0}$ with

$$
\frac{d \nu}{d \nu_{0}}=\exp (-\Phi(u ; \delta))
$$

Theorem 2.4 Assume that $\Phi: X \times Y \rightarrow \mathbb{R}$ is $\nu_{0}$-measurable with

$$
Z:=\int_{X} \exp (-\Phi(u ; \delta)) \pi_{0}(d u)>0 \quad \mathbb{Q}_{0} \text {-a.e. } \delta \in Y
$$

Then the law of $u \mid \delta$, denoted as $\pi^{\delta}$, exists for $\mathbb{Q}_{0}$-a.e. $\delta$, and $\pi^{\delta} \ll \pi_{0}$. Moreover, for $\nu$-a.e. $(u, \delta)$ holds

$$
\frac{d \pi^{\delta}}{d \pi_{0}}=\frac{1}{Z} \exp (-\Phi(u ; \delta))
$$

We refer to $[6$, Theorem 3.4] for a proof.

# 3 Examples 

Throughout this section we use several times the following: if the forward solution operator $G$ : $X^{\prime} \rightarrow \mathbb{R}^{K}$ is continuous, then $G$ is Borel-Borel measurable and thus the map $(u, \delta) \mapsto \rho(\delta-G(u))$ is Borel-Borel measurable provided the density $\rho: \mathbb{R}^{K} \rightarrow \mathbb{R}$ is Borel-Borel measurable. Since $\rho$ is a Lebesgue density, $\rho$ is in general only Lebesgue-Borel measurable. However, in all examples considered below, $\rho$ is actually continuous and thus also Borel-Borel measurable.

With this prerequisite in mind, the preceding, abstract setting accommodates a wide range of Bayesian inverse problems. Before addressing DNN expression rate bounds, we illustrate in this section the scope of the present setting by verifying the above, general assumptions for a selection of parametric PDE problems with uncertain PDE inputs from (subsets $X^{\prime}$ of) function spaces $X$. The prior probability measure $\pi_{0}$ in (10) will, in this case, be a pushforward of a probability measure $\mathbb{P}$ on a measurable space $(\Omega, \mathcal{F})$, to a separable subset $X^{\prime}$ of a Banach space $X$ of admissible inputs for the PDE under consideration. Specifically, we suppose that $u: \Omega \rightarrow$ $X$ is strongly measurable. This implies by Pettis' theorem that there exists a measurable subset $\Omega^{\prime} \subset \Omega$ such that $\mathbb{P}\left(\Omega^{\prime}\right)=1$ and $\left\{u(\omega): \omega \in \Omega^{\prime}\right\}$ is separable in $X$, cf. [36, Theorem V.4]. We refer to [5, Section 2] for a detailed derivation of such priors $\pi_{0}$ for linear, well-posed elliptic PDEs with uncertain coefficients. Rather than covering the most general case, we opt for developing two PDE models and also discuss examples of priors, which we construct as the law of a strongly measurable random field $u$. More PDE problems are admissible in our framework, for example the problem to recover the unknown conductivity from noisy boundary measurements in Calderón problems, see [1] and the references therein.

### 3.1 PDE models

We will consider forward data-to-solution maps which are realized through the solution of a governing PDE for uncertain function space input. Generally, the uncertain function space input is denoted by $u \in X$, which should be constrained such that the PDE under consideration is well-posed for this input data. For that reason, we may restrict the function space $X$ to a subset $X^{\prime}$. The unique solution given input $u$ is denoted by $q \in V$ and the forward solution map is denoted by $\mathcal{S}$, i.e., $u \mapsto q=\mathcal{S}(u) \in V$, where $V$ is a Banach space.

In numerical Bayesian inversion of a PDE, we aim at computing a conditional expectation of a Quantity of Interest ("Qol" for short) $\phi \in V^{*}$, which is here assumed to be a linear functional. To this end, we assume at hand (noisy) observations $\mathcal{O} \circ \mathcal{S}(u)+\eta$ where $\mathcal{O} \in\left(V^{*}\right)^{K}$, and, as before, $\eta \in \mathbb{R}^{K}$ is a RV on $\mathbb{R}^{K}$ whose law admits a Borel measurable density $\rho$. In this case the input-to-observation map $G$ in (9) is given by $G=\mathcal{O} \circ \mathcal{S}$. We assume that the prior $\pi_{0}$ is the law of a random field $u: \Omega \rightarrow X^{\prime}$, which is strongly measurable with respect to the topology of $X$. Moreover, we assume that the forward solution operator $\mathcal{S}: X^{\prime} \rightarrow V$ is continuous. Then, as a composition of two continuous maps the data-to-observation map $\mathcal{O} \circ \mathcal{S}: X^{\prime} \rightarrow Y$ is also continuous. The strong measurability of $u: \Omega \rightarrow X^{\prime}$ implies that the

observable $[G: \Omega \rightarrow Y: u \mapsto(\mathcal{O} \circ \mathcal{S})(u)]$ is a RV, i.e. measurable with respect to the Borel sigma algebra of $Y=\mathbb{R}^{K}$. Let us assume that

$$
\mathbb{E}^{\pi_{0}}[|\phi|]=\int_{X}|(\phi \circ \mathcal{S})(u)| \pi_{0}(\mathrm{~d} u)<\infty
$$

Then, given noisy observation data $\delta \in Y$, the posterior expectation of the QoI $\phi$ takes the form

$$
\mathbb{E}^{\pi^{\delta}}[\phi]=\frac{1}{Z} \int_{X}(\phi \circ \mathcal{S})(u) \rho(\delta-G(u)) \pi_{0}(\mathrm{~d} u)
$$

This expression is well-defined by the (assumed) measurability of the density $\rho$ with respect to the Borel sigma-algebra.

# 3.1.1 Diffusion equations 

In a bounded Lipschitz domain $D \subset \mathbb{R}^{d}$, given a (assumed uncertain) coefficient $u \in L^{\infty}(D)$ and a deterministic (i.e., deterministic assumed known with certainty) source term $f \in L^{2}(D)$, as a forward model, we are interested in finding $q \in H_{0}^{1}(D)$ such that

$$
f+\nabla \cdot(u \nabla q)=0 \quad \text { in } \quad H^{-1}(D),\left.q\right|_{\partial D}=0
$$

As is well-known, for every $u \in L^{\infty}(D)$ such that $\operatorname{ess} \inf _{x \in D} u(x)>0$, the forward problem (16) admits a unique variational solution $q \in H_{0}^{1}(D)$. Here $V=H_{0}^{1}(D)$. For fixed $f$ in (16), the input-to-solution map

$$
\mathcal{S}:\left\{u \in L^{\infty}(D): \operatorname{ess} \inf _{x \in D} u(x)>0\right\} \rightarrow V: u \mapsto q
$$

induced by (16) satisfies

$$
\|\mathcal{S}(u)\|_{V} \leq \frac{\|f\|_{V^{*}}}{\operatorname{ess} \inf _{x \in D}\{u(x)\}}
$$

The map $\mathcal{S}$ is Lipschitz continuous which implies measurability of the likelihood as follows. For any $u, u^{\prime} \in\left\{u \in L^{\infty}(D): \operatorname{ess} \inf _{x \in D} u(x)>0\right\}$ such that $\mathcal{S}(u) \in W^{1, r}(D)$ for some $r \in[2, \infty)$, there holds

$$
\left\|\mathcal{S}(u)-\mathcal{S}\left(u^{\prime}\right)\right\|_{V} \leq \frac{\|\nabla \mathcal{S}(u)\|_{L^{r}(D)}}{\operatorname{ess} \inf _{x \in D}\left\{u^{\prime}(x)\right\}}\left\|u-u^{\prime}\right\|_{L^{2 r /(r-2)}(D)}
$$

For $X^{\prime} \subset\left\{u \in L^{\infty}: \operatorname{ess} \inf _{x \in D}\{u(x)\}>0\right\}$ being (Borel) measurable, we endow $X^{\prime}$ with the $L^{\infty}(D)$-norm and suppose that $X^{\prime}$ is separable with respect to the $L^{\infty}(D)$-norm. In this case, $r^{\prime}=2 r /(r-2)=\infty$ and $r=2$. Note that $\mathcal{S}(u) \in W^{1, r}(D)$ is satisfied by (18). Thus, by (19), the forward operator $\mathcal{S}: X^{\prime} \rightarrow V$ is Lipschitz continuous. But the verification of the condition (14) becomes non-trivial and shall be discussed in the particular construction of the prior (see ahead Section 3.2.3).

Suppose there exists $C>0$ such that

$$
X^{\prime}=\left\{u \in L^{\infty}(D): C^{-1} \leq \operatorname{ess} \inf _{x \in D}\{u(x)\} \leq\|u\|_{L^{\infty}(D)} \leq C\right\}
$$

Then, there exists $r>2$ such that $\mathcal{S}(u) \in W^{1, r}(D)$ for every $u \in X^{\prime}$ if also $f \in\left(W_{0}^{1, r /(r-1)}(D)\right)^{*}$, cf. [2, Proposition 1] (the conditions of [2, Proposition 1] are verified for example by [20, Theorems 1.1 and 1.3]). Note that the earlier assumption $f \in L^{2}(D)$ implies that $f \in$ $\left(W_{0}^{1, r /(r-1)}(D)\right)^{*}$ for $d=1,2$ and for $d>2$ if $r<2 d /(d-2)$. We endow $X^{\prime}$ with the $L^{r^{\prime}}(D)$-norm for $r^{\prime}=2 r /(r-2)$. Thus, by (19), $\mathcal{S}: X^{\prime} \rightarrow V$ is Lipschitz continuous. In this case the condition (14) is always satisfied, which follows by (18).

# 3.1.2 Scalar hyperbolic conservation law 

We consider the Cauchy problem for the scalar, nonlinear hyperbolic conservation law

$$
\partial_{t} q+\partial_{x}(u(q))=0,\left.\quad q\right|_{t=0}=q_{0}
$$

The initial condition $q_{0} \in L^{1}(\mathbb{R})$ has bounded variation and is assumed known, i.e., deterministic. Denote by $M:=\left\|q_{0}\right\|_{L^{\infty}(\mathbb{R})}$ and note the maximum principle satisfied by the (unique) entropy solutions, cf. e.g. [16, Theorem 2.14(i)]. The Lipschitz continuous flux function $u \in W^{1, \infty}([-M, M])$ in (20) is assumed to be uncertain. For any fixed realization $u \in W^{1, \infty}([-M, M])$ of the flux function $u$ in (20), there exists a unique entropy solution $q$ to (20) by [16, Theorem 2.14]. For fixed $t>0$, let us denote by $\mathcal{S}_{t}$ the operator $u \mapsto q(t)$, for fixed initial data $q_{0}$, i.e., $q(t)=\mathcal{S}_{t}(u)$. The $L^{1}(\mathbb{R})$-norm of the entropy solution at time $t>0$ is bounded in terms of the data. Specifically, by [16, Theorem 2.14(vi)] and the triangle inequality

$$
\left\|\mathcal{S}_{t}(u)\right\|_{L^{1}(\mathbb{R})} \leq\left\|q_{0}\right\|_{L^{1}(\mathbb{R})}+t \operatorname{TV}\left(q_{0}\right)\left\|\partial_{x} u\right\|_{L^{\infty}([-M, M])}
$$

Furthermore, the entropy solution $q$ depends Lipschitz continuously on the flux function $u$ : by [16, Theorem 2.13], for any two Lipschitz flux functions $u, \tilde{u} \in W^{1, \infty}([-M, M])$ and for every $t>0$ holds

$$
\left\|\mathcal{S}_{t}(u)-\mathcal{S}_{t}(\tilde{u})\right\|_{L^{1}(\mathbb{R})} \leq t \operatorname{TV}\left(q_{0}\right)\left\|\partial_{x}(u-\tilde{u})\right\|_{L^{\infty}([-M, M])}
$$

Thus, for every $t>0$, the forward ("flux-to-entropy solution") map $\mathcal{S}_{t}: W^{1, \infty}([-M, M]) \rightarrow$ $L^{1}(\mathbb{R})$ is continuous. Hence, we are in the abstract setting for Bayesian inversion with $X=$ $W^{1, \infty}([-M, M])$ and $V=L^{1}(\mathbb{R})$.

### 3.2 Priors

We present several constructions of parametric function space priors, for which our results apply. These constructions are by no means meant to be exhaustive; they are merely listed, with references, in order to illustrate the scope of applicability of our principal results on DNN expression rate bounds for PDE constrained Bayesian inverse problems.

### 3.2.1 Level set priors

We shall discuss in some detail a class of uncertain $u$ such that $u$ is piecewise constant and attains known values (or levels) at uncertain locations in the spatial domain $D$, i.e.,

$$
u=\sum_{i=1}^{n} u_{i} \mathbb{1}_{D_{i}}
$$

for certain numbers $u_{i} \in(0, \infty), i=1, \ldots, n$, and uncertain subsets $D_{i}$ of $D$ such that $\bar{D}=$ $\bigcup_{i=1}^{n} \bar{D}_{i}$ and $D_{i} \cap D_{i^{\prime}}=\emptyset$ for $i \neq i^{\prime}$. Suppose that $g: \Omega \rightarrow C^{0}(\bar{D})$ is a strongly measurable Gaussian random field on an auxiliary probability space $(\Omega, \mathcal{A}, \mathbb{P})$. For constants $-\infty=c_{0}<$ $c_{1}<\ldots<c_{n}=\infty$, define the function $F: \mathbb{R} \rightarrow(0, \infty)$ by

$$
F=\sum_{i=1}^{n} u_{i} \mathbb{1}_{\left[c_{i-1}, c_{i}\right)}
$$

It follows that $u:=F(g)$ satisfies (23) with the uncertain sets $D_{i}$ defined by

$$
D_{i}:=\left\{x \in D: c_{i-1} \leq g(x)<c_{i}\right\}, \quad i=1, \ldots, n
$$

Lemma 3.1 The level set random field $u: \Omega \rightarrow L^{r}(D)$ defined by $\omega \mapsto u(\omega):=F(g(\omega))$ is strongly measurable with respect to the topology of $L^{r}(D)$ for every $r \in[1, \infty)$.

Proof. By definition of strong measurability, cf. e.g. [36, Definition V.4.1], there exist functions $g_{i}^{N} \in C^{0}(\bar{D})$ and measurable, disjoint sets $\Omega_{i}^{N} \subset \Omega$ such that $\left\|g-\sum_{i=1}^{N} g_{i}^{N} \mathbb{1}_{\Omega_{i}^{N}}\right\|_{C^{0}(\bar{D})} \rightarrow 0$ as $N \rightarrow \infty \mathbb{P}$-a.s. This is to say that $\sum_{i=1}^{N} g_{i}^{N} \mathbb{1}_{\Omega_{i}^{N}}$ is "finitely valued". We observe that $F\left(\sum_{i=1}^{N} g_{i}^{N} \mathbb{1}_{\Omega_{i}^{N}}\right)=\sum_{i=1}^{N} F\left(g_{i}^{N}\right) \mathbb{1}_{\Omega_{j}^{N}}$ is also finitely valued, since the sets $\Omega_{i}^{N}, i=1, \ldots, N$, are disjoint.

We state the fact that $v \mapsto F \circ v$ is a mapping from $C^{0}(\bar{D})$ to $L^{r}(D)$ for every $r \in[1, \infty)$. This map is continuous at some $v \in C^{0}(\bar{D})$ if and only if $\left\{x \in D: v(x)=c_{i}\right\}$ is a nullset with respect to the Lebesgue measure for every $i=2, \ldots, n-1$, cf. e.g. [19, Proposition 2.6].

By [19, Proposition 7.2], $\mathbb{P}\left(\left\{\omega \in \Omega:\left|\left\{x \in D: g(\omega)(x)=c_{i}\right\}\right|=0\right\}\right)=1, i=1, \ldots, n-1$. This implies with the aforementioned fact that $\|F(g)-\sum_{i=1}^{N} F\left(g_{i}^{N}\right) \mathbb{1}_{\Omega_{i}^{N}}\left\|_{L^{r}(D)} \rightarrow 0\right.$ as $N \rightarrow \infty$ $\mathbb{P}$-a.s. Thus, the composition $F \circ g$ is strongly measurable with respect to $L^{r}(D)$.

# Diffusion equations 

In the setting of Section 3.1.1 we let $X:=L^{\infty}(D), X^{\prime}:=\left\{u \in L^{\infty}(D): \min \left\{u_{1}, \ldots, u_{n}\right\} \leq\right.$ $u(x) \leq \max \left\{u_{1}, \ldots, u_{n}\right\}$ a.e. $\left.x \in D\right\}$ and $V:=H_{0}^{1}(D)$. There exists $r^{\prime} \in[2, \infty)$ in dependence of $\min \left\{u_{1}, \ldots, u_{n}\right\}$ and $\max \left\{u_{1}, \ldots, u_{n}\right\}$ such that $\mathcal{S}: X^{\prime} \rightarrow V$ is continuous, see Section 3.1.1, where we endowed $X^{\prime}$ with the $L^{r^{\prime}}(D)$-norm. By Lemma 3.1, $u: \Omega \rightarrow X^{\prime}$ is strongly measurable and we define the prior on $X^{\prime}$ as the law of $u$, i.e., $\pi_{0}(A)=\mathbb{P}\left((F \circ g)^{-1}(A)\right)$ for all Borel measurable $A \subseteq X^{\prime}$.

### 3.2.2 Affine parametric priors

For $m \in\{0,1\}$ fixed, we assume at hand a nominal coefficient $u_{0} \in W^{m, \infty}(D)$ and a countable representation system $\boldsymbol{\Psi}=\left\{\psi_{j}\right\}_{j \geq 1} \subset X=W^{m, \infty}(D)$ such that

$$
\sum_{j \geq 1}\left\|\psi_{j}\right\|_{W^{m, \infty}(D)}<\infty
$$

Then we consider the parametric coefficient $u$ given by

$$
u(x, \boldsymbol{y})=u_{0}(x)+\sum_{j \geq 1} y_{j} \psi_{j}(x), x \in D, \quad \boldsymbol{y}=\left(y_{j}\right)_{j \geq 1} \in \Omega:=[-1,1]^{\infty}
$$

By our assumptions, for every fixed $\boldsymbol{y} \in \Omega$ the infinite series (25) converges in $W^{m, \infty}(D)$. We denote by $X^{\prime} \subset X$ the set of all limits in $X$ of the parametric expansion (25), $X^{\prime}=\{u: u=$ $\left.u_{0}+\sum_{j \geq 1} y_{j} \psi_{j}\right.$ for some $\left.\boldsymbol{y} \in \Omega\right\}$.

We now construct a prior measure $\pi_{0}$ on the set $X^{\prime} \subset X$. Again, we start by introducing an auxiliary probability space $(\Omega, \mathcal{A}, \mathbb{P})$, where $\Omega=[-1,1]^{\infty}, \mathcal{A}=\otimes_{j \in \mathbb{N}} \mathcal{B}([-1,1])$ is the product Borel sigma algebra, and $\mathbb{P}:=\otimes_{j \geq 1} \lambda / 2$ is the (countable) product probability measure, where $\lambda / 2$ denotes the scaled Lebesgue measure on $[-1,1]$. The measure $\pi_{0}$ is defined as the pushforward of $\mathbb{P}$ under $u: \Omega \rightarrow X^{\prime}$ as in (25), i.e. $\pi_{0}(A)=\mathbb{P}\left(u^{-1}(A)\right)$ for all measurable $A \subseteq X^{\prime}$. The sigma-algebra on $X^{\prime} \subseteq X$ is again the Borel sigma-algebra. The set $\Omega=[-1,1]^{\infty}$ is compact when equipped with the product topology, by Tychonoff's theorem (e.g. [28, Theorem A3]). In this case the Borel-sigma algebra on $\Omega$ coincides with the product sigma-algebra $\otimes_{j \in \mathbb{N}} \mathcal{B}([-1,1])$.

## Diffusion equations

For the diffusion equation (16), we let $m=0$ so that $X=L^{\infty}(D)$ and $V=H_{0}^{1}(D)$. Furthermore, we impose $X^{\prime} \subset\left\{u \in L^{\infty}: \operatorname{ess} \inf _{x \in D}\{u(x)\}>0\right\}$ to ensure well-posedness of the

forward problem. Suppose in addition that $u_{0}$ satisfies

$$
\underset{x \in D}{\operatorname{ess}} u_{0} \geq u_{\min }>0
$$

Then

$$
\inf _{u \in X^{\prime}} \underset{x \in D}{\operatorname{ess}} u=\inf _{\boldsymbol{y} \in \Omega} \underset{x \in D}{\operatorname{ess}} u(x, \boldsymbol{y}) \geq(1-\kappa) u_{\min }>0
$$

which implies $X^{\prime} \subset\left\{u \in L^{\infty}: \operatorname{ess} \inf _{x \in D} u(x)>(1-\kappa) u_{\min }\right\}$. By (18), condition (14) is satisfied for every QoI $\phi \in V^{*}$.

# Scalar conservation laws 

For the scalar conservation law in (20), we let $m=1, X=W^{1, \infty}(D)$ and $D=[-M, M]$, where we recall that $M=\left\|q_{0}\right\|_{L^{\infty}(\mathbb{R})}$ and $q_{0}$ is the initial data. Furthermore $V=L^{1}(\mathbb{R})$. The condition (14) is satisfied as a consequence of (21) and (24) for every QoI $\phi \in V^{*}$.

### 3.2.3 Log-Besov parametric priors

Let $m \in\{0,1\}$. Consider now $u$ taking log-affine form, i.e.

$$
u(x, \boldsymbol{y})=u_{0}(x)+\exp \left(\sum_{j \geq 1} y_{j} \psi_{j}(x)\right) \quad x \in D
$$

where $u_{0} \in X=W^{m, \infty}(D)$ satisfies $\operatorname{ess} \inf _{x \in D}\left\{u_{0}(x)\right\} \geq 0$. We assume again summability of the function system $\boldsymbol{\Psi}=\left(\psi_{j}\right)_{j \geq 1}$, i.e.,

$$
\sum_{j \geq 1}\left\|\psi_{j}\right\|_{W^{m, \infty}(D)}<\infty
$$

For $p \in[1,2]$, we define $\mathbb{P}$ as the product measure

$$
\mathbb{P}(\mathrm{d} \boldsymbol{y}):=\bigotimes_{j \geq 1} \frac{p}{2 p^{1 / p} \Gamma(1 / p)} e^{-\frac{\left|y_{j}\right|^{p}}{p}} \mathrm{~d} y_{j}
$$

on $\Omega:=\mathbb{R}^{\infty}$, where $\Gamma$ denotes the Gamma function. We suppose that $\boldsymbol{y}$ in (26) is distributed according to $\mathbb{P}$, and $\Omega$ is equipped with the product Borel sigma algebra. As a consequence [10, Proposition 3.2], it holds that

$$
\underset{x \in D}{\operatorname{ess} \inf }\{u(x, \boldsymbol{y})\}>0 \quad \mathbb{P}-\text { a.e. } \boldsymbol{y} \in \Omega
$$

where for $p=1$ we require $\sup _{j \geq 1}\left\|\psi_{j}\right\|_{L^{\infty}}<1$.
Here, $X^{\prime}=\left\{u \in W^{m, \infty}(D): u=u_{0}+\exp \left(\sum_{j \geq 1} y_{j} \psi_{j}\right), \mathbb{P}-\right.$ a.e. $\left.\boldsymbol{y} \in \Omega\right\}$. By [10, Proposition 3.2], in the case $p \in(1,2]$

$$
\int_{\Omega}\|u(\boldsymbol{y})\|_{W^{m, \infty}(D)} \mathbb{P}(\mathrm{d} \boldsymbol{y})<\infty
$$

which also holds for $p=1$ if $\sup _{j \geq 1}\left\{\left\|\psi_{j}\right\|_{W^{m, \infty}(D)}\right\}<1$. Also by [10, Proposition 3.2], $u$ : $\Omega \rightarrow X^{\prime}$ is strongly measurable. We construct the Besov prior $\pi_{0}$ as the law of $u$, i.e., $\pi_{0}(A):=$ $\mathbb{P}\left(u^{-1}(A)\right)$ for every measurable $A \subset X^{\prime}$.

## Diffusion equations

For the PDE model (16), $m=0, X=L^{\infty}(D)$ and $V=H_{0}^{1}(D)$. The condition (14) is satisfied by (27) and (18) for every QoI $\phi \in V^{*}$.

# Scalar conservation laws 

For the PDE model (20), $m=1, X=W^{1, \infty}(D)$ and $D=[-M, M]$, where we recall that $M=\left\|q_{0}\right\|_{L^{\infty}(\mathbb{R})}$ and $q_{0}$ is the initial condition. Furthermore $V=L^{1}(\mathbb{R})$. The condition (14) is satisfied by (27) and (21) for every QoI $\phi \in V^{*}$.

Remark 3.2 For $p=2$ and $u_{0} \equiv 0$, we recover the widely used case of parametric log-Gaussian diffusion coefficients $u$ as a special case of the constructed log-Besov priors. Hence, BIPs with Gaussian priors are also covered by the present results.

## 4 Regularity of the Data-to-QoI Map

We now investigate the regularity of the data-to-QoI map $\delta \mapsto \mathbb{E}^{\pi^{s}}[\phi]$. This regularity is crucial, on the one hand, for the ensuing DNN expression rate analysis and, on the other hand, will be seen to determine to some extent the DNN architecture. As we show, this regularity is strongly dependent on the regularity of the density $\rho$ of the observation noise $\eta$ in the additive model $(1)$.

### 4.1 Lipschitz Regularity

We assume we are in the finite-dimensional case described in Theorem 2.1 and that the density function $\rho$ in (3), (4) is globally Lipschitz continuous.

Example 4.1 Consider the function $\rho: \mathbb{R}^{K} \rightarrow \mathbb{R}: \zeta \mapsto \exp \left(-\|\zeta\|_{1}\right)$ where, for $\zeta \in \mathbb{R}^{K}$, $\|\zeta\|_{1}=\left|\zeta_{1}\right|+\ldots+\left|\zeta_{K}\right|$. Then $\rho$ is globally Lipschitz, since for $\zeta, \zeta^{\prime} \in \mathbb{R}^{K}$ holds

$$
\left|\rho(\zeta)-\rho\left(\zeta^{\prime}\right)\right|=\left|\exp \left(-\|\zeta\|_{1}\right)-\exp \left(-\left\|\zeta^{\prime}\right\|_{1}\right)\right| \leq\left|\|\zeta\|_{1}-\left\|\zeta^{\prime}\right\|_{1}\right| \leq\left\|\zeta-\zeta^{\prime}\right\|_{1}
$$

The Lipschitz property of $\rho$ is inherited by the data-to-QoI map $\delta \mapsto \mathbb{E}^{\pi^{s}}[\phi]$ in (8).
Proposition 4.2 In the setting of Theorem 2.1, assume that $\rho \in \operatorname{Lip}\left(\mathbb{R}^{K}\right)$ with respect to some norm $\|\circ\|$ on $\mathbb{R}^{K}$. Suppose in addition that the QoI $\phi$ satisfies $\phi \in L^{1}\left(X^{\prime}, \pi_{0}\right)$.

Then, for every $r>0$ the map $\left[\delta \mapsto \mathbb{E}^{\pi^{s}}[\phi]\right] \in \operatorname{Lip}\left(B_{r}(0)\right)$, i.e, there exists a constant $C(r, \phi)>0$ such that

$$
\forall \delta, \delta^{\prime} \in B_{r}(0): \quad\left|\mathbb{E}^{\pi^{s}}[\phi]-\mathbb{E}^{\pi^{s^{\prime}}}[\phi]\right| \leq C\left\|\delta-\delta^{\prime}\right\|
$$

Proof. Let for the moment $\delta, \delta^{\prime} \in \mathbb{R}^{K}$ be arbitrary realizations of the observation data. Furthermore, denote $Z^{\prime}(\delta):=Z(\delta) \mathbb{E}^{\pi^{s}}[\phi]$ and $Z^{\prime}\left(\delta^{\prime}\right):=Z\left(\delta^{\prime}\right) \mathbb{E}^{\pi^{s^{\prime}}}[\phi]$. Then,

$$
\left|\frac{Z^{\prime}(\delta)}{Z(\delta)}-\frac{Z^{\prime}\left(\delta^{\prime}\right)}{Z\left(\delta^{\prime}\right)}\right| \leq \frac{\left|Z^{\prime}(\delta)-Z^{\prime}\left(\delta^{\prime}\right)\right|}{Z(\delta)}+\frac{\left|Z^{\prime}\left(\delta^{\prime}\right)\right|\left|Z\left(\delta^{\prime}\right)-Z(\delta)\right|}{Z(\delta) Z\left(\delta^{\prime}\right)}
$$

By assumption the density $\rho$ is globally Lipschitz on $\mathbb{R}^{K}$ with constant $C_{\text {Lip }}>0$. Thus by definition of $Z$ in (6),

$$
\left|Z(\delta)-Z\left(\delta^{\prime}\right)\right| \leq C_{\mathrm{Lip}}\left\|\delta-\delta^{\prime}\right\| \int_{X} \pi_{0}(\mathrm{~d} u)=C_{\mathrm{Lip}}\left\|\delta-\delta^{\prime}\right\|
$$

Similarly, $\left|Z^{\prime}(\delta)-Z^{\prime}\left(\delta^{\prime}\right)\right| \leq C_{\text {Lip }} \mathbb{E}^{\pi_{0}}[|\phi|]\left\|\delta-\delta^{\prime}\right\|$. For every $r>0$, define

$$
Z_{\min , r}:=\inf _{\delta^{\prime \prime} \in B_{r}(0)} Z\left(\delta^{\prime \prime}\right) \quad \text { and } \quad Z_{\max , r}^{\prime}:=\sup _{\delta^{\prime \prime} \in B_{r}(0)} Z^{\prime}\left(\delta^{\prime \prime}\right)
$$

Since we have proven already that the nonnegative mappings $[\delta \rightarrow Z(\delta)]$ and $\left[\delta \rightarrow Z^{\prime}(\delta)\right]$ are Lipschitz continuous on $\mathbb{R}^{K}$, they achieve their minimum and maximum on the compact set $B_{r}(0) \subset \mathbb{R}^{K}$ so that $0<Z_{\min , r}, Z_{\max , r}^{\prime}<\infty$. The assertion now follows with

$$
C=C_{\text {Lip }}\left(\frac{\mathbb{E}^{\pi_{0}}[|\phi|]}{Z_{\min , r}}+\frac{Z_{\max , r}^{\prime}}{Z_{\min , r}^{2}}\right)
$$

# 4.2 Holomorphy 

We now establish one core result of the present paper, namely the holomorphy of the data-to-QoI map which results from the expectation under the Bayesian posterior, for the negative log-likelihood $\Phi$ in (5) being a quadratic. This corresponds to the important assumption that the additive observation noise $\eta$ in (1) is centered, Gaussian.

As we admit vector-valued data $\delta \in \mathbb{R}^{K}$ for some integer $K \geq 1$ and as the QoI $\phi$ is assumed to be scalar, taking values in $\mathbb{R}$ (or, upon complexification, in $\mathbb{C}$ ), this amounts to verifying holomorphy of a scalar function of $K$ complex variables. Based on standard results (e.g. [17]) on functions of several complex variables, we shall verify this first in the univariate case (i.e., for $K=1$ ) and subsequently infer holomorphy of the multivariate map by means of Hartogs' theorem.

We shall use the following technical result on averages of holomorphic maps, which is assertion $C_{3}$ of the Theorem in [23].

Proposition 4.3 Let $(\Omega, \mathcal{A}, \pi)$ be a measure space and let $G \subset \mathbb{C}$ be an open set. Suppose that the functions $f: \Omega \times G \rightarrow \mathbb{C}$ satisfies
(i) $[\Omega \ni \omega \mapsto f(\omega, z)]$ is measurable with respect to $\mathcal{A}$ for every $z \in G$.
(ii) $[G \ni z \mapsto f(\omega, z)]$ is holomorphic for every $\omega \in \Omega$.
(iii) for every $z_{0} \in G$, there is $\delta>0$ such that $\sup _{z \in G,\left|z-z_{0}\right| \leq \delta} \int_{\Omega}|f(\omega, z)| \pi(\mathrm{d} \omega)<\infty$.

Then, $[G \ni z \mapsto \int_{\Omega} f(\omega, z) \pi(\mathrm{d} \omega)]$ is holomorphic.
Proof. This is assertion $C_{3}$ of the theorem in [23].

### 4.2.1 Univariate Data $(K=1)$

Lemma 4.4 In the setting of Theorem 2.4 let $K=1$ and assume that $\phi \in L^{1}\left(X^{\prime}, \pi_{0}\right)$ and $\Phi(u ; \delta)=(\delta-G(u))(\delta-G(u)) /\left(2 \sigma^{2}\right)$ for some $\sigma>0$.

Then the map

$$
\delta \mapsto \int_{X^{\prime}} \phi(u) \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u)
$$

is holomorphic on $\mathbb{C}$.
Proof. We shall verify assumptions (i), (ii) and (iii) in Proposition 4.3. Since $u \mapsto \Phi(u ; \delta)$ is measurable for every $\delta \in \mathbb{C}$ and $\delta \mapsto \Phi(u ; \delta)$ is holomorphic for $\pi_{0}$-a.e. $u \in X^{\prime}$, the same is true for the map $(u, \delta) \mapsto \exp (-\Phi(u ; \delta))$. This completes the verification of assumptions (i) and (ii) in Proposition 4.3.

It remains to show that $\int_{X^{\prime}} \phi(u) \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u)$ is locally bounded for every $\delta \in \mathbb{C}$. It holds $2 \sigma^{2} \Phi(u ; \delta)=\delta^{2}-2 \delta G(u)+G(u)^{2}$. For some $\delta_{0} \in \mathbb{C}$, consider arbitrary $\delta \in \mathbb{C}$ such that $\left|\delta-\delta_{0}\right| \leq 1$. By the triangle inequality,

$$
|\exp (-\Phi(u ; \delta))| \leq\left|\exp \left(-\delta^{2} /\left(2 \sigma^{2}\right)\right)\right| \exp \left(-\left(G(u)^{2}-2\right| G(u)|\left[\left|\delta_{0}\right|+1\right]\right) /\left(2 \sigma^{2}\right)\right)
$$

By maximizing the quadratic polynomial $G(u) \mapsto-G(u)^{2}+2 G(u)\left(\left|\delta_{0}\right|+1\right)$ we get

$$
|\exp (-\Phi(u ; \delta))| \leq\left|\exp \left(-\delta^{2} /\left(2 \sigma^{2}\right)\right)\right| \exp \left(\frac{\left(\left|\delta_{0}\right|+1\right)^{2}}{2 \sigma^{2}}\right)
$$

where we used that $G(u) \in \mathbb{R}$ for $u \in X^{\prime}$. Thus, for every $\delta_{0} \in \mathbb{C}$,

$$
\begin{aligned}
& \sup _{\delta \in \mathbb{C},\left|\delta-\delta_{0}\right| \leq 1} \int_{X^{\prime}}|\phi(u) \| \exp (-\Phi(u ; \delta)) \mid \pi_{0}(\mathrm{~d} u) \\
& \quad \leq \sup _{\delta \in \mathbb{C},\left|\delta-\delta_{0}\right| \leq 1}\left|\exp \left(-\delta^{2} /\left(2 \sigma^{2}\right)\right)\right| \exp \left(\frac{\left(\left|\delta_{0}\right|+1\right)^{2}}{2 \sigma^{2}}\right) \mathbb{E}^{\pi_{0}}(|\phi|)<\infty
\end{aligned}
$$

This verifies assumption (iii) in Proposition 4.3, i.e., $\delta \mapsto \int_{X^{\prime}}|\phi(u)|\left|\exp (-\Phi(u ; \delta))\right| \pi_{0}(\mathrm{~d} \boldsymbol{y})$ is locally bounded for every $\delta \in \mathbb{C}$. The assertion of the lemma is then implied Proposition 4.3.

# 4.2.2 Multivariate Case $(K>1)$ 

Lemma 4.5 In the setting of Theorem 2.4 let $1<K \in \mathbb{N}$ and assume that $\phi \in L^{1}\left(X^{\prime}, \pi_{0}\right)$ and $\Phi(u ; \delta)=(\delta-G(u))^{\top} \Sigma^{-1}(\delta-G(u)) / 2$ for some SPD matrix $\Sigma \in \mathbb{R}^{K \times K}$.

Then, for each fixed $\widetilde{\delta} \in \mathbb{R}^{K-1}$, the mapping

$$
\delta \mapsto \int_{X^{\prime}} \phi(u) \exp (-\Phi((\delta ; \widetilde{\delta}) ; u)) \pi_{0}(\mathrm{~d} u)
$$

is holomorphic on $\mathbb{C}$.
Proof. The assertion is proven similarly to Lemma 4.4 and is also a consequence of Proposition 4.3. In the ensuing argument, for $\delta \in \mathbb{C}^{K}$ we denote by $|\delta|=+\sqrt{\delta^{H} \widetilde{\delta}}$ the modulus with respect to the Euclidean norm on $\mathbb{C}^{K}$ and, for a $K \times K$ real, symmetric matrix $\Sigma$, we denote by $\|\Sigma\|$ its spectral norm.

The mapping $\mathbb{C} \ni \delta \rightarrow \int_{X^{\prime}} \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u)$ satisfies assumptions (i) and (ii) in Proposition 4.3.

For the verification of assumption (iii) in Proposition 4.3, let us denote $\boldsymbol{\delta}:=(\delta ; \widetilde{\delta}) \in \mathbb{C}^{K}$ for any $\delta \in \mathbb{C}$ and $\widetilde{\delta} \in \mathbb{C}^{K-1}$. We observe that $2 \Phi(u ; \boldsymbol{\delta})=\left(\boldsymbol{\delta}^{\top} \Sigma^{-1} \boldsymbol{\delta}-2 \boldsymbol{\delta}^{\top} \Sigma^{-1} G(u)+\right.$ $\left.G(u)^{\top} \Sigma^{-1} G(u)\right)$. Moreover, it holds that

$$
\left|\exp \left(\boldsymbol{\delta}^{\top} \Sigma^{-1} G(u)\right)\right|=\exp \left(\boldsymbol{\delta}_{\mathrm{Re}}^{\top} \Sigma^{-1} G(u)\right)\left(\leq \exp \left(\left\|\Sigma^{-1}\right\| \mid \boldsymbol{\delta}_{\mathrm{Re}} \| G(u)\right)\right)
$$

Let $\widetilde{\delta} \in \mathbb{C}^{K-1}$ and $\delta_{0} \in \mathbb{C}$ be arbitrary and denote $\boldsymbol{\delta}_{0}:=\left(\delta_{0} ; \widetilde{\delta}\right)$. Denote by $\sigma>0$ the largest eigenvalue of $\Sigma^{1 / 2}$. For every $\delta \in \mathbb{C}$ such that $\left|\delta-\delta_{0}\right| \leq 1$,

$$
\begin{aligned}
& \left|\exp (-\Phi(u ; \boldsymbol{\delta}))\right| \\
& \quad \leq\left|\exp \left(-\boldsymbol{\delta}^{\top} \Sigma^{-1} \boldsymbol{\delta} / 2\right)\right|\left|\exp \left(-|G(u)|^{2} /\left(2 \sigma^{2}\right)+\left\|\Sigma^{-1}\right\|\left(\left|\widetilde{\delta}\right|+\left|\delta_{0}\right|+1\right)|G(u)|\right)\right|\right.
\end{aligned}
$$

Similar as in the proof of Lemma 4.4, maximizing the polynomial $s \mapsto-s^{2} /\left(2 \sigma^{2}\right)+\left\|\Sigma^{-1}\right\|(|\tilde{\delta}|+$ $\left.\left|\delta_{0}\right|+1\right) s$, we get

$$
|\exp (-\Phi(u ; \boldsymbol{\delta}))| \leq\left|\exp \left(-\boldsymbol{\delta}^{\top} \Sigma^{-1} \boldsymbol{\delta} / 2\right)\right| \exp \left(\frac{\left(|\widetilde{\delta}|+\left|\delta_{0}\right|+1\right)^{2} \sigma^{2}\left\|\Sigma^{-1}\right\|^{2}}{2}\right)
$$

Thus

$$
\sup _{\delta \in \mathbb{C},\left|\delta-\delta_{0}\right| \leq 1} \int_{X^{\prime}}|\phi(u)|\left|\exp (-\Phi(u ; \boldsymbol{\delta}))\right| \pi_{0}(\mathrm{~d} u)<\infty
$$

which establishes the local boundedness of $\delta \rightarrow \int_{X^{\prime}} \phi(u) \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u)$, i.e., assumption (iii) in Proposition 4.3. The assertion of this lemma then follows from Proposition 4.3.

Proposition 4.6 In the setting of Theorem 2.4 let $1<K \in \mathbb{N}$ and assume that $\phi \in L^{1}\left(X^{\prime}, \pi_{0}\right)$ and $\Phi(u ; \delta)=(\delta-G(u))^{\top} \Sigma^{-1}(\delta-G(u)) / 2$ for some SPD matrix $\Sigma \in \mathbb{R}^{K \times K}$.

Then the map

$$
\delta \mapsto \int_{X^{\prime}} \phi(u) \exp \left(-\Phi(u ; \delta) \pi_{0}(\mathrm{~d} u)\right.
$$

is holomorphic on $\mathbb{C}^{K}$.
Proof. Lemma 4.5 implies holomorphy in every coordinate $\delta_{i}, i=1, \ldots, K$. The assertion now follows by Hartogs' theorem, cf. [17, Theorem 2.2.8].

Corollary 4.7 In the setting of Theorem 2.4 and Proposition 4.6, and for every finite $r>0$, the map

$$
[-r, r]^{K} \ni \delta \mapsto \mathbb{E}^{\pi^{\delta}}[\phi] \in \mathbb{R}
$$

admits a holomorphic extension to some open set $\mathcal{E}$ such that $[-r, r]^{K} \subset \mathcal{E} \subset \mathbb{C}^{K}$.
Proof. By Proposition 4.6 the maps

$$
\delta \mapsto Z^{\prime}(\delta)=\int_{X^{\prime}} \phi(u) \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u), \quad \delta \mapsto Z(\delta)=\int_{X^{\prime}} \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u)
$$

admit (unique) holomorphic extensions to all of $\mathbb{C}^{K}$. Furthermore $Z(\delta)>0$ for $\mathbb{Q}_{0}$-a.e. $\delta \in \mathbb{R}^{K}$. Since $\mathbb{Q}_{0}$ is a Gaussian measure and $Z$ is continuous (even analytic) as a function of $\delta \in \mathbb{R}^{K}$, we conclude $Z(\delta)>0$ for every $\delta \in \mathbb{R}^{K}$. Hence there is a bounded set $\mathcal{E} \subset \mathbb{C}^{K}$ strictly containing $[-r, r]^{K}$ such that the map $\delta \mapsto \mathbb{E}^{\pi^{\delta}}[\phi]=Z^{\prime}(\delta) / Z(\delta)$ admits an holomorphic extension to $\mathcal{E}$.

# 5 Exponential DNN Expression Rate 

Using the holomorphy of the data-to-QoI map $\left[\delta \mapsto \mathbb{E}^{\pi^{\delta}}[\phi]\right]$ established in Section 2 (for additive, centered Gaussian observation noise $\eta$ in (1)), in this section we prove for this map and deep ReLU NNs an exponential expression rate bound in term of the overall NN size. As a byproduct of the proof, we also show exponential convergence rates for polynomial and rational approximations.

The approximation of the data-to-QoI map $\left[\delta \mapsto \mathbb{E}^{\pi^{\delta}}[\phi]\right]$ by ReLU NNs will be developed for observation data $\delta$ in compact subsets of $\mathbb{R}^{K}$. We immediately point out that in the (assumed) observation noise model (1), i.e. $\delta=G(u)+\eta$, the RV $\delta$ can take arbitrarily large values with positive probability. This may be due to the unboundedness of the uncertain input $u$ (see for example Section 3.2.3) or due to the unboundedness of the additive noise $\eta$, e.g. additive Gaussian noise. However, bounds on the tails of these distributions entail that the RV $\delta$ takes values in a compact set with high probability. Specifically, in the case of a prior measure with bounded support, the probability of data $\delta$ outside of a compact box $[-r, r]^{K}$ decays double exponentially, i.e., upper bounded by $C \exp \left(-r^{2} K /\left(2 \lambda_{\max }\right)\right)$, where $\lambda_{\max }$ is the largest eigenvalue of the covariance matrix of the additive Gaussian observation noise and $C>0$ is a constant that does not depend on $r$, cf. [18, Theorem 1]. The approximation of the data-to-QoI map by ReLU NNs will be developed on compact subsets of $\mathbb{R}^{K}$.

The structure of the section is as follows: in Section 5.1, we recapitulate notation and define the architecture for the ReLU DNN approximations to be analyzed. The main reference here is [25], and also [26, 31]. We remark in passing that alternative, more involved architectures could afford better expression rate bounds; we refer to [35] and to the discussion in Section 6 ahead. Next, in Section 5.2 we show exponential convergence of polynomial and rational approximations, with a slightly better result in the latter case. Finally, in Section 5.3, we infer the expression rate bounds, based on holomorphy $\left(\delta \mapsto Z^{\prime}(\delta)\right.$ and $\delta \mapsto Z(\delta)$ are holomorphic on all of $\mathbb{C}^{K}$ ) of the data-to-QoI map in Bayesian inversion.

# 5.1 Definitions and Architecture of Deep ReLU NNs 

We consider feed-forward deep neural networks (DNNs). These DNNs are obtained as iterated compositions of linear transformations followed by a nonlinearity. This nonlinearity is specified via the so-called activation function $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ of the DNN. The architecture of the DNN comprises a fixed number of hidden layers $L \in \mathbb{N}$, numbers $N_{\ell} \in \mathbb{N}$ of computation nodes in layer $\ell \in\{0, \ldots, L\}$, the map $f: \mathbb{R}^{N_{0}} \rightarrow \mathbb{R}^{N_{L+1}}$ is said to be realized by a feedforward neural network, if for certain weights $w_{i, j}^{\ell} \in \mathbb{R}$, and biases $b_{j}^{\ell} \in \mathbb{R}$ it holds for all $x=\left(x_{i}\right)_{i=1}^{N_{0}}$

$$
z_{j}^{1}=\sigma\left(\sum_{i=1}^{N_{0}} w_{i, j}^{1} x_{i}+b_{j}^{1}\right), \quad j \in\left\{1, \ldots, N_{1}\right\}
$$

and

$$
z_{j}^{\ell+1}=\sigma\left(\sum_{i=1}^{N_{\ell}} w_{i, j}^{\ell+1} z_{i}^{\ell}+b_{j}^{\ell+1}\right), \quad \ell \in\{1, \ldots, L-1\}, \quad j \in\left\{1, \ldots, N_{\ell+1}\right\}
$$

and finally

$$
f(x)=\left(z_{j}^{L+1}\right)_{j=1}^{N_{L+1}}=\left(\sum_{i=1}^{N_{L}} w_{i, j}^{L+1} z_{i}^{L}+b_{j}^{L+1}\right)_{j=1}^{N_{L+1}}
$$

In this case $n=N_{0}$ is the dimension of the NN input, and $m=N_{L+1}$ is the dimension of the output. Furthermore $z_{j}^{\ell}$ denotes the output of unit $j$ in layer $\ell$. The weight $w_{i, j}^{\ell}$ has the interpretation of connecting the $i$ th unit in layer $\ell-1$ with the $j$ th unit in layer $\ell$. We do not distinguish between the network (which is defined through $\sigma$, the $w_{i, j}^{\ell}$ and $b_{j}^{\ell}$ ) and the function $f: \mathbb{R}^{N_{0}} \rightarrow \mathbb{R}^{N_{L+1}}$ it realizes although such distinction is mathematically at times mandatory (we refer to the discussion in [26, Definition 2.1]. The number of hidden layers $L$ of a NN is referred to as depth of the DNN. We shall in particular consider DNNs with the so-called ReLU activation $\sigma=\sigma_{1}$ given by $x \mapsto \sigma_{1}(x):=\max \{0, x\}$. Let us also denote the tensor of weights by $\boldsymbol{w}$ and of biases by $\boldsymbol{b}$.

### 5.2 Polynomial and Rational Approximation

We have seen in the previous sections that the data-to-QoI map with respect to the unnormalized posterior density, i.e., $\delta \mapsto \int_{X^{\prime}} \phi(u) \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u)$ can be extended to an entire function on $\mathbb{C}^{K}$ under the assumption of additive, nondegenerate Gaussian observation noise. Holomorphy implies fast convergence of Taylor expansions as we recall this in the next theorem. The proof (which is based on standard arguments) is provided in Appendix A.

Theorem 5.1 Let $K \in \mathbb{N}$, and assume that $f: \mathbb{C}^{K} \rightarrow \mathbb{C}$ is holomorphic. Then, for every $\kappa>1$ and every $r>0$, there exists $C_{\kappa, f, r}>0$ such that for every $n \in \mathbb{N}$ it holds

$$
\sup _{\delta \in[-r, r]^{K}}\left|f(\delta)-\sum_{\|\boldsymbol{\nu}\|_{\ell} \infty} \leq n \frac{\partial^{\boldsymbol{\nu}} f(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right| \leq C_{\kappa, f, r} \exp (-\kappa n)
$$

Note that $\left\{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}:\|\boldsymbol{\nu}\|_{\ell \infty} \leq n\right\}$ has cardinality $c_{n}:=(n+1)^{K}$. With respect to the number $c_{n}$ of terms in the Taylor expansion, the error in (33) thus decreases exponentially, namely like $\exp \left(-\kappa c_{n}^{1 / K}\right)$.

In the following we show two approximation results for the data-to-QoI map $\left[\delta \mapsto \mathbb{E}^{\pi^{2}}[\phi]\right]$. As earlier, for every $\delta \in \mathbb{R}^{K}$ we denote

$$
Z^{\prime}(\delta):=\mathbb{E}^{\pi^{2}}[\phi] Z(\delta) \quad \text { and } \quad Z(\delta):=\int_{X^{\prime}} \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u)
$$

It is classical, that holomorphic (but not necessarily entire) functions can be approximated at an exponential rate with polynomial functions (see for example [24, Theorem 3.5]). As a consequence of Corollary 4.7 we thus have the following statement.

Proposition 5.2 Suppose the setting of Theorem 2.4 and Section 4.2 (i.e. the observation noise $\eta$ in (1) is Gaussian). Let $r>0$ and $K \in \mathbb{N}$. Then there exist constants $\kappa>0$ and $C_{r}>0$ (depending also on the observation noise covariance $\Sigma$ ) such that for every $n \in \mathbb{N}$ there exists a polynomial $p_{n} \in \operatorname{span}\left\{\boldsymbol{y}^{\boldsymbol{\nu}}:\|\boldsymbol{\nu}\|_{\ell \infty} \leq n\right\}$ such that

$$
\sup _{\delta \in[-r, r]^{K}}\left|\mathbb{E}^{\pi^{\delta}}[\phi]-p_{n}(\delta)\right| \leq C_{r} \exp (-\kappa n)
$$

Using Theorem 5.1, we can improve this statement using a rational rather than a polynomial approximation (note that, contrary to Proposition 5.2, $\kappa$ in Proposition 5.3 can be arbitrarily large).

Proposition 5.3 Suppose the setting of Theorem 2.4 and Section 4.2 (i.e. the observation noise $\eta$ in (1) is Gaussian). Let $r>0$ and $K \in \mathbb{N}$. For every $\kappa>0$ there exists $C_{r, \kappa}>0$ (depending also on the observation noise covariance $\Sigma$ ) and $n_{0} \in \mathbb{N}$ such that for all $n \geq n_{0}$

$$
\sup _{\delta \in[-r, r]^{K}}\left|\mathbb{E}^{\pi^{\delta}}[\phi]-\left(\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z^{\prime}(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right)\left(\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right)^{-1}\right| \leq C_{r, \kappa} \exp (-\kappa n)
$$

Proof. Recall that $\mathbb{E}^{\pi^{\delta}}[\phi]=Z^{\prime}(\delta) / Z(\delta)$, where $Z^{\prime}(\delta)$ and $Z(\delta)$ are defined in (34). By Proposition 4.6 and Theorem 5.1, for every $\kappa>0$, there exists a constant $C_{r, \kappa}>0$ such hat for every $n \in \mathbb{N}$,

$$
\sup _{\delta \in[-r, r]^{K}}\left\{\left|Z^{\prime}(\delta)-\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z^{\prime}(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right|+\left|Z(\delta)-\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right|\right\} \leq C_{r, \kappa} \exp (-\kappa n)
$$

Since $Z(\delta)>0$ for every $\delta \in \mathbb{R}^{K}$, it holds that $Z_{\min }:=\inf _{\delta \in[-r, r]^{K}} Z(\delta)>0$, where we used that $\left[\mathbb{R}^{K} \ni \delta \mapsto Z(\delta)\right]$ is continuous by Proposition 4.6. Thus by (35), there exists $n_{0} \in \mathbb{N}$ such that for every $n \geq n_{0}$ and every $\delta \in[-r, r]^{K}$

$$
\frac{Z_{\min }}{2} \leq \sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}
$$

Thus,

$$
\begin{aligned}
& \mathbb{E}^{\pi^{\delta}}[\phi]-\left(\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z^{\prime}(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right)\left(\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right)^{-1} \mid \\
& \quad \leq \frac{\left|Z^{\prime}(\delta)-\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z^{\prime}(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right|}{Z_{\min }}+\frac{\left|\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z^{\prime}(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right|\left|Z(\delta)-\sum_{\|\boldsymbol{\nu}\|_{l \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} Z(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right|}{\frac{1}{2}\left(Z_{\min }\right)^{2}}
\end{aligned}
$$

which follows similarly as (28). The asserted estimate follows now by (35).

# 5.3 Deep ReLU Approximation 

The following result is an improvement of the exponential convergence rate in [25, Theorem 3.7] for the (smaller) class of entire, analytic functions. The proof is similar to the argument in [25], being mainly based on the NN approximation results of [34]. For convenience of the reader, we provide a proof in Appendix B.

Theorem 5.4 Let $K \in \mathbb{N}$ and assume that $f: \mathbb{C}^{K} \rightarrow \mathbb{C}$ is holomorphic such that $f: \mathbb{R}^{K} \rightarrow \mathbb{R}$. Then for all $\kappa>1, r>0$ there exists a constant $C_{f, r, \kappa}>0$ such that for all $n \in \mathbb{N}$ there exists a ReLU $N N \tilde{f}_{n}:[-r, r]^{K} \rightarrow \mathbb{R}$ with

$$
\sup _{\delta \in[-r, r]^{K}}\left|f(\delta)-\tilde{f}_{n}(\delta)\right| \leq C_{f, r, \kappa} \exp (-\kappa n)
$$

Moreover, there exists a constant $C>0$ which is independent of $\kappa$ such that $\operatorname{depth}\left(\tilde{f}_{n}\right) \leq$ $C(1+n \log (n))$, and $\operatorname{size}\left(\tilde{f}_{n}\right) \leq C(1+n)^{K+1}$ for all $n \in N$.

In the following we use again the notation

$$
Z^{\prime}(\delta)=\int_{X^{\prime}} \phi(u) \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u), \quad Z(\delta)=\int_{X^{\prime}} \exp (-\Phi(u ; \delta)) \pi_{0}(\mathrm{~d} u)
$$

so that $\mathbb{E}^{\pi^{\delta}}[\phi]=Z^{\prime}(\delta) / Z(\delta)$. Furthermore, we define the (finite, under the made assumptions) constants

$$
Z_{\min }:=\inf _{\delta \in[-r, r]^{K}} Z(\delta), \quad Z_{\max }:=\sup _{\delta \in[-r, r]^{K}} Z(\delta)
$$

and

$$
Z_{\min }^{\prime}:=\inf _{\delta \in[-r, r]^{K}} Z^{\prime}(\delta), \quad Z_{\max }^{\prime}:=\sup _{\delta \in[-r, r]^{K}} Z^{\prime}(\delta)
$$

While of independent interest, the preceding Theorem 5.4 is a key ingredient in the proof of the following result, which is a principal result of the present paper.

Theorem 5.5 Suppose the setting of Theorem 2.4 and Section 4.2 (in particular that the observation noise $\eta$ in (1) is Gaussian). Let $r>0$ and $K \in \mathbb{N}$. Then there holds
(i) For $K=1$, there exist $\kappa>0$ (independent of $r, Z$ and $Z^{\prime}$ ) and a constant $C_{\kappa}>0$ (depending on $\kappa, r, Z, Z^{\prime}, \Sigma$ ) such that for all $n \in \mathbb{N}$ there exists a ReLU $N N \tilde{f}_{n}$ : $[-r, r]^{K} \rightarrow \mathbb{R}$ such that

$$
\sup _{\delta \in[-r, r]^{K}}\left|\mathbb{E}^{\pi^{\delta}}[\phi]-\tilde{f}_{n}(\delta)\right| \leq C_{\kappa} \exp \left(-\frac{\kappa}{\sqrt{\left|\log \left(Z_{\max } / Z_{\min }\right)\right|}} n\right)
$$

Furthermore, there exists a constant $C>0$ (independent of $Z, Z^{\prime}$ ) such that for every $n \in \mathbb{N}$ holds

$$
\begin{aligned}
\operatorname{depth}\left(\tilde{f}_{n}\right) & \leq C\left(1+n \log (n)+\log ^{3}(n)\right) \\
\operatorname{size}\left(\tilde{f}_{n}\right) & \leq C\left[1+n^{2}\left(\log (n)+\log \left(\left\lceil\sqrt{\log \left(Z_{\max } / Z_{\min }\right)}\right\rceil\right)\right)\right]
\end{aligned}
$$

(ii) For $K>1$, for every $\kappa>0$ there exists a constant $C_{\kappa}>0$ (depending on $\kappa, r, Z, Z^{\prime}, \Sigma$ ) such that for all $n \in \mathbb{N}$ there exists a ReLU $N N \tilde{f}_{n}:[-r, r]^{K} \rightarrow \mathbb{R}$ such that

$$
\sup _{\delta \in[-r, r]^{K}}\left|\mathbb{E}^{\pi^{\delta}}[\phi]-\tilde{f}_{n}(\delta)\right| \leq C_{\kappa} \exp (-\kappa n)
$$

Furthermore, there exists a positive constant $C>0$ such that for every $n \in \mathbb{N}$ holds

$$
\operatorname{depth}\left(\tilde{f}_{n}\right) \leq C\left(1+n^{3 / 2} \log (n)\right), \quad \operatorname{size}\left(\tilde{f}_{n}\right) \leq C(1+n)^{K+1}
$$

Proof. Step 1. We provide the proof for $K>1$. Fix $\kappa>1$ (arbitrarily large). Due to the compactness of $[-r, r]^{K}$ and the continuity (even analyticity) of $\delta \mapsto Z(\delta)$ and $\delta \mapsto Z^{\prime}(\delta)$, we have (cf. (36))

$$
0<Z_{\min }, \quad Z_{\max }<\infty, \quad-\infty<Z_{\min }^{\prime}, \quad Z_{\max }^{\prime}<\infty
$$

To approximate $\mathbb{E}^{\pi^{s}}[\phi]$ as a function of $\delta \in[-r, r]^{K}$ we will combine the following NNs:
(i) $\tilde{Z}_{n}:[-r, r]^{K} \rightarrow\left[Z_{\min }-\varepsilon_{Z}, Z_{\max }+\varepsilon_{Z}\right]$ : By Proposition 4.6 and Theorem 5.4 there exists a constant $C_{r, \kappa}, C>0$ and a network $\tilde{Z}_{n}$ such that

$$
\sup _{\delta \in[-r, r]^{K}}\left|Z(\delta)-\tilde{Z}_{n}(\delta)\right| \leq C_{r, \kappa} \exp (-\kappa n)=: \varepsilon_{Z}
$$

and $\operatorname{depth}\left(\tilde{Z}_{n}\right) \leq C(1+n \log (n))$ and $\operatorname{size}\left(\tilde{Z}_{n}\right) \leq C(1+n)^{K+1}$.
(ii) $\tilde{Z}_{n}^{\prime}:[-r, r]^{K} \rightarrow\left[Z_{\min }^{\prime}-\varepsilon_{Z^{\prime}}, Z_{\max }^{\prime}+\varepsilon_{Z^{\prime}}\right]$ : By Proposition 4.6 and by Theorem 5.4, there exists a constant $C_{r, \kappa}, C>0$ and a network $\tilde{Z}_{n}^{\prime}$ such that

$$
\sup _{\delta \in[-r, r]^{K}}\left|Z^{\prime}(\delta)-\tilde{Z}_{n}^{\prime}(\delta)\right| \leq C_{r, \kappa} \exp (-\kappa n)=: \varepsilon_{Z^{\prime}}
$$

and $\operatorname{depth}\left(\tilde{Z}_{n}^{\prime}\right) \leq C(1+n \log (n))$ and $\operatorname{size}\left(\tilde{Z}_{n}^{\prime}\right) \leq C(1+n)^{K+1}$ (without loss of generality we use here the same symbol for the constant $C_{r, \kappa}$ as in (i)).
(iii) $\tilde{d}_{n}:\left[Z_{\min } / 2,2 Z_{\max }\right] \rightarrow\left[\left(2 Z_{\max }\right)^{-1}-\varepsilon_{d},\left(Z_{\min } / 2\right)^{-1}+\varepsilon_{d}\right]$ : The map $x \mapsto 1 / x$ is analytic on $\mathbb{C} \backslash\{0\}$. Hence, by [25, Theorem 3.7] there exists $\kappa_{0}, C>0$ and a NN $\tilde{d}_{n}$ such that

$$
\left|x^{-1}-\tilde{d}_{n}(x)\right| \leq \exp \left(-\kappa_{0} n\right)=: \varepsilon_{d} \quad \forall x \in\left[Z_{\min } / 2,2 Z_{\max }\right]
$$

with $\operatorname{size}\left(\tilde{d}_{n}\right) \leq C(1+n)^{2}$ and $\operatorname{depth}\left(\tilde{d}_{n}\right) \leq C(1+n \log (n))$.
(iv) $\tilde{m}_{n}:\left[Z_{\min }^{\prime} / 2,2 Z_{\max }^{\prime}\right] \times\left[\left(4 Z_{\max }\right)^{-1},\left(Z_{\min } / 4\right)^{-1}\right] \rightarrow \mathbb{R}$ : by [31, Proposition 3.1] (this is a variation of the original result from [34]), there exists $\kappa_{0}>0$ and a ReLU NN $\tilde{m}_{n}$ such that

$$
\left|x y-\tilde{m}_{n}(x, y)\right| \leq \exp \left(-\kappa_{0} n\right) \quad \forall(x, y) \in\left[Z_{\min }^{\prime} / 2,2 Z_{\max }^{\prime}\right] \times\left[\left(4 Z_{\max }\right)^{-1},\left(Z_{\min } / 4\right)^{-1}\right]
$$

(without loss of generality we use here the same symbol for the constant $\kappa_{0}$ as in (iii)). Furthermore, there is a constant $C>0$ such that for every $n \in \mathbb{N}$ holds $\operatorname{size}\left(\tilde{m}_{n}\right) \leq C n$ and $\operatorname{depth}\left(\tilde{m}_{n}\right) \leq C n$.

Now consider

$$
\tilde{f}_{n}(\delta):=\tilde{m}_{\left\lceil n^{3 / 2}\right\rceil}\left(\tilde{Z}_{n}^{\prime}(\delta), \tilde{d}_{\left\lceil n^{3 / 2}\right\rceil}\left(\tilde{Z}_{n}(\delta)\right)\right)
$$

As a consequence of (39), the terms $\varepsilon_{Z}, \varepsilon_{Z}^{\prime}$ and $\varepsilon_{d}$ tend to 0 as $n \rightarrow \infty$. Hence there exists $n_{0} \in \mathbb{N}$ (depending on $\kappa, \kappa_{0}, Z$ and $Z^{\prime}$ ), such that for the composition of networks in (40), the output of each network belongs to the domain of the network it is composed with.

We now bound the approximation error. For every $\delta \in[-r, r]^{K}$ and for all $n \geq n_{0}$

$$
\begin{aligned}
\left|\mathbb{E}^{\pi^{s}}[\phi]-\tilde{f}_{n}(\delta)\right| \leq & \left|\tilde{m}_{\left\lceil n^{3 / 2}\right\rceil}\left(\tilde{Z}_{n}^{\prime}(\delta), \tilde{d}_{\left\lceil n^{3 / 2}\right\rceil}\left(\tilde{Z}_{n}(\delta)\right)\right)-\tilde{Z}_{n}^{\prime}(\delta) \tilde{d}_{\left\lceil n^{3 / 2}\right\rceil}\left(\tilde{Z}_{n}(\delta)\right)\right| \\
& +\left|\tilde{d}_{\left\lceil n^{3 / 2}\right\rceil}\left(\tilde{Z}_{n}(\delta)\right)-\tilde{Z}_{n}(\delta)^{-1}\right|\left|\tilde{Z}_{n}^{\prime}(\delta)\right|+\left|\frac{\tilde{Z}_{n}^{\prime}(\delta)}{\tilde{Z}_{n}(\delta)}-\frac{Z_{n}^{\prime}(\delta)}{Z_{n}(\delta)}\right|
\end{aligned}
$$

which implies

$$
\left|\mathbb{E}^{\pi^{s}}[\phi]-\tilde{f}_{n}(\delta)\right| \leq \exp \left(-\kappa_{0}\left\lceil n^{3 / 2}\right\rceil\right)+\exp \left(-\kappa_{0}\left\lceil n^{3 / 2}\right\rceil\right)\left|\tilde{Z}_{n}^{\prime}(\delta)\right|+\left|\frac{\tilde{Z}_{n}^{\prime}(\delta)}{\tilde{Z}_{n}(\delta)}-\frac{Z_{n}^{\prime}(\delta)}{Z_{n}(\delta)}\right|
$$

We have

$$
\left|\tilde{Z}_{n}^{\prime}(\delta)\right| \leq\left|Z^{\prime}(\delta)-\tilde{Z}_{n}^{\prime}(\delta)\right|+\left|Z^{\prime}(\delta)\right| \leq C_{r, \kappa} \exp (-\kappa n)+Z_{\max }^{\prime}
$$

Hence

$$
\begin{aligned}
\exp \left(-\kappa_{0}\left\lceil n^{3 / 2}\right\rceil\right)+\exp \left(-\kappa_{0}\left\lceil n^{3 / 2}\right\rceil\right)\left|\tilde{Z}_{n}(\delta)\right| & \leq\left(1+C_{r, \kappa}+Z_{\max }\right) \exp \left(-\kappa_{0}\left\lceil n^{3 / 2}\right\rceil\right) \\
& \leq\left(1+C_{r, \kappa}+Z_{\max }\right) \exp (-\kappa n)
\end{aligned}
$$

as long as $\left\lceil n^{3 / 2}\right\rceil \kappa_{0} \geq \kappa n$, which is ensured by the condition $n \geq\left(\kappa / \kappa_{0}\right)^{2}$. Next, using that for arbitrary $a, b, c, d \in \mathbb{R}$ it holds $|a / b-c / d|=|a d-c b| /|b d|$ and $|a d-c b|=|(a-c) d+(d-b) c| \leq$ $|a-c||d|+|d-b||c|$, we find

$$
\left|\frac{\tilde{Z}_{n}^{\prime}(\delta)}{\tilde{Z}_{n}(\delta)}-\frac{Z_{n}^{\prime}(\delta)}{Z_{n}(\delta)}\right| \leq \frac{\tilde{Z}_{\max }\left|\tilde{Z}_{n}^{\prime}(\delta)-Z^{\prime}(\delta)\right|+Z_{\max }^{\prime}\left|\tilde{Z}_{n}(\delta)-Z(\delta)\right|}{\tilde{Z}_{\min } Z_{\min }} \leq C_{r, \kappa} \exp (-\kappa n) \frac{Z_{\max }^{\prime}+\tilde{Z}_{\max }}{\tilde{Z}_{\min } Z_{\min }}
$$

In all

$$
\left|\mathbb{E}^{\pi^{\delta}}[\phi]-\tilde{f}_{n}(\delta)\right| \leq\left(1+C_{r, \kappa}+Z_{\max }+C_{r, \kappa} \frac{\tilde{Z}_{\max }+Z_{\max }^{\prime}}{Z_{\min } \tilde{Z}_{\min }}\right) \exp (-\kappa n)
$$

provided that $n \geq \max \left\{n_{0},\left(\kappa / \kappa_{0}\right)^{2}\right\}$. In the complementary case $n<\max \left\{n_{0},\left(\kappa / \kappa_{0}\right)^{2}\right\}$, we set $\tilde{f}_{n}:=0$ so that

$$
\left|\mathbb{E}^{\pi^{\delta}}[\phi]-\tilde{f}_{n}(\delta)\right| \leq \frac{\sup _{\delta \in[-r, r] \kappa}\left|\mathbb{E}^{\pi^{\delta}}[\phi]\right|}{\exp \left(-\kappa \max \left\{n_{0},\left(\kappa / \kappa_{0}\right)^{2}\right\}\right)} \exp (-\kappa n)
$$

Hence, with

$$
C_{\kappa}:=\max \left\{\frac{\sup _{\delta \in[-r, r] \kappa}\left|\mathbb{E}^{\pi^{\delta}}[\phi]\right|}{\exp \left(-\kappa \max \left\{n_{0},\left(\kappa / \kappa_{0}\right)^{2}\right\}\right)}, 1+C_{r, \kappa}+Z_{\max }+C_{r, \kappa} \frac{\tilde{Z}_{\max }+Z_{\max }^{\prime}}{Z_{\min } \tilde{Z}_{\min }}\right\}
$$

we have $\left|\mathbb{E}^{\pi^{\delta}}[\phi]-\tilde{f}_{n}(\delta)\right| \leq C_{\kappa} \exp (-\kappa n)$ for all $n \in \mathbb{N}$.
Next we bound the size of the network. Since $K \geq 2$, there exists a constant such that it holds for all $n \in \mathbb{N}$

$$
\begin{aligned}
\operatorname{size}\left(\tilde{f}_{n}\right) & \leq C\left(\operatorname{size}\left(\tilde{Z}_{n}\right)+\operatorname{size}\left(\tilde{Z}_{n}^{\prime}\right)+\operatorname{size}\left(\tilde{m}_{\left\lceil n^{3 / 2}\right\rceil}\right)+\operatorname{size}\left(\tilde{d}_{\left\lceil n^{3 / 2}\right\rceil}\right)\right) \\
& \leq C\left((1+n)^{K+1}+(1+n)^{K+1}+\left(1+n^{3 / 2}\right)^{2}+n^{3 / 2}\right) \\
& \leq C(1+n)^{K+1}
\end{aligned}
$$

Here, the positive constant $C$ is independent also of $\kappa$. Similarly, one verifies the claimed bound on the depth of $\tilde{f}_{n}$. This completes the proof for $K \geq 2$.

For $K=1$, the proof will differ in the approximation of the map $[x \mapsto 1 / x]$. By Lemma C.1, there exists a constant $C>0, \bar{\kappa}>0(C, \kappa$ are independent of $Z_{\min }, Z_{\max }$ ) such that for every $n \in \mathbb{N}$ there exists a ReLU NN $\tilde{p}_{n}$ such that

$$
\begin{aligned}
& \sup _{x \in\left[Z_{\min } / 2,2 Z_{\max }\right]}\left|x^{-1}-\tilde{p}_{n}(x)\right| \\
& \quad \leq C \frac{\left\lceil\log \left(Z_{\max } / Z_{\min }\right)\right\rceil}{Z_{\min }}\left(1+\frac{1}{Z_{\max }-Z_{\min }}\right) \exp \left(-\frac{\bar{\kappa} n}{\sqrt{\left\lceil\log \left(Z_{\max } / Z_{\min }\right)\right\rceil}}\right) \\
& =: \widetilde{C} \exp \left(-\frac{\bar{\kappa} n}{\sqrt{\left\lceil\log \left(Z_{\max } / Z_{\min }\right)\right\rceil}}\right)
\end{aligned}
$$

Moreover, there exists a constant $\bar{C}>0$ such that depth $\left(\tilde{p}_{n}\right) \leq \bar{C}\left(1+n \log (n)+\log ^{3}(n)\right)$ and such that $\operatorname{size}\left(\tilde{p}_{n}\right) \leq \bar{C}\left[1+n^{2}\left(\log (n)+\log \left(\sqrt{\left[\log \left(\bar{Z}_{\max } / \bar{Z}_{\min }\right)\right]}\right)\right)\right]$. In this case we consider

$$
\tilde{f}_{n}(\delta):=\tilde{m}_{n}\left(\tilde{Z}_{n}^{\prime}(\delta), \tilde{p}_{n}\left(\tilde{Z}_{n}(\delta)\right)\right)
$$

By a version of (41) (obtained by replacing $\tilde{d}_{\lceil n^{3 / 2}\rceil}$ by $\tilde{p}_{n}$ and $\tilde{m}_{\lceil n^{3 / 2}\rceil}$ by $\tilde{m}_{n}$ ) and (42), there exists a constant $C$ (independent of $n$ ) such that

$$
\left|\mathbb{E}^{\pi^{2}}[\phi]-\tilde{f}_{n}(\delta)\right| \leq C \exp \left(-\frac{\min \left\{\kappa, \kappa_{0}, \bar{\kappa}\right\} n}{\sqrt{\left[\log \left(\bar{Z}_{\max } / \bar{Z}_{\min }\right)\right]}}\right)
$$

Finally, we bound the size of the network. There exists a constant $C>0$ such that it holds

$$
\begin{aligned}
\operatorname{size}\left(\tilde{f}_{n}\right) & \leq C\left(\operatorname{size}\left(\tilde{Z}_{n}\right)+\operatorname{size}\left(\tilde{Z}_{n}^{\prime}\right)+\operatorname{size}\left(\tilde{m}_{\lceil n\rceil}\right)+\operatorname{size}\left(\tilde{p}_{\lceil n\rceil}\right)\right) \\
& \leq C\left((1+n)^{2}+(1+n)^{2}+(1+n)^{2}+1+n^{2}\left(\log (n)+\log \left(\sqrt{\left[\log \left(\bar{Z}_{\max } / \bar{Z}_{\min }\right)\right]}\right)\right)\right) \\
& \leq C\left(1+n^{2}\left(\log (n)+\log \left(\sqrt{\left[\log \left(\bar{Z}_{\max } / \bar{Z}_{\min }\right)\right]}\right)\right)\right)
\end{aligned}
$$

Similarly, one verifies the claimed bound on the depth of $\tilde{f}_{n}$.
The appearance of the exponent $3 / 2$ in the depth of ReLU NN in Theorem 5.5 (ii) is an artifact of the proof technique. In (40), $n^{3 / 2}$ may be replaced by $n \varphi(n)$ for any strictly increasing function $\varphi:[1, \infty) \rightarrow[1, \infty)$ which tends to infinity as $n \rightarrow \infty$. Possible choices include $\varphi(n)=n^{\varepsilon}$ for $0<\varepsilon \ll 1$ or $\varphi(n)=\log (n)+1$ (in Theorem 5.5 (ii) $\varphi(n)=n^{1 / 2}$ was used). This would result in depth $(\tilde{f}) \leq C(1+n \varphi(n) \log (n))$, but would also result in a potentially larger constant $C_{\kappa}$ with $\left(\kappa / \kappa_{0}\right)^{2}$ replaced by $\varphi^{-1}\left(\kappa / \kappa_{0}\right)$ in (43).

# 6 Conclusions and Extensions 

We established the holomorphy of the data-to-(Bayesian) prediction (aka. "data-to-QoI") map for a finite-dimensional quantity of interest in PDE-constrained Bayesian inverse problems. It is applicable to general well-posed PDEs with uncertain input from function spaces, for observation data $\delta$ subject to additive Gaussian observation noise $\eta$. Based on the holomorphy of this map, we inferred exponential bounds on the expression rate of deep ReLU NNs for these maps.

We analyzed here in detail only the expression by deep ReLU NNs. As is well-known, this implies also exponential expression rates for DNNs with more regular activation functions (see, e.g. the discussion in [31, Section 3.3]).

We also showed that for more general, non-Gaussian observation noise models, the BIP is still well-posed but holomorphy of the data-to-QoI map can, in general, not be expected. In the particular case of finite-dimensional uncertain input and finite-dimensional observables, and for observation noise with Lipschitz continuous density, we showed that the data-to-QoI map is likewise Lipschitz continuous. In this case, exponential convergence rates can still be realized by DNNs with more elaborate architectures. Let us give here only one illustrative result indicating possible gains afforded by admitting DNN architectures with activation functions which are more general than ReLU.

Proposition 6.1 In the setting of Theorem 2.1, and for additive observation noise $\eta \in \mathbb{R}^{K}$ in (9) with law admitting a density $\rho$ with respect to $\lambda^{K}$ which is Lipschitz, $\rho \in \operatorname{Lip}\left(\mathbb{R}^{K}\right)$, for every $0<r<\infty$, there exist constants $c_{1, K}, C>0$ such that, for every $W \in \mathbb{N}$, there exists a DNN $\tilde{f}_{W, r}:[-r, r]^{K} \rightarrow \mathbb{R}$ with both ReLU and sinusoidal activation functions and with at most $W$ nonzero weights such that

$$
\sup _{\delta \in[-r, r]^{K}}\left|\mathbb{E}^{\pi^{2}}[\phi]-\tilde{f}_{W, r}(\delta)\right| \leq C \exp \left(-c_{1, K} \sqrt{W}\right)
$$

Proof. The assumptions imply with Proposition 4.2 that the data-to-QoI map $\left[\delta \mapsto \mathbb{E}^{\pi^{s}}[\phi]\right] \in$ $\operatorname{Lip}\left([-r, r]^{K}\right)$. The assertion follows from [35, Theorem 5.1] by scaling and translation from the unit box $[0,1]^{K}$ (which was studied in [35]) to $[-r, r]^{K}$.

We note that the proof of [35, Theorem 5.1] allows to infer information about the architecture of the DNN $\tilde{f}$ which appears in Proposition 6.1. In particular, the constant $c_{1, K}$ will, in general, be upper bounded by $c / K$, where $K$ is the (finite) dimension of the data space and and $c>0$ a generic constant independent of $K$, cf. [35, p. 7]. Furthermore, analyticity of the data-to-QoI map $f: B_{r}(0) \rightarrow \mathbb{R}$ is not required for Proposition 6.1 to hold. However, the very accurate and stable evaluation of the sinusoidal activations is crucial for the expression rate in Proposition 6.1 to materialize. As all DNNs are operated in finite float point precision, often with rather short mantissas (e.g. when rather strong quantization of NN weights is employed), the scope of Proposition 6.1 could be limited, in practice. Nevertheless, the result does cover, for example, the finite dimensional setting in Theorem 2.1, where the law $\mathbb{Q}_{0}$ admits merely a Lipschitz density w.r. to the Lebesgue measure $\lambda^{K}$ on the space of observation data $\delta$.

Remark 6.2 We remark that improved approximation rates when admitting a wider range of activation functions in an otherwise fixed DNN architecture is not surprising. See, e.g., [27, Theorem 7.1]. In practice, however, issues of DNN stability in finite precision arithmetic, in particular under quantization, of DNNs with these rather intricate activation functions arise.

In the present paper, we analyzed the rates of expressive power of deep ReLU NN surrogates for data-to-QoI maps for Bayesian inversion of well-posed PDEs subject to uncertain input data from function spaces. The present analysis substantiates recent numerical evidence (e.g. in [22] and the references there) that even for rather complex PDE models of physical systems, with possible rough/ singular solutions, rather small DNNs can provide highly accurate surrogates for input-to-observable maps in forward UQ and for data-to-QoI maps in Bayesian PDE inversion. The mathematical convergence rate bounds in the present paper are a stepping stone to the analysis of the generalization error, and to the mathematical analysis and the design of multilevel training algorithms, which we shall provide in [11].

The constants in the expressive power estimates depend on the covariance $\Sigma$ of the additive Gaussian observation noise and on the dimension $K$ of the data space. A detailed analysis of the effect when $\Sigma \rightarrow 0$ on the expressive power of ReLU NNs to approximate data-to-QoI maps will be developed elsewhere.

# A Proof of Theorem 5.1 

Fix $\gamma>1$. We provide the standard bound on the Taylor coefficients and assume first that $K=1$, i.e. $f: \mathbb{C} \rightarrow \mathbb{C}$. Cauchy's integral formula gives for every $j \in \mathbb{N}_{0}$

$$
\left|\frac{f^{(j)}(0)}{j!}\right|=\left|\frac{1}{2 \pi \mathrm{i}} \int_{|\xi|=\gamma} \frac{f(\xi)}{\xi^{j+1}} \mathrm{~d} \xi\right| \leq \frac{\sup _{|\xi|=\gamma}|f(\xi)|}{\gamma^{j}}
$$

where $\mathrm{i}=\sqrt{-1} \in \mathbb{C}$ denotes a complex root of -1 . Here we used that $f$ is holomorphic on $\mathbb{C}$ which in particular contains the ball of radius $\gamma$ around 0 .

If $K>1$, we repeatedly apply the estimate (45) in each variable to obtain for every $\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}$

$$
\left|\frac{\partial^{\boldsymbol{\nu}} f(0)}{\boldsymbol{\nu}!}\right| \leq \frac{\sup _{|\xi_{i}|=\gamma}\left|f\left(\xi_{1}, \ldots, \xi_{K}\right)\right|}{\gamma^{|\boldsymbol{\nu}|}}
$$

For $\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}$ denote $t_{\boldsymbol{\nu}}:=\partial^{\boldsymbol{\nu}} f(0) / \boldsymbol{\nu}!$. Since $f$ is holomorphic it admits a convergent multivariate Taylor expansion

$$
f(\delta)=\sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{k}} t_{\boldsymbol{\nu}} \delta^{\boldsymbol{\nu}} \quad \forall \delta \in \mathbb{C}^{K}
$$

We point out that with $\tilde{C}_{f, \gamma}:=\sup _{|\xi|=\gamma}\left|f\left(\xi_{1}, \ldots, \xi_{K}\right)\right|$, (46) shows $\left|t_{\boldsymbol{\nu}}\right| \leq \tilde{C}_{f, \gamma} \gamma^{-|\boldsymbol{\nu}|}$ and therefore

$$
\sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}}\left|t_{\boldsymbol{\nu}}\right| \leq \tilde{C}_{f, \gamma} \sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}} \gamma^{-|\boldsymbol{\nu}|}=\tilde{C}_{f, \gamma} \prod_{i=1}^{K}\left(\sum_{j \in \mathbb{N}_{0}} \gamma^{-j}\right)<\infty
$$

Thus the order of summation in (47) does not matter, as the series is absolutely convergent for all $|\delta|<\gamma$.

Now let us estimate the error of the truncated Taylor expansion. Fix $n \in \mathbb{N}, r>0$ and $\gamma>r$. Then

$$
\sup _{\delta \in[-r, r]^{K}}\left\|f(\delta)-\sum_{\|\boldsymbol{\nu}\|_{c \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} f(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right\| \leq \tilde{C}_{f, \gamma} \sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K} \backslash\{0, \ldots, n\}^{K}}\left(\frac{\gamma}{r}\right)^{-|\boldsymbol{\nu}|}
$$

Next,

$$
\begin{aligned}
\sup _{\delta \in[-r, r]^{K}}\left\|f(\delta)-\sum_{\|\boldsymbol{\nu}\|_{c \infty} \leq n} \frac{\partial^{\boldsymbol{\nu}} f(0)}{\boldsymbol{\nu}!} \delta^{\boldsymbol{\nu}}\right\| & \leq \tilde{C}_{f, \gamma} \sum_{\|\boldsymbol{\mu}\|_{c \infty}=n} \sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}}\left(\frac{\gamma}{r}\right)^{-|\boldsymbol{\mu}|-|\boldsymbol{\nu}|} \\
& \leq \tilde{C}_{f, \gamma}\left(\frac{\gamma}{r}\right)^{-n}\left|\left\{\boldsymbol{\mu} \in \mathbb{N}_{0}^{K}:\|\boldsymbol{\mu}\|_{\ell \infty}=n\right\}\right| \sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}}\left(\frac{\gamma}{r}\right)^{-|\boldsymbol{\nu}|} \\
& \leq \tilde{C}_{f, \gamma}\left(\frac{\gamma}{r}\right)^{-n}(1+n)^{K} \sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}}\left(\frac{\gamma}{r}\right)^{-|\boldsymbol{\nu}|} \\
& \leq \tilde{C}_{f, \gamma, r}\left(\frac{\gamma}{r}\right)^{-n}(1+n)^{K}
\end{aligned}
$$

where $\tilde{C}_{f, \gamma, r}:=\tilde{C}_{f, \gamma} \sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{k}}(\gamma / r)^{-|\boldsymbol{\nu}|}$ is finite by the same argument as in (48).
For a given $\kappa>1$ we can now choose $\gamma>\max \{\kappa, r\}$ so large that

$$
\left(\frac{\gamma}{r}\right)^{-n}(1+n)^{K} \leq \exp (-\kappa n) \quad \forall n \in \mathbb{N}
$$

The statement now follows by (49) and (50), which completes the proof of Theorem 5.1.

# B Proof of Theorem 5.4 

We prove the theorem for the case $r=1$. The general case $r>0$ follows by setting $f_{r}(\delta):=$ $f(r \delta)$, and then approximating $f_{r}$ with a neural network $\tilde{f}_{r, n}$ on $[-1,1]^{K}$. Then $\tilde{f}_{n}(\delta):=$ $\tilde{f}_{r, n}(\delta / r)$ is a suitable network approximating $f$, since for every $\delta \in[-r, r]^{K}$ it holds $f(\delta)-$ $\tilde{f}_{n}(\delta)=f_{r}(\delta / r)-\tilde{f}_{r, n}(\delta / r)$. Furthermore we assume for now $K \geq 2$, and discuss the case $K=1$ in the last step.

According to [31, Proposition 3.3], for every $m \in \mathbb{N}$ and every $\gamma>0$ there exists a network $\widetilde{\Pi}_{m, \gamma}:[-2,2]^{m} \rightarrow \mathbb{R}$ of size $C(1+m \log (m / \gamma))$ and depth $C(1+\log (m) \log (m / \gamma))$ such that

$$
\sup _{|x_{i}| \leq 2}\left|\prod_{j=1}^{m} x_{j}-\widetilde{\Pi}_{m, \gamma}\left(x_{1}, \ldots, x_{m}\right)\right| \leq \gamma
$$

Step 1. Using (51), for every $i \in \mathbb{N}_{0}$ and $\gamma>0$ we define $p_{i, \gamma}:[-1,1] \rightarrow \mathbb{R}$ by $p_{i, \gamma}(x):=$ $\widetilde{\Pi}_{i, \gamma}(x, \ldots, x)$. Then, there exists a constant $C>0$ such that for every $i \in \mathbb{N}_{0}$ and $\gamma>0$ holds $\operatorname{size}\left(p_{i, \gamma}\right) \leq C(1+i \log (i / \gamma))$ and

$$
\sup _{|x| \leq 1}\left|p_{i, \gamma}(x)-x^{i}\right| \leq \gamma
$$

Fix $n \in \mathbb{N}, \kappa>1$ and set $g(\delta):=\sum_{\|\boldsymbol{\nu}\|_{l} \infty} t_{\boldsymbol{\nu}} \delta^{\boldsymbol{\nu}}$, where $t_{\boldsymbol{\nu}}=\partial^{\boldsymbol{\nu}} f(0) / \boldsymbol{\nu}$. We have the bound (33), i.e. $\sup _{|\delta| \leq r}|f(\delta)-g(\delta)| \leq C_{f, r, \kappa} \exp (-\kappa n)$. We now construct a neural network approximation $\tilde{g}$ to $g$. We define

$$
\tilde{g}(\delta):=\sum_{\|\boldsymbol{\nu}\|_{l} \infty \leq n} t_{\boldsymbol{\nu}} \tilde{\prod}_{K, \gamma}\left(p_{\nu_{1}, \gamma}\left(\delta_{1}\right), \ldots, p_{\nu_{K}, \gamma}\left(\delta_{K}\right)\right)
$$

and fix here and throughout the rest of this proof

$$
\gamma:=\exp (-\kappa n) \leq 1
$$

Since each $p_{\nu_{j}, \gamma}\left(\delta_{j}\right)$ is the realization of a ReLU network, also $\tilde{g}$ is the realization of ReLU network.

Step 2. We bound the error $\sup _{\delta \in[-1,1]^{K}}|g(\delta)-\tilde{g}(\delta)|$. First, note that for any $\delta \in[-1,1]^{K}$ holds $\left|\prod_{j=1}^{K} p_{\nu_{j}, \gamma}\left(\delta_{j}\right)\right| \leq 2^{K}$ since $\left|p_{\nu_{j}, \gamma}\left(\delta_{j}\right)-\delta_{j}^{\nu_{j}}\right| \leq \gamma \leq 1$ for all $j$. Thus for any $\delta \in[-1,1]^{K}$ by $(51)$

$$
\begin{aligned}
|g(\delta)-\tilde{g}(\delta)| & \leq \sum_{\|\boldsymbol{\nu}\|_{l} \infty \leq n}\left|t_{\boldsymbol{\nu}}\right|\left|\prod_{j=1}^{K} \delta_{j}^{\nu_{j}}-\tilde{\prod}_{K, \gamma}\left(p_{\nu_{1}, \gamma}\left(\delta_{1}\right), \ldots, p_{\nu_{K}, \gamma}\left(\delta_{K}\right)\right)\right| \\
& \leq \sum_{\|\boldsymbol{\nu}\|_{l} \infty \leq n}\left|t_{\boldsymbol{\nu}}\right|\left(\gamma+\left|\prod_{j=1}^{K} \delta_{j}^{\nu_{j}}-\prod_{j=1}^{K} p_{\nu_{j}, \gamma}\left(\delta_{j}\right)\right|\right) \\
& \leq \sum_{\|\boldsymbol{\nu}\|_{l} \infty \leq n}\left|t_{\boldsymbol{\nu}}\right|\left(\gamma+\sum_{i=1}^{K}\left|\prod_{j=1}^{i-1} \delta_{j}^{\nu_{j}}\right|\left|\delta_{i}^{\nu_{i}}-p_{\nu_{i}, \gamma}\left(\delta_{i}\right)\right|\left|\prod_{j=i+1}^{K} p_{\nu_{j}, \gamma}\left(\delta_{j}\right)\right|\right) \\
& \leq \sum_{\|\boldsymbol{\nu}\|_{l} \infty \leq n}\left|t_{\boldsymbol{\nu}}\right|\left(\gamma+K 2^{K} \gamma\right)\left|\leq \gamma\left(1+K 2^{K}\right) \sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{N}}\right| t_{\boldsymbol{\nu}} \mid
\end{aligned}
$$

Since $f$ is an entire function, it holds $\sum_{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}}\left|t_{\boldsymbol{\nu}}\right|<\infty$. This can be shown using bounds on $t_{\boldsymbol{\nu}}$ as for instance provided in the proof of Theorem 5.1 (cf. (46)). Hence we have shown $\sup _{\delta \in[-1,1]^{K}}|g(\delta)-\tilde{g}(\delta)| \leq C \gamma=C \exp (-\kappa n)$.

Step 3. We estimate the size and depth of (one realization of) the network $\tilde{g}$.
Whereas it is easy to see that $\tilde{g}$ is the realization of a neural network, there are different ways to construct such a network (i.e. having different network architecture). In order to provide one, we first consider a network $F: \mathbb{R}^{K} \rightarrow \mathbb{R}^{K n}$ which takes as input $\delta \in \mathbb{R}^{K}$ and computes in parallel the $K n$ dimensional output $\left(p_{i, \gamma}\left(\delta_{j}\right)\right)_{i=1, \ldots, n ; j=1, \ldots, K}$. For each $i=1, \ldots, n$, as stated above,

$$
\operatorname{size}\left(p_{i, \gamma}\right) \leq C(1+i \log (i / \gamma)), \quad \operatorname{depth}\left(p_{i, \gamma}\right) \leq C(1+\log (i) \log (i / \gamma))
$$

Concatenating $p_{i, \gamma}$ with $O(\log (n) \log (n / \gamma)-\log (i) \log (i / \gamma))$ times the one layer identity network $x=\sigma(x)-\sigma(-x)$ (where $\sigma(x)=\max \{0, x\}$ is the ReLU), we find that $F$ can be realized by a network of size $C K n(1+n \log (n / \gamma))$ and depth $C(1+\log (n) \log (n / \gamma))$. Using $\gamma=\exp (-\kappa n)$ we get

$$
\begin{gathered}
\operatorname{size}(F) \leq C K n(1+n \log (n / \gamma)) \leq C\left(1+n^{3}\right) \\
\operatorname{depth}(F) \leq C(1+\log (n) \log (n / \gamma)) \leq C(1+n \log (n))
\end{gathered}
$$

for a constant $C$ depending on $K$ and $\kappa$.
Next, we concatenate the NN $F$ with a NN $G$ expressing $\tilde{g}$ in (52) given the output $\left(p_{i, \gamma}\left(\delta_{j}\right)\right)_{i=1, \ldots, n ; j=1, \ldots, K}$ of $F$. By (52), the size of this second part of the network can be bounded by $(n+1)^{K} \operatorname{size}\left(\prod_{K, \gamma}\right)+(n+1)^{K}$, where the last $(n+1)^{K}$ stems from the summation

over the set $\left\{\boldsymbol{\nu} \in \mathbb{N}_{0}^{K}:\|\boldsymbol{\nu}\|_{\ell^{\infty}} \leq n\right\}$ in (52), which has cardinality $(1+n)^{K}$. Hence (since $\gamma=\exp (-\kappa n))$

$$
\operatorname{size}(G) \leq(n+1)^{K} \operatorname{size}\left(\prod_{K, \gamma}\right)+(n+1)^{K} \leq C\left(1+K \log (K / \gamma)\right)(n+1)^{K} \leq \widetilde{C}(n+1)^{K+1}
$$

for constants $C, \widetilde{C}$ depending on $K$ and $\kappa$. For the depth of $G$ we obtain $1+\operatorname{depth}\left(\prod_{K, \gamma}\right) \leq$ $C\left(1+\log (K) \log (K / \gamma)\right) \leq \widetilde{C}(n+1)$ as an upper bound. In total

$$
\operatorname{size}(\tilde{g})=\operatorname{size}(G \circ F) \leq C(n+1)^{K+1}+n^{3} \leq C(n+1)^{K+1}
$$

is an upper bound of the complete network $G \circ F: \mathbb{R}^{K} \rightarrow \mathbb{R}$ realizing $\tilde{g}$ (here we used that $K \geq 2$ ). Together with the previous two steps we arrive at

$$
\sup _{\delta \in[-1,1]^{K}}|f(\delta)-\tilde{g}(\delta)| \leq \sup _{|\delta| \leq 1}|f(\delta)-g(\delta)|+\sup _{|\delta| \leq 1}|g(\delta)-\tilde{g}(\delta)| \leq C \exp (-\kappa n)
$$

Finally, depth $(\tilde{g}) \leq 1+\operatorname{depth}(F)+\operatorname{depth}(G) \leq C(1+n \log (n))$.
Step 4. We show the theorem in case $K=1$. Fix again $n \in \mathbb{N}, \kappa>0$ and for $\delta \in[-1,1]$ set $g(\delta)=\sum_{j=0}^{n} t_{j} \delta^{j}$ where $t_{j}=g^{(j)}(0) / j$ !. By Theorem 5.1 there exists $C_{\kappa, f, 1}$ such that $\sup _{|\delta| \leq 1}|f(\delta)-g(\delta)| \leq C_{\kappa, f, 1} \exp (-\kappa n)$.

Now we approximate $g$ by a neural network $\tilde{f}_{n}$ up to the error $\gamma:=\exp (-\kappa n)$. First, since $f: \mathbb{C} \rightarrow \mathbb{C}$ is an entire function, we have $C_{0}:=\sum_{j \in \mathbb{N}_{0}}\left|t_{j}\right|<\infty$. By [24, Proposition 4.2], there exists a neural network $\tilde{f}_{n}:[-1,1] \rightarrow \mathbb{R}$ such that $\sup _{|\delta| \leq 1}\left|g(\delta)-\tilde{f}_{n}(\delta)\right| \leq \gamma, \operatorname{size}\left(\tilde{f}_{n}\right) \leq$ $C\left(1+n \log \left(C_{0} / \gamma\right)+n \log (n)\right)$ and $\operatorname{depth}\left(\tilde{f}_{n}\right) \leq C\left((1+\log (n)) \log \left(C_{0} / \delta\right)+\log (n)^{3}\right)$ with $C$ independent of $n$ and $\gamma$. With $\gamma=\exp (-\kappa n)$ there exists a constant $C>0$ such that for every $n \in \mathbb{N}$ holds $\operatorname{size}\left(\tilde{f}_{n}\right) \leq C\left(1+n^{2}\right)$ and $\operatorname{depth}\left(\tilde{f}_{n}\right) \leq C(1+n \log (n))$. This completes the proof of Theorem 5.4.

# C ReLU Neural Network Approximation of $x \mapsto 1 / x$ 

The approximation of rational functions by ReLU NNs is studied in [33]. In the particular approximation of the map $[x \mapsto 1 / x]$, we apply a different proof technique. We first construct a sequence of certain variable degree, free-knot continuous splines with exponential convergence rate bounds. We re-express these spline approximations subsequently by a corresponding sequence of deep ReLU NNs, with exponential error bounds. The following lemma should be compared to [33, Lemma 3.5] in the approximation of the mapping $[x \mapsto 1 / x]$ on an interval $[a, 1]$ for $0<a \leq 1 / 2$, where possibly $a$ is close to zero. Specifically, to achieve an accuracy $0<\varepsilon<1$ required in $\left[33\right.$, Lemma 3.5] ${ }^{1} \mathrm{a} \operatorname{ReLU} \mathrm{NN}$ with size $\mathrm{O}\left([\log (1 / a)]^{4}[\log (1 / \varepsilon)]^{3}\right)$. In the following lemma, we construct a ReLU NN with size $\mathrm{O}\left([\log (1 / a)]\left[[\log (1 / \varepsilon)]^{2}+[\log (1 / a)]^{2}\right]\right)$ that achieves an accuracy $0<\varepsilon<1$.

Lemma C. 1 Let $0<a<b<\infty$. There exists $\kappa>0$ and constants $C, C_{1}>0$ that are independent of $a, b$ such that for every $n \in \mathbb{N}$, there exists a ReLU NN $\tilde{f}_{n}$ such that

$$
\sup _{x \in[a, b]}\left|\frac{1}{x}-\tilde{f}_{n}(x)\right| \leq C \frac{[\log (b / a)]}{a}\left(1+\frac{1}{b-a}\right) \exp \left(-\frac{\kappa}{\sqrt{[\log (b / a)]}} n\right)
$$

Furthermore, it holds that depth $\left(\tilde{f}_{n}\right) \leq C_{1}\left(1+n \log (n)+\log ^{3}(n)\right)$ and size $\left(\tilde{f}_{n}\right) \leq C_{1}[1+$ $\left.\left.n^{2}(\log (n)+\log (\sqrt{[\log (b / a)]})\right)\right]$

[^0]
[^0]:    ${ }^{1}$ The words "size" and "depth" in the statement of [33, Lemma 3.5] should be interchanged.

Proof: The proof is structured in two steps.
In the first step, we construct a polynomial spline approximation and in the second step, we apply approximation error bounds of ReLU NNs for splines, see [24].

The interval $J:=[a, b]$ is decomposed into $J=\bigcup_{i=1}^{L} J_{i}$, where $J_{i}=\left[a 2^{i-1}, a 2^{i}\right], i=$ $1, \ldots, L-1$, and $J_{L}=\left[a 2^{L-1}, b\right]$ for $L=\left[\log _{2}(b / a)\right]$. Note that $\operatorname{dist}\left(0, J_{i}\right) /\left|J_{i}\right|=1, i=$ $1, \ldots, L-1$, and $\operatorname{dist}\left(0, J_{L}\right) /\left|J_{L}\right| \geq 1$.

Let $i=1, \ldots, L-1$ be arbitrary. Let $F_{i}: J_{i} \mapsto[-1,1]$ be the unique affine bijection with affine inverse $F_{i}^{-1}:[-1,1] \mapsto J_{i}$. Specifically, $F_{i}(x)=x /\left(a 2^{i-2}\right)-3$ and $F_{i}^{-1}(x)=$ $a\left(2^{i-2} x+3 \cdot 2^{i-2}\right)$. Note that the zero of $F_{i}^{-1}$ is at $x=-3$ and neither depends on $i$ nor on $a$. Thus, the map $\left[z \mapsto 1 / F_{i}^{-1}(z)\right]$ is holomorphic on the ellipse $\mathcal{E}_{\varrho}=\left\{\frac{z+z^{-1}}{2}: z \in \mathbb{C},|z| \leq \varrho\right\}$ for any $\varrho \in\left(1, \varrho_{0}\right)$ with $\varrho_{0}:=3+2 \sqrt{2}$. It is easy to see that $\left|F_{i}^{-1}(z)\right| \geq a 2^{i-2}\left(3-\frac{\varrho+\varrho^{-1}}{2}\right)$ for every $z \in \mathcal{E}_{\varrho}$. By [25, Theorem 3.5] for every $0<\beta<\log (\varrho)<\log (3)$ there exists a constant $C>0$ that neither depends on $a, b$ nor on $i$ such that for every $p \in \mathbb{N}$,

$$
\sup _{x \in[-1,1]}\left|\frac{1}{F_{i}^{-1}(x)}-P_{p}^{i}(x)\right| \leq C \frac{1}{a 2^{i-2}(3-\varrho)} \exp (-\beta p), \quad i=1, \ldots, L-1
$$

where $P_{p}^{i}$ is the $p$ th order Legendre expansion of the mapping $\left[x \mapsto 1 /\left(F_{i}^{-1}(x)\right)\right]$; we also used that $\varrho>\left(\varrho+\varrho^{-1}\right) / 2$. Denote by $\bar{P}_{p}^{i}$ the Langrange interpolant of the mapping $[[-1,1] \ni x \mapsto$ $\left.1 /\left(F_{i}^{-1}(x)\right)\right]$ using Gauss-Lobatto interpolation points. In particular, $\bar{P}_{p}^{i}(-1)=1 /\left(F_{i}^{-1}(-1)\right)$ and $\bar{P}_{p}^{i}(1)=1 /\left(F_{i}^{-1}(1)\right)$. Let us denote this Langrange interpolation operator by $I_{p}$. By [4, Equation (1.14)], the Lebesgue constant of the first $p+1$ Gauss-Lobatto points in $[-1,1]$ satisfies the bound (suboptimal, see [32], but sufficient for our purposes),

$$
\forall p \geq 1, \forall f \in C^{0}([-1,1]): \quad\left\|I_{p} f\right\|_{C^{0}([-1,1])} \leq 5(p+1)^{2} \log (p+1)\|f\|_{C^{0}([-1,1])}
$$

Since $I_{p}\left(P_{p}^{i}\right)=P_{p}^{i}$, the estimate of the Lebesgue constant in (57) implies with (56) for every $i=1, \ldots, L-1$

$$
\begin{aligned}
\sup _{x \in[-1,1]}\left|\frac{1}{F_{i}^{-1}(x)}-\bar{P}_{p}^{i}(x)\right| & \leq \sup _{x \in[-1,1]}\left|\bar{P}_{p}^{i}(x)-P_{p}^{i}(x)\right|+\sup _{x \in[-1,1]}\left|\frac{1}{F_{i}^{-1}(x)}-P_{p}^{i}(x)\right| \\
& \leq C \frac{5(p+2)^{2} \log (p+1)}{a 2^{i-2}(3-\varrho)} \exp (-\beta p)
\end{aligned}
$$

The case $i=L$ follows similarly. Define the spline interpolant $\mathcal{I}_{p, L}:[a, b] \mapsto \mathbb{R}$ by $\mathcal{I}_{p, L}(x):=$ $\bar{P}_{p}^{i}\left(F_{i}(x)\right), x \in J_{i}, i=1, \ldots, L$. Note that $\mathcal{I}_{p, L}$ is continuous and restricted to $J_{i}$ a polynomial of degree $p$ and has in total $(p+1) L$ degrees of freedom. By the estimate (58), for every $0<\beta<\log (3)$ there exists a constant $C>0$ that neither depends on $a$ nor on $b$ such that for every $p \in \mathbb{N}$

$$
\sup _{x \in[a, b]}\left|\frac{1}{x}-\mathcal{I}_{p, L}(x)\right| \leq \max _{i=1, \ldots, L} \sup _{x \in[-1,1]}\left|\frac{1}{F_{i}^{-1}(x)}-\bar{P}_{p}^{i}(x)\right| \leq \frac{C}{a} \exp (-\beta p)
$$

In the second step, we approximate the continuous piecewise polynomial $\mathcal{I}_{p, L}$ by a ReLU NN. We shall bound the $W^{1,1}([a, b])$-norm of $\mathcal{I}_{p, L}$. By the Markov inequality, cf. [13, Equation (1.16) and p. 736], for every polynomial $g$ of degree $p$,

$$
\sup _{x \in J_{i}}\left|g^{\prime}(x)\right| \leq \frac{4 e p^{2}}{\left|J_{i}\right|} \sup _{x \in J_{i}}|g(x)|
$$

Thus, by the upper bound on the Lebesgue constant of the Gauss-Lobatto points in (57)

$$
\begin{aligned}
\left\|\mathcal{I}_{p, L}\right\|_{W^{1,1}([a, b])} & \leq \sum_{i=1}^{L}\left|J_{i}\right| \sup _{x \in J_{i}}\left|\left(\mathcal{I}_{p, L}\right)^{\prime}(x)\right| \\
& \leq 4 e p^{2} \sum_{i=1}^{L} \sup _{x \in J_{i}}\left|\bar{P}_{p}^{i}(x)\right| \leq 20 e p^{2}(p+2)^{2} \log (p+1) \frac{[\log (b / a)]}{a}
\end{aligned}
$$

Continuous polynomial splines may be approximated by ReLU NNs due to [24, Proposition 5.1]. Specifically, by [24, Proposition 5.1] and by (60), there exists $\kappa>0$ and constants, $C, C_{1}>0\left(\kappa, C, C_{1}\right.$ are independent of $a, b$ ) such that for every $n \in \mathbb{N}$, there exists a ReLU NN $\tilde{f}_{n}$ such that

$$
\left\|\mathcal{I}_{p, L}-\tilde{f}_{n}\right\|_{W^{1,1}([a, b])} \leq \frac{C\lceil\log (b / a)\rceil}{a} \exp \left(-\frac{\kappa n}{\sqrt{\lceil\log (b / a)\rceil}}\right)
$$

Moreover, depth $\left(\tilde{f}_{n}\right) \leq C_{1}(1+p \log (p)+n \log (p)+\log ^{3}(p))$ and $\operatorname{size}\left(\tilde{f}_{n}\right) \leq C_{1}(\lceil\log (b / a)\rceil p^{2}+$ $n \sqrt{\lceil\log (b / a)\rceil} p \log (p))$. We recall that by the Sobolev embedding, cf. [3, Theorems 8.6 and 8.8], there exists a constant $C>0$ (independent of $a, b$ ) such that for every $g \in W^{1,1}([a, b])$,

$$
\|g\|_{L^{\infty}([a, b])} \leq C\left(1+\frac{1}{b-a}\right)\|g\|_{W^{1,1}([a, b])}
$$

The asserted error estimate of this lemma follows by (59) and by (61) and by (62) using the triangle inequality, where we choose $p=\lceil\kappa n /(\beta \sqrt{\lceil\log (b / a)\rceil})\rceil$. Thus, by this choice, depth $\left(\tilde{f}_{n}\right) \leq C_{1}\left(1+n \log (n)+\log ^{3}(n)\right)$ and $\operatorname{size}\left(\tilde{f}_{n}\right) \leq C_{1}\left[1+n^{2}(\log (n)+\log (\sqrt{\lceil\log (b / a)\rceil})\right)$ ].

# References 

[1] K. Abraham and R. Nickl. On statistical Calderón problems. arXiv e-prints, 2019. arXiv:1906.03486.
[2] A. Bonito, R. A. DeVore, and R. H. Nochetto. Adaptive finite element methods for elliptic problems with discontinuous coefficients. SIAM J. Numer. Anal., 51(6):3106-3134, 2013.
[3] H. Brezis. Functional analysis, Sobolev spaces and partial differential equations. Universitext. Springer, New York, 2011.
[4] M. A. Chkifa. On the Lebesgue constant of Leja sequences for the complex unit disk and of their real projection. J. Approx. Theory, 166:176-200, 2013.
[5] M. Dashti, S. Harris, and A. Stuart. Besov priors for Bayesian inverse problems. Inverse Probl. Imaging, 6(2):183-200, 2012.
[6] M. Dashti and A. M. Stuart. The Bayesian approach to inverse problems. In Handbook of uncertainty quantification. Vol. 1, 2, 3, pages 311-428. Springer, Cham, 2017.
[7] J. Dick, R. N. Gantner, Q. T. L. Gia, and C. Schwab. Multilevel higher-order quasi-Monte Carlo Bayesian estimation. Math. Models Methods Appl. Sci., 27(5):953-995, 2017.
[8] J. Dick, R. N. Gantner, Q. T. L. Gia, and C. Schwab. Higher order quasi-Monte Carlo integration for Bayesian PDE inversion. Comput. Math. Appl., 77(1):144-172, 2019.

[9] J. Dick, F. Y. Kuo, Q. T. LeGia, and C. Schwab. Multilevel higher order QMC PetrovGalerkin discretization for affine parametric operator equations. SIAM J. Numer. Anal., $54(4): 2541-2568,2016$.
[10] L. Herrmann, M. Keller, and C. Schwab. Quasi-Monte Carlo Bayesian estimation under Besov priors in elliptic inverse problems. Technical Report 2019-41, Seminar for Applied Mathematics, ETH Zürich, Switzerland, 2019.
[11] L. Herrmann, C. Schwab, and J. Zech. Deep ReLU neural network generalization rates for data-to-QoI maps in Bayesian PDE inversion. Technical report, 2019. in preparation.
[12] L. Herrmann and Ch. Schwab. Multilevel quasi-Monte Carlo uncertainty quantification for advection-diffusion-reaction. Technical Report 2019-06, Seminar for Applied Mathematics, ETH Zürich, 2019. To appear in Monte Carlo and Quasi-Monte Carlo Methods - MCQMC, Rennes, France, July 2018.
[13] E. Hille, G. Szegö, and J. D. Tamarkin. On some generalizations of a theorem of A. Markoff. Duke Math. J., 3(4):729-739, 1937.
[14] V. H. Hoang, J. H. Quek, and C. Schwab. Analysis of multilevel MCMC-FEM for Bayesian inversion of log-normal diffusions. Technical Report 2019-05 (revised), Seminar for Applied Mathematics, ETH Zürich, 2019. (To appear in Inverse Problems (2020)).
[15] V. H. Hoang, C. Schwab, and A. Stuart. Complexity analysis of accelerated MCMC methods for Bayesian inversion. Inverse Problems, 29(8):085010, 37, 2013.
[16] H. Holden and N. H. Risebro. Front tracking for hyperbolic conservation laws, volume 152 of Applied Mathematical Sciences. Springer-Verlag, New York, 2002.
[17] L. Hörmander. An introduction to complex analysis in several variables, volume 7 of NorthHolland Mathematical Library. North-Holland Publishing Co., Amsterdam, third edition, 1990.
[18] J. Hüsler, R. Y. Liu, and K. Singh. A formula for the tail probability of a multivariate normal distribution and its applications. J. Multivariate Anal., 82(2):422-430, 2002.
[19] M. A. Iglesias, Y. Lu, and A. Stuart. A Bayesian level set method for geometric inverse problems. Interfaces Free Bound., 18(2):181-217, 2016.
[20] D. Jerison and C. E. Kenig. The inhomogeneous Dirichlet problem in Lipschitz domains. J. Funct. Anal., 130(1):161-219, 1995.
[21] M. Lassas, E. Saksman, and S. Siltanen. Discretization-invariant Bayesian inversion and Besov space priors. Inverse Probl. Imaging, 3(1):87-122, 2009.
[22] K. Lye, S. Mishra, and R. Molinaro. A multi-level procedure for enhancing accuracy of machine learning algorithms. Technical Report 2019-54, Seminar for Applied Mathematics, ETH Zürich, Switzerland, 2019.
[23] L. Mattner. Complex differentiation under the integral. Nieuw Arch. Wiskd. (5), 2(1):3235, 2001.
[24] J. A. A. Opschoor, P. C. Petersen, and C. Schwab. Deep ReLU networks and high-order finite element methods. Technical Report 2019-07, Seminar for Applied Mathematics, ETH Zürich, Switzerland, 2019. to appear in Analysis and Applications.

[25] J. A. A. Opschoor, C. Schwab, and J. Zech. Exponential ReLU DNN expression of holomorphic maps in high dimension. Technical Report 2019-35, Seminar for Applied Mathematics, ETH Zürich, 2019.
[26] P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep ReLU neural networks. Neural Netw., 108:296 - 330, 2018.
[27] A. Pinkus. Approximation theory of the MLP model in neural networks. In Acta numerica, 1999, volume 8 of Acta Numer., pages 143-195. Cambridge Univ. Press, Cambridge, 1999.
[28] W. Rudin. Functional analysis. International Series in Pure and Applied Mathematics. McGraw-Hill, Inc., New York, second edition, 1991.
[29] C. Schillings and C. Schwab. Sparse, adaptive Smolyak quadratures for Bayesian inverse problems. Inverse Problems, 29(6), 2013.
[30] C. Schwab and S. Tokareva. High order approximation of probabilistic shock profiles in hyperbolic conservation laws with uncertain initial data. ESAIM Math. Model. Numer. Anal., 47(3):807-835, 2013.
[31] C. Schwab and J. Zech. Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in UQ. Analysis and Applications, Singapore, $17(1): 19-55,2019$.
[32] B. Sündermann. Lebesgue constants in Lagrangian interpolation at the Fekete points. Mitt. Math. Ges. Hamburg, 11(2):204-211, 1983.
[33] M. Telgarsky. Neural networks and rational functions. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3387-3393. PMLR, International Convention Centre, Sydney, Australia, 06-11 Aug 2017.
[34] D. Yatotsky. Error bounds for approximations with deep ReLU networks. Neural Netw., $94: 103-114,2017$.
[35] D. Yatotsky and A. Zhevnerchuk. The phase diagram of approximation rates for deep neural networks, 2019. arXiv:1906.09477.
[36] K. Yosida. Functional analysis. Classics in Mathematics. Springer-Verlag, Berlin, 1995. Reprint of the sixth (1980) edition.
[37] J. Zech, D. Dung, and C. Schwab. Multilevel approximation of parametric and stochastic PDEs. Math. Models Methods Appl. Sci., 29(9):1753-1817, 2019.
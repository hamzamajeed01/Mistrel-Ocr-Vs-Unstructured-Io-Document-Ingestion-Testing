['Document 1: 22Vol100No3.pdf\nJournal of Theoretical and Applied Information Technology\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\n\nISSN: 1992-8645\nWww.jatit.org\nE-ISSN: 1817-3195\nAN OVERVIEW OF CLOUD COMPUTING FOR THE ADVANCEMENT OF THE E-LEARNING PROCESS\nASHRAF ALI\nFaculty of Computer Studies, Arab Open University, Kingdom of Bahrain\nE-mail: ashraf.ali@aou.org.bh\nABSTRACT\nAs an aid in the teaching-learning process, online communications systems are used to facilitate e-learning, a form of virtualized computing and distant learning. The rise of E-learning platforms emerged drastically in the past two years. Data mining for education information processing uses facts generated from internet databases to enhance the educational learning paradigm for educational purposes when the learning process is computerized. Cloud computing is a suitable platform for supporting e-learning solutions. It can be automatically altered by providing a scalable solution for transforming computer resource consumption in the long run. It also makes things simpler to use data mining techniques in a distributed environment when interacting with massive e-learning datasets. A summary of the current state of cloud computing is provided in the study and examples of infrastructure explicitly designed for such a system. In addition, it also discusses examples of cloud computing and e-learning methodologies.\nKeywords: E-Learning, Cloud Computing, Virtual Learning, SaaS, PaaS, IaaS\n1. INTRODUCTION\nE-Learning emerged due to the widespread use of the internet and other digital communication systems and distance education [11]. It makes use of multiple formats and functions that might best aid classroom instruction. These include Virtual instruction, emails and web links, discussion boards, and other learning platforms, among other things. As a result of the online integration of students, content producers, and professionals, the learning experience is better handled. Learning with web-based tools has many benefits, the most prominent of which are the tasks\' consistency and recurrence, adaptability, accessibility, and easier access [16]. E-learning or virtual teaching platforms are becoming increasingly popular in information technology (IT), particularly after the outbreak of Covid-19 and digital advancement. Different educational levels have associated efforts, such as Massive Open Online Courses (MOOCs), Blackboard, Desire to Learn (D2L), and the Virtual Learning Center at various universities, implemented as E-learning format globally [21,22]. Compared to the conventional attendance class, virtual programs, fully endorsed by the e-learning paradigm, have an obvious optimal learning environment, a notably greater frequency for those who can acquire their material online [6, 13,20]. These proportions have a lot of consequences; for\nexample, the infrastructure requirements to provide a concurrent service for that many learners far surpass the capability of traditional web application users. Moreover, the need for instructional resources often fluctuates rapidly and dynamically, with significant activity spikes. To respond to requests without affecting other system services, a much more advanced infrastructure will be needed than what is normally required for the learning institution to function normally during these periods. Providing services based on usage and only paying as the pay-per-use policy for resources that are used is an option. Cloud computing technology provides the solution to these problems. Cloud computing was first proposed to reduce computational costs while enhancing system reliability and availability [1, 30]. These goals have since evolved into those of cloud computing. Nevertheless, there is a distinction between the two regarding how the tasks are calculated in each setting [40]. In terms of technical resources, a computing grid is more stable, and it is primarily designed to maximize the performance of a computer system. On the other side, Cloud computing aims to provide transparent mobility while allowing users to acquire various services rather than familiar with the basic infrastructure. It does not have a limited range of services, including hosting services and word processing [37]. It\'s important to note that one of the foundations of\n847\nISSN: 1992-8645\nJournal of Theoretical and Applied Information Technology\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\nWww.jatit.org\n5 S,\nE-ISSN: 1817-3195\ncloud computing is Service Oriented Architecture (SOA). There are many dispersed organizational computing barriers that this type of technology is intended to help programmers to transcend, such as application integration, concurrency control, and security protocols, as well as numerous different systems and protocols and the use of hardware and software to which we might have direct exposure, and existing data systems [24, 39]. All of a cloud platform\'s functions are made accessible in a way that hides the location and other technical aspects of the computing infrastructure from users [45]. In comparison to other competing technologies, the advantages of this new computing paradigm are plain to see. Users don\'t have to invest money on new hardware to use the application because cloud software vendors attempt to deliver comparable or better capabilities and functions than if the applications were loaded locally on end-user machines [28]. This storage capacity and computing initiatives help corporations to get their software fully operational faster, with a lesser provision of services from the IT division because it instantaneously intends the business needs by interactively assigning IT assets (servers) based on the computation complexity in virtual environments [14]. Massive e-learning environments, such as those discussed earlier, also produce large archives of student participation with peers and teachers. Significant data is stored in these systems that haven\'t been explicitly declared. You\'ll need to use data mining algorithms [25]. Educational data mining (EDM) is a technique that helps both instructors and learners enhance teaching and learning in this situation [2]. The creation of novel strategies for examining the data created by the aforementioned current education system activity is the focus of this discipline. This method\'s ultimate goal is to understand student performance better and create protocols and resources that will make learning more engaging and easier. There are computer-based tutoring systems that are specifically developed to assist in the teaching and learning process and directly link to this approach. These are sophisticated programs that support students learning by monitoring their performance and providing them with feedback. An instructional model interacts with the EDM process, which extends and refines the knowledge it has. Considering the size and capacity expansion of computer capabilities (solid space, ram, and CPUs), cloud hosting is a sequence for adopting data mining algorithms and implementing them towards every database [15, 42]. Several more data mining\nmethods, on the other hand, aren\'t very scalable.\nThis is a topic that is becoming extremely relevant, and scholars and businesses alike are taking notice.\nDue to the Covid-19 pandemic educational institutions around the globe moving to either use blended learning or fully E-learning. The major challenge is to deliver secure and adequate resources to support the E-learning process. This research aims to review cloud computing services for E-learning to enable the educator to utilize the benefits of cloud services such as scalability, flexibility, and security to support and enhance the E-learning process. The remainder of this paper is organized as follows. Section 2 introduces the fundamental notions of cloud computing, section 3 discusses E-learning tasks and cloud computing, section 4 describes the perspective challenges of e- learning and cloud computing. Finally, section 5 concludes the paper.\n2. FUNDAMENTAL NOTIONS OF CLOUD COMPUTING\nAll the analysis in the preceding sections are the review of the cloud computing. The review is based on the qualitative analysis, which allows researchers to present the notion in elaborative way. A literature review examines publications, academic papers, and any other source materials pertaining to a particular issue, area of investigation, or concept, and provides an overview, synopsis, and analysis of a research subject in order to address the research. Cloud computing is an emerging approach in which different resources and services such as data storage, servers, databases, networking, and software are delivered via the web. This brings us to the conception of SOA [36], a framework for integration consisting of a combination of a rational and technology framework to assist and incorporate all range of facilities. In essence, service in the context of cloud computing is a function that has been wrapped in a somewhat form that it could be mechanized and delivered to customers in a standardized and structured way. Any element, from those adjacent to equipment, such as storage capacity or processing time, to software elements targeted at verifying a user or handling mail, database administration, or regulating the use of the operating system, can be regarded as a service.\nEssentially, the cloud computing philosophy suggests a shift in how challenges are tackled through technology [38]. Using and combining services is the basis for application\n848\nJournal of Theoretical and Applied Information Technology\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\n5 SA\nISSN: 1992-8645\nWww.jatit.org\nE-ISSN: 1817-3195\ndesign. Instead of relying on the concept of processor algorithms, as with more conventional methods, such as distributed systems, the provision of functioning depends on the use and integration of services. In other words, this has benefits in terms of adaptability, dependability, scalability, and so on. For illustration, more instances of a specific service could be launched so that the application\'s response time stays appropriate for consumers during a spike of resource requirements due to a rise in customers or a rise in computational load.\nAs a consequence of a decline in demand, available resources should be made available. Everything is done sensibly to the customer. Among the most notable cloud computing are its minimal connection, high degree of interoperability, and protocols that separate the provider\'s execution and environment [41]. It\'s not uncommon for an SOA to divide its operations into levels or layers (rather than in precise boundaries). Some components make usage services rendered by lower tiers to allow other capabilities to higher ones. Aside from that, these divisions could have multiple corporate frameworks, architectural designs, and so on. According to the type of arrangement being offered, there are generally three basic types of layers together, which form what is described as According to the kind of arrangement being offered. There are generally three basic types of coatings together, which include what described as a cloud-based storage system that provides data storage depending on "files" or "blocks." Cloud computing is a collection of registers, columns, or entities that offer services and complete execution services are available by a compute cloud. Mega projects benefiting from the cloud computing model [35]. Many scientific and business applications are well-known burdened by heavy computational requirements. A constant data is flow necessitates an elevated communication link since it involves handling enormous amounts of data contained in stable systems, which indicates a high amount of storage space.\nService-oriented systems can be grouped into a variety of areas. The complexity degree that these systems provide to the system user is a commonly used parameter for grouping them. As illustrated in Figure 1, this method frequently distinguishes between three distinct levels.\nInfrastructure as a Service (IaaS) provides infrastructure, i.e., data centers, network technology, memory, or computing, and essential\ncomponents like computer systems and abstraction of hardware elements [26]. If we compare the IaaS to a mono computer platform, the software and computer program together represent the IaaS. The operating system manages the system resources and makes them accessible. Rather than purchasing and establishing its entire computing infrastructure, the IaaS customer leases computational capabilities from the IaaS provider. Since services are typically priced based on actual usage, the customer only charges for whatever they consume. Because of cloud computing\'s dynamic scalability, they utilize (and spend for) fewer resources when the workload is light. Where there is a more critical requirement for help, IaaS can make them available to meet the demands of that specific customer. Most service agreements specify a maximum value that a customer may not go beyond. As an example, scholars and practitioners in the scientific community are prototypical IaaS customers. These clients can design experiments and interpret information to the degree that would not be feasible without the IaaS and the large amount of infrastructure it provides as a service. Amazon\'s Elastic Computer Cloud is one of the most popular IaaS suppliers today (EC2). Other notable IaaS providers include RackSpace, Google Compute Engine, and Windows Azure.\n                                   \nFigure 1: Layers of Cloud computing Source [7]\nThe second level, namely Platform as a service (PaaS), is a provider-provided infrastructure that includes an integrated software package with everything a development hub to construct apps at the design and delivery stages [27, 31]. PaaS providers don\'t offer infrastructure explicitly, but utilizing IaaS services provides developers with the tools they have to have an indirect connection to the IaaS infrastructure and, therefore, the architecture they require [31]. The PaaS could be regarded as a software layer,\' allowing elements for apps and\n849\nISSN: 1992-8645\nJournal of Theoretical and Applied Information Technology\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\nWww.jatit.org\n5 S,\nE-ISSN: 1817-3195\napps altogether to be produced on top of the PaaS. An interconnected developer setup or a collection of stand-alone tools will help engineers work on software glitches throughout the entire software development lifecycle. This includes everything from analyzing and modeling a challenge to designing a remedy to testing and deploying it. Similar to this, a computer language that uses several operating mechanism compilers and modules makes it possible to deploy the same application on numerous systems without having to rewrite any code. Major examples of PaaS-cloud computing services market players include “Google App Engine”, “Amazon Web Services”, “Heroku”, “OpenShift- Red Hat” etc.\nSoftware as a Service (SaaS) is the highest level in the pioneering use of cloud services when internet usage was growing in prominence [32]. Originating in the host functions of the Platform as a service, some organizations provided to everyone the applications appeared as customer interaction managements from these applications [28]. There are now numerous options available, both for businesses and private individuals as well as for education. Even though these services are delivered over the internet, which allows for geographic versatility, the direct sharing of data in this manner does not ensure its confidentiality. That\'s why VPNs are frequently used, as they enable data to be sent over the internet in an encoded file, keeping user and SaaS data safe and secure.\n3. E-LEARNING TASKS AND CLOUD COMPUTING\nE-learning systems advent expand at an exponential rate due to the suspension of on- campus classes, tremendous expansion in the number of students, instructional content, services available, and materials made accessible [21,23]. It\'s essential to select a platform that can scale to meet demand while still keeping expenses in check while optimizing resource processing, storing, and communication requirements. Cloud computing is what\'s happening here in the shape of delivery and retrieval of information and content. In contrast to previous \'traditional\' learning environments, defining the promise of SaaS applications for resilient and comprehensive distance learning may help us comprehend the advantages of cloud computing mostly on a technological and pedagogical level. Throughout terms of achieving a beneficial system for online tools and interactive services, such as teaching materials, recordings,\neducational materials, peer instruction, and so on, we ought to offer the \'road\' for supporting migration to such a model.\nMany educational institutions are now using cloud technology, and it\'s evident that it has a promising future in [19]. In many countries, namely the UK, initiatives like JISC (2012) are in place to include an education cloud with the required tools to manage data and store the data [33]. Education SaaS refers to a cloud-based e-learning system that allows users to gain the benefit of cloud computing. Due to its modest hardware requirements, it can be swiftly deployed by the end-user. Moreover, it relieves the supplier of system service and maintenance responsibility, permitting the manufacturer to focus on the most critical business while receiving free automatic updates and providing essential resources via Web 2.0.\nE-learning system architecture and cloud computing systems as part of consistency, harmony, effective resource use, and the long-term stability of the e-learning ecology from a technological standpoint in education [10]. In [29], the authors summarized the repercussions and ramifications of developing e-learning solutions in the cloud computing system. At the onset, there is a greater demand for web development abilities because the application may be accessed from anywhere, at any time. As a result, the subscriber has saved money by not paying for software, deployment, or server management. As a result, the institution will spend less money overall, have a faster deployment, and need fewer IT workers. This will be equally handy for the situations like Coivd- 19 where the moment is restricted [16]. It is appropriate for the program type education sector to pay for content peruse, making it available to more sophisticated programs and required applications. Numerous educational establishments can use a SaaS server. Scalability is built-in to the system because it is hosted on a cloud server. The software\'s performance will not deteriorate as student usage increases. To acquire the confidence of consumers and a comprehensive providing users system software, the SaaS provider needs a sophisticated level of security. The consumer data is dispersed throughout various services and therefore must be consolidated in obtaining a comprehensive picture of the business, resulting in an increased need for platforms and data integrators for education. The advantages of a cloud-based curriculum have previously been studied from a technological standpoint by specific authors. While\n850\nJournal of Theoretical and Applied Information Technology\n5 SA\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\nWww.jatit.org\nE-ISSN: 1817-3195\nIt\'s easy to see in Figure 2 that most cloud e-learning techniques use three fundamental layers: a virtualized platform on top and a cloud management system and services layer underneath that. Two computer pools are used for teaching: a C pool with a thin client and a server pool running the hypervisor, with the private cloud architecture created using vSphere. It is possible to observe and manage all of the virtual infrastructure\'s hosts and services instantaneously using a web browser. Things like efficiency and configuration can be monitored along with saving alarm information and permission settings.\nTo allow multiple operating systems, a single hardware host hypervisor is essential. A hypervisor prevents virtual machines from interfering with one another by allocating resources to each element as they are required. In this case, a hypervisor that runs directly on the underlying hardware is the better option. This layer, which serves as an interface to the outside world, provides the PaaS and SaaS cloud users\' needs. The instructional coordinators build the virtual PCs, choosing the baseline images and installing the software they\'ve chosen afterward [27]. Thus, standardized web technologies are generated for specific course projects, and learners may connect to the respective VM using the remote network. Figure 3. shows the personalized virtual model for E-Learning.\n                                \nFigure 3: Personalized E-learning Architecture. Source [17]\nThe integration of cloud technology and e- learning has received more attention from the institutions due to its high demand to continue education. Almost all the institutions of schooling deemed it to be an operative and suitable alternative for e-Learning. Nevertheless, an absence of research may provide a theoretical foundation from which a methodology could be constructed. The\n851\nISSN: 1992-8645\naffordability is the most frequently cited concern, other considerations include those highlighted for cloud use throughout the practice [33, 39, 40, 41]. It is not necessary to back up and move data between devices using a hard drive. Creating a reservoir of information means that students can keep it for as long as they desire, and it will continue to grow with them. Recovering after a crash seems to be almost entirely superfluous in this situation. There\'s essentially no information lost if the user machine fails. While working from numerous locations, students can access their files and modify them using virtualized programs that have also helped institutions implement E-Learning recently and notably during the lockdown. It offers academic organizations a minimal cost-effective alternative for their academics, staff, and students.\nData access monitoring is made simpler by the notion that just one location must be controlled rather than hundreds of computers dispersed across a larger region. Furthermore, because the cloud has a single database for all users, cybersecurity modifications can be efficiently evaluated and deployed [8]. Subsequently, even though more efforts are required to determine how cloud-related pedagogies or assessments of learning purposes [11], from a scholarly perspective, one of the advantages of the cloud is its ease of access [16], as it is mainly created to permit users to collaborate from anywhere at any given time. It can reach more learners outside the traditional teaching environment and meet their requirements. It can provide more meaningful information to a broader spectrum of students in a more comprehensive range of contexts [10]. Figure 2, shows the dimensions of cloud computing in its association to E-Learning.\n                             \nFigure 2: A glimpse of Cloud computing for E-Learning. Source [12]\nJournal of Theoretical and Applied Information Technology\n5 S,\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\nWww.jatit.org\nE-ISSN: 1817-3195\nA cloud-based system integrates the inherent advantages of cloud technology, such as cost savings, fault tolerance, and enhanced accessibility and remote connectivity into e-learning. Cloud technology benefits can be maximized with proper pre-implementation planning [3, 4, 5]. Businesses can utilize any options listed below to move from their present e-learning system to cloud-based e- learning. The process of converting an e-learning program involves several steps, including installing the operating system and middleware and implementing the server and client modules. A migration feasibility study must include user needs, existing IT infrastructure availability, and a cost/benefit analysis [44]. A system\'s monetary cost can be kept to a low by optimally mapping existing resources to the cloud tiered architecture using virtualization to reduce resource under-utilization.\nEven though connectivity and speed have improved dramatically over the previous decade to an acceptable level worldwide, a slow internet connection can significantly impede cloud-based education and e-learning. The situation is exacerbated even further when data and services are accessed from non-regional cloud datacenters. Due to this problem, users and students of cloud-based e-learning systems may be subjected to excessive delays. The cloud may not be the appropriate Platform for teaching specific topics and disciplines if students need to use specialist software or equipment and resources in physical labs [33]. Digital forensics, mainboards, physical network devices, and robotics can be considered equipment if they require a hardware dongle. It is possible to use the cloud in part for this purpose, although it may not be possible in all cases. The use of cloud power must be thoroughly investigated and studied for such topics. Tools that closely imitate the hardware environment may hold the key to this problem\'s resolution. Using resources and software from both on- and off-cloud should be part of the hybrid cloud concept.\n5. CONCLUSION\nThe overview presented in the analysis assert that using cloud services in E-learning is a nice alternative because it allows teachers to leverage cloud adaptability, flexibility, and security to represent the main framework of E-learning — instruction providing access anywhere, at any time, and from any gadget. When an efficient learning environment with specialized content is easily adaptable to today\'s educational paradigm, we can fully utilize the opportunities it presents. Increased\n852\nISSN: 1992-8645\nflexibility implicit in the cloud strategy, on the other hand, could\'ve been highlighted as a considerable advantage in producing an analytical framework and creating successful teaching techniques [34]. The drawback in this field is that few studies provide a strategic or tactical of the subject.\nConversely, the overall characteristics of the cloud are associated with social engagement and collaboratively learning pursuit in the literature [28]. In [9], the authors investigate students\' views of excellence and responsibility about various kinds of interaction within Google Docs. Instructional methods that use technology to alter and improve students\' collective experience when producing a joint assignment. Additionally, various cloud- related studies may be found for measuring the results of online models to conventional approaches [43].\n4. PERSPECTIVE CHALLENGES E- LEARNING AND CLOUD COMPUTING\nE-learning may benefit greatly from today\'s cloud computing, applications, and capabilities as a lucrative industry [4,13]. A cloud-based e-learning system can provide significant assistance in overcoming the shortcomings of conventional local physical labs and computing platforms. Nevertheless, fundamental problems and barriers must be solved before the cloud can be widely used and adopted to facilitate and promote e-learning.\nIt is essential that instructors and students undergo a learning curve and that academic institutions give IT support to make good use of cloud computing for e-learning and teaching [18, 33]. Use third-party solutions or current public or commercial cloud resources or services however you like. Along with training, the instructor should be well-versed in cloud capabilities and consult with the university\'s IT department to establish the best cloud model for the class\'s requirements. The instructor must be taught how to set up and assign cloud resources and manage student accounts. Students must also be coached and instructed on how to access and use the cloud-based course resources. Depending on the course design and requirements, the learning curve for instructors and students might be steep or easy. Faculty in fields like computer science and related courses may have an easier time learning about and using the cloud than faculty in other areas.\nJournal of Theoretical and Applied Information Technology\n5 S,\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\nWww.jatit.org\nE-ISSN: 1817-3195\nlearning analytics for 21st century higher education: A review and synthesis. Telematics and Informatics, 37, 13-49.\n[3] Ali, A., & Alourani, A. (2021). An Investigation of Cloud Computing and E- Learning for Educational Advancement. IJCSNS, 21(11), 216-222.\n[4] Ali, A., Manzoor, D., Alouraini, A., The implementation of Government Cloud for the Services under E-Governance in the KSA. Science International Journal, 2021. 3(3): 249- 257.\n[5] Ali, A., Cloud computing adoption at higher educational institutions in the KSA Sustainable Development. International Journal of Advanced Computer Science Applications, 2020. 11(3):413-419. for and\n[6] AlKhunzain, A., & Khan, R. (2021). The Use of M-Learning: A Perspective of Learners’ Perceptions on M-Blackboard Learn.\n[7] Azam, M. G. (2019). Application of cloud computing in library management: innovation, opportunities and challenges. Int. J. Multidiscip., 4(1), 2-11.\n[8] Bhardwaj, A., & Goundar, S. (2019). A framework to define the relationship between cyber security and cloud performance. Computer Fraud & Security, 2019(2), 12-19.\n[9] Blau, I., & Caspi, A. (2009). What type of collaboration helps? Psychological ownership, perceived learning and outcome quality of collaboration using Google Docs. Paper presented at the Proceedings of the Chais conference on instructional technologies research.\n[10] Bora, U. J., & Ahmed, M. (2013). E-learning using cloud computing. International Journal of Science and Modern Engineering, 1(2), 9-12.\n[11] Clark, R. C., & Mayer, R. E. (2016). E-learning and the science of instruction: Proven guidelines for consumers and designers of multimedia learning: john Wiley & sons.\n[12] Fernandez, A., Peralta, D., Herrera, F., & Benítez, J. (2012). An overview of e-learning in cloud computing. Paper presented at the Workshop on Learning Technology for Education in Cloud (LTEC\'12).\n[13] Galić, S., Lušić, Z., & Stanivuk, T. (2020). E- learning in maritime affairs. Journal of Naval Architecture and Marine Engineering, 17(1), 38-50.\n[14] Haji, L. M., Zeebaree, S., Ahmed, O. M., Sallow, A. B., Jacksi, K., & Zeabri, R. R.\n853\nISSN: 1992-8645\nstorage, computation, and network connectivity are a few advantages of integrating an e-learning system into the cloud. Software and hardware savings should be prioritized. In contrast, it has a more incredible selection of educational programs at a lesser license cost. However, the replacement rate for student computers is reduced due to the longer machine life. These savings are boosted by the decrease in IT personnel costs associated with computer lab maintenance and software updates.\nToday\'s e-learning services and systems fall short when it comes to customizing and personalizing learning for each user. Students obtain generic e-learning that is not personalized to their needs as a result of this practice. New research and development are required for cloud-based personalized learning to be used and developed across many topic areas. In most modern systems, the interaction between professors and students is critical to increasing the quality of the learning experience for each individual. Integrating cloud- based e-learning services, such as video conferencing or instant messaging, should be possible with online and real-time training. Modern cloud-based e-learning systems make up for these shortcomings by using email, voice-over-IP, and apps like Skype. For the great majority of cloud- hosted services, this is still a concern. There are numerous factors to consider when estimating the size of a problem. Cloud service providers have made significant investments in cloud infrastructure and platforms in response to client concerns about security and privacy. Furthermore, country restrictions are essential since some countries demand that data be kept within their borders, making data storage remotely or outside the country a criminal offense. According to the current research, academics have an abundance of data at their disposal to aid in the development of cloud- based e-learning frameworks and implementations. a quantitative evaluation of the impact on numerous parameters such as access speed, influence on educational quality, and return of migrating to a cloud e-learning environment will be a future inquiry.\nREFERENCES:\n[1] Alam, T. (2021). Cloud Computing and its role in the Information Technology. IAIC Transactions on Sustainable Digital Innovation (ITSDI), 1, 108-115.\n[2] Aldowah, H., Al-Samarraie, H., & Fauzy, W. M. (2019). Educational data mining and\nJournal of Theoretical and Applied Information Technology\n5 S,\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\nWww.jatit.org\nE-ISSN: 1817-3195\n[24] Kumar, P. R., Raj, P. H., & Jelciana, P. (2018). Exploring data security issues and solutions in cloud computing. Procedia Computer Science, 125, 691-697.\n[25] Kundu, K., & Sharma, M. (2020). Data Mining and Techniques. EmErging TrEnds in Big Data, IoT and CyBEr sECuriTy, 33.\n[26] Manvi, S. S., & Shyam, G. K. (2014). Resource management for Infrastructure as a Service (IaaS) in cloud computing: A survey. Journal of network and computer applications, 41, 424- 440.\n[27] Manzoor, D., Ali, A., & Ahmad, A., Cloud and Web Technologies: Technical Improvements and Their Implications on E-Governance. International Journal of Advanced Computer Science and Applications, 2014.5(5): 196-201.\n[28] Marinescu, D. C. (2017). Cloud computing: theory and practice: Morgan Kaufmann.\n[29] Masud, M. A. H., & Huang, X. (2011). ESaaS: A new education software model in E-learning systems. Paper presented at the International Conference on Information and Management Engineering.\n[30] Mell, P., & Grance, T. (2011). The NIST definition of cloud computing.\n[31] Pahl, C. (2015). Containerization and the PaaS cloud. IEEE Cloud Computing, 2(3), 24-31.\n[32] Palos-Sanchez, P. R., Arenas-Marquez, F. J., & Aguayo-Camacho, M. (2017). Cloud computing (SaaS) adoption as a strategic technology: Results of an empirical study. Mobile Information Systems, 2017.\n[33] Panoutsopoulos, H., Donert, K., Papoutsis, P., & Kotsanis, I. (2015). Education on the Cloud: Researching Student-Centered, Cloud- Based Learning Prospects in the Context of a European Network. International Association for Development of the Information Society.\n[34] Park, J. H., & Park, J. H. (2017). Blockchain security in cloud computing: Use cases, challenges, and solutions. Symmetry, 9(8), 164.\n[35] Pekane, A. (2015). Adoption of cloud computing to enhance project management processes and outcomes in South Africa in the private sector. Cape Peninsula University of Technology.\n[36] Shawish, A., & Salama, M. (2014). Cloud computing: paradigms and technologies Inter- cooperative collective intelligence: Techniques and applications (pp. 39-67): Springer.\n[37] Srivastava, P., & Khan, R. (2018). A review paper on cloud computing. International Journal\n854\nISSN: 1992-8645\n(2020). Dynamic resource allocation for distributed systems and cloud computing. TEST Engineering & Management, 83, 22417-22426.\n[15] Hashem, I. A. T., Yaqoob, I., Anuar, N. B., Mokhtar, S., Gani, A., & Khan, S. U. (2015). The rise of “big data” on cloud computing: Review and open research issues. Information systems, 47, 98-115.\n[16] Kaisara, G., & Bwalya, K. J. (2021). Investigating the E-Learning Challenges Faced by Students during COVID-19 in Namibia. International Journal of Higher Education, 10(1), 308-318.\n[17] Kausar, S., Huahu, X., Hussain, I., Wenhao, Z., & Zahid, M. (2018). Integration of data mining clustering approach in the personalized E- learning system. IEEE Access, 6, 72724-72734.\n[18] Khan, R. M. I., Ali, A., Alourani, A., Kumar, T., & Shahbaz, M. (2021). An Investigation of the Educational Challenges During COVID-19: A Case Study of Saudi Students\' Experience. An Investigation of the Educational Challenges During COVID-19: A Case Study of Saudi Students\' Experience, 11(1), 353-363.\n[19] Khan, I., Ibrahim, A. H., Kassim, A., & Khan, R. M. I. (2020). Exploring The EFI Learners\' Attitudes Towards the Integration of Active Reading Software in Learning Reading Comprehension at Tertiary Level. MIER Journal of Educational Studies Trends & Practices, 248-266.\n[20] Khan, R. M. I., Kumar, T., Supriyatno, T., & Nukapangu, V. (2021). The Phenomenon of Arabic-English Translation of Foreign Language Classes During The Pandemic. Ijaz Arabi Journal of Arabic Learning, 4(3).\n[21] Khan, R. M. I., Radzuan, N., Farooqi, S., Shahbaz, M., & Khan, M. (2021). Learners’ Perceptions on WhatsApp Integration as a Learning Tool to Develop EFL Spoken Vocabulary. International Journal of Language Education, 5(2), 1-14.\n[22] Khan, R. M. I., Radzuan, N. R. M., Shahbaz, M., & Ibrahim, A. H. (2018). EFL Instructors’ Perceptions on the Integration and Implementation of MALL in EFL Classes. International Journal of Language Education and Applied Linguistics, 39-50.\n[23] Khan, R. M. I., Shahbaz, M., Kumar, T., & Khan, I. (2020). Investigating Reading Challenges Faced by EFL Learners at Elementary Level. Register Journal, 13(2), 277- 292.\nJournal of Theoretical and Applied Information Technology\n15th February 2022. Vol.100. No 3 © 2022 Little Lion Scientific\nISSN: 1992-8645\nWww.jatit.org\nof Advanced Research in Computer Science and Software Engineering, 8(6), 17-20.\n[38] Sultan, N. (2014). Making use of cloud computing for healthcare provision: Opportunities and challenges. International Journal of Information Management, 34(2), 177-184.\n[39] Sunyaev, A. (2020). Cloud computing Internet computing (pp. 195-236): Springer.\n[40] Varghese, B., & Buyya, R. (2018). Next generation cloud computing: New trends and research directions. Future Generation Computer Systems, 79, 849-861.\n[41] Wang, L., Ranjan, R., Chen, J., & Benatallah, B. (2017). Cloud computing: methodology, systems, and applications: CRC Press.\n[42] Xu, Z., Cheng, C., & Sugumaran, V. (2020). Big data analytics of crime prevention and control based on image processing upon cloud computing. Journal of Surveillance, Security and Safety, 1(1), 16-33.\n[43] Zhang, L., Luo, Y., Tao, F., Li, B. H., Ren, L., Zhang, X., . . . Liu, Y. (2014). Cloud manufacturing: a new manufacturing paradigm. Enterprise Information Systems, 8(2), 167-187.\n[44] Zhang, Y., Zhang, G., Liu, Y., & Hu, D. (2017). Research on services encapsulation and virtualization access model of machine for cloud manufacturing. Journal of Intelligent Manufacturing, 28(5), 1109-1123.\n[45] Ziani, A., Sadouq, Z. A., & Medouri, A. (2019). Use of cloud computing and GIS on vehicle traffic management. International Journal of Intelligent Enterprise, 6(2-4), 382- 392.\n5 S,\nE-ISSN: 1817-3195\n855', 'Document 2: NIPS-2017-attention-is-all-you-need-Paper.pdf\nAttention Is All You Need\nAshish Vaswani∗ Google Brain avaswani@google.com\nLlion Jones∗\nGoogle Research llion@google.com\nNoam Shazeer∗ Google Brain noam@google.com\nNiki Parmar∗\nGoogle Research nikip@google.com\nJakob Uszkoreit∗ Google Research usz@google.com\nAidan N. Gomez∗ †\nŁukasz Kaiser∗\nUniversity of Toronto aidan@cs.toronto.edu\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n1 Introduction\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28].\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n2\nOutput Probabilities Multi-Head Feed Attention Forward ) Nx L Nx Masked Multi-Head Multi-Head Attention Attention 4t _t Positional Positional Encoding Q @ Q Encoding Inputs Outputs (shifted right)\nFigure 1: The Transformer - model architecture.\nwise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n3\nScaled Dot-Product Attention\nSoftMax Mask (opt.) Scale\nMulti-Head Attention \nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\nquery with all keys, divide each by dk, and apply a softmax function to obtain the weights on the values.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\nAttention(Q, K, V ) = softmax( QK T √ dk )V\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor 1√ of . Additive attention computes the compatibility function using a feed-forward network with dk a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ . dk\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n“To illustrate why the dot products get large, assume that the components of ¢ and k are independent random variables with mean 0 and variance 1. Then their dot product, ¢ - k = iki, has mean 0 and variance dj.\n(1)\n4\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O where headi = Attention(QW Q i , KW K i , V W V i )\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk , W K i ∈ Rdmodel×dk , W V i ∈ Rdmodel×dv\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality df f = 2048.\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √ dmodel.\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the\n(2)\n5\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\n<table><thead><tr><th>Layer Type</th><th>Complexity per Layer</th><th>Sequential Operations</th><th>Maximum Path Length</th></tr></thead><tbody><tr><td>Self-Attention</td><td>O(n-d)</td><td>o(1)</td><td>o(1)</td></tr><tr><td>Recurrent</td><td>O(n-d?)</td><td>O(n)</td><td>O(n)</td></tr><tr><td>Convolutional</td><td>O(k-n-d)</td><td>D o</td><td>O(logk(n))</td></tr><tr><td>Self-Attention (restricted)</td><td>O(r-n-d)</td><td>)</td><td>O(n/r)</td></tr></tbody></table>\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and ﬁxed [8].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos/100002i/dmodel) P E(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k, P Epos+k can be represented as a linear function of P Epos.\nWe also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n6\nthe input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [17] with 8; = 0.9, 2 = 0.98 and € = 10~. We varied the learning rate over the course of training, according to the formula:\nlrate = d−0.5 model · min(step_num−0.5, step_num · warmup_steps−1.5)\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\n(3)\n7\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n<table><thead><tr><th rowspan="2">Model</th><th colspan="2">BLEU</th><th colspan="2">Training Cost (FLOPs)</th></tr><tr><th>EN-DE</th><th>EN-FR</th><th>EN-DE</th><th>EN-FR</th></tr></thead><tbody><tr><td colspan="5">ByteNet 23.75</td></tr><tr><td>Deep-Att + PosUnk [32]</td><td></td><td>39.2</td><td></td><td>1.0 10%0</td></tr><tr><td>GNMT + RL [31]</td><td>24.6</td><td>39.92</td><td>2.3- 10</td><td>14 10%0</td></tr><tr><td>ConvS2S</td><td>25.16</td><td>40.46</td><td>108</td><td>15 10%0</td></tr><tr><td>MoE</td><td>26.03</td><td>40.56</td><td>2.0- 10"</td><td>1.2 100</td></tr><tr><td>Deep-Att + PosUnk Ensemble [32]</td><td></td><td>40.4</td><td></td><td>8.0 10%0</td></tr><tr><td>GNMT + RL Ensemble [31]</td><td>26.30</td><td>41.16</td><td>1.8 10%</td><td>11 10!</td></tr><tr><td>ConvS2S Ensemble [8]</td><td>26.36</td><td>41.29</td><td>77 10"</td><td>1.2 10!</td></tr><tr><td>Transformer (base model)</td><td>27.3</td><td>38.1</td><td>3.3 R</td><td>1</td></tr><tr><td>Transformer (big)</td><td>28.4</td><td>41.0</td><td colspan="2">2.3 -10</td></tr></tbody></table>\nLabel Smoothing During training, we employed label smoothing of value €;, = 0.1 [30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31].\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n<table><thead><tr><th></th><th></th><th>model</th><th>d</th><th>h</th><th>d</th><th>d,</th><th>€ls</th><th>train steps</th><th>PPL (dev)</th><th>BLEU (dev)</th><th>params 108</th></tr></thead><tbody><tr><td>base</td><td></td><td>512</td><td>2048</td><td></td><td>64</td><td>64</td><td>0.1</td><td>100K</td><td>4.92</td><td>25.8</td><td>65</td></tr><tr><td rowspan="4">(A)</td><td></td><td></td><td></td><td>—_</td><td>512</td><td>512</td><td></td><td></td><td>5.29</td><td>24.9</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td>5.00</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td>16</td><td>32</td><td>32</td><td></td><td></td><td>491</td><td>25.8</td><td></td></tr><tr><td></td><td></td><td></td><td>32</td><td>16</td><td>16</td><td></td><td></td><td>5.01</td><td>25.4</td><td></td></tr><tr><td rowspan="2">(B)</td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td>5.16</td><td>25.1</td><td>58</td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td>60</td></tr><tr><td rowspan="7">©</td><td>B</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>23.7</td><td>36</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.19</td><td>25.3</td><td>50</td></tr><tr><td>o0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.88</td><td>25.5</td><td>80</td></tr><tr><td></td><td>256</td><td></td><td></td><td>32</td><td>32</td><td></td><td></td><td>5.75</td><td>24.5</td><td>28</td></tr><tr><td></td><td>1024</td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td>4.66</td><td>26.0</td><td>168</td></tr><tr><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td>5.12</td><td>25.4</td><td>53</td></tr><tr><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td>4.75</td><td>26.2</td><td>90</td></tr><tr><td rowspan="4">D)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.77</td><td>24.6</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.95</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>4.67</td><td>25.3</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>5.47</td><td>25.7</td><td></td></tr><tr><td>®)</td><td></td><td></td><td>positional</td><td>embedding</td><td></td><td>instead of</td><td>sinusoids</td><td></td><td>4.92</td><td>25.7</td><td></td></tr><tr><td>big</td><td></td><td>1024</td><td>4096</td><td>16</td><td></td><td></td><td></td><td>300K |</td><td>4.33</td><td>264</td><td>213</td></tr></tbody></table>\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model.\n7 Conclusion\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\n9\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in recurrent nets: the difﬁculty of learning long-term dependencies, 2001.\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\n10\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n11', 'Document 3: 20-1361.pdf\nJournal of Machine Learning Research 23 (2022) 1-40\nSubmitted 12/20; Revised 9/21; Published 2/22\nSpatial Multivariate Trees for Big Data Bayesian Regression\nMichele Peruzzi\nDavid B. Dunson\nDepartment of Statistical Science Duke University Durham, NC 27708-0251, USA\nmichele.peruzzi@duke.edu dunson@duke.edu\nEditor: John Cunningham\nAbstract\nHigh resolution geospatial data are challenging because standard geostatistical models based on Gaussian processes are known to not scale to large data sizes. While progress has been made towards methods that can be computed more eﬃciently, considerably less attention has been devoted to methods for large scale data that allow the description of complex relationships between several outcomes recorded at high resolutions by diﬀerent sensors. Our Bayesian multivariate regression models based on spatial multivariate trees (SpamTrees) achieve scalability via conditional independence assumptions on latent ran- dom eﬀects following a treed directed acyclic graph. Information-theoretic arguments and considerations on computational eﬃciency guide the construction of the tree and the related eﬃcient sampling algorithms in imbalanced multivariate settings. In addition to simulated data examples, we illustrate SpamTrees using a large climate data set which combines satellite data with land-based station data. Software and source code are available on CRAN at https://CRAN.R-project.org/package=spamtree.\nKeywords: Directed acyclic graph, Gaussian process, Geostatistics, Multivariate regres- sion, Markov chain Monte Carlo, Multiscale/multiresolution.\n1. Introduction\nIt is increasingly common in the natural and social sciences to amass large quantities of geo- referenced data. Researchers seek to use these data to understand phenomena and make predictions via interpretable models that quantify uncertainty taking into account the spa- tial and temporal dimensions. Gaussian processes (GP) are ﬂexible tools that can be used to characterize spatial and temporal variability and quantify uncertainty, and considerable attention has been devoted to developing GP-based methods that overcome their notoriously poor scalability to large data. The literature on scaling GPs to large scale is now exten- sive. We mention low-rank methods (Quiñonero-Candela and Rasmussen, 2005; Snelson and Ghahramani, 2007; Banerjee et al., 2008; Cressie and Johannesson, 2008); their extensions (Low et al., 2015; Ambikasaran et al., 2016; Huang and Sun, 2018; Geoga et al., 2020); methods that exploit special structure or simplify the representation of multidimensional inputs—for instance, a Toeplitz structure of the covariance matrix scales GPs to big time series data, and tensor products of scalable univariate kernels can be used for multidimen- sional inputs (Gilboa et al., 2015; Moran and Wheeler, 2020; Loper et al., 2020; Wu et al.,\n©2022 Michele Peruzzi and David B. Dunson.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v23/20-1361.html.\nPeruzzi & Dunson\nMODIS-TERRA GHCN Clear_sky_days Clear_sky_nights PRCP o LST_Day_CMG LST_Night_CMG s &\nFigure 1: Observed data of Section 4.2. Missing outcomes are in magenta. GHCN data are much more sparsely observed compared to satellite imaging from MODIS.\n2021). These methods may be unavailable or perform poorly in geostatistical settings, which focus on small-dimensional inputs, i.e. the spatial coordinates plus time. In these scenarios, low-rank methods oversmooth the spatial surface (Banerjee et al., 2010), Toeplitz-like struc- tures are typically absent, and so-called separable covariance functions obtained via tensor products poorly characterize spatial and temporal dependence. To overcome these hurdles, one can use covariance tapering and domain partitioning (Furrer et al., 2006; Kaufman et al., 2008; Sang and Huang, 2012; Stein, 2014; Katzfuss, 2017) or composite likelihood methods and sparse precison matrix approximations (Vecchia, 1988; Rue and Held, 2005; Eidsvik et al., 2014); refer to Sun et al. (2011), Banerjee (2017), Heaton et al. (2019) for reviews of scalable geostatistical methods.\nAdditional diﬃculties arise in multivariate (or multi-output) regression settings. Mul- tivariate geostatistical data are commonly misaligned, i.e. observed at non-overlapping spatial locations (Gelfand et al., 2010). Figure 1 shows several variables measured at non- overlapping locations, with one measurement grid considerably sparser than the others. In these settings, replacing a multi-output regression with separate single-output models is a valid option for predicting outcomes at new locations. While single-output models may some- times perform equally well or even outperform multi-output models, they fail to characterize and estimate cross-dependences across outputs; testing the existence of such dependences may be scientiﬁcally more impactful than making predictions. This issue can be solved by modeling the outputs via latent spatial random eﬀects thought of as a realization of an underlying multivariate GP and embedded in a larger hierarchical model.\nUnfortunately, GP approximations that do not correspond to a valid stochastic process may inaccurately characterize uncertainty, as the models used for estimation and interpola- tion may not coincide. Rather than seeking approximations to the full GP, one can develop valid standalone spatial processes by introducing conditional independence across spatial locations as prescribed by a sparse directed acyclic graph (DAG). These models are advan- tageous because they lead to scalability by construction; in other words, posterior computing algorithms for these methods can be interpreted not only as approximate algorithms for the full GP, but also as exact algorithms for the standalone process.\n2\nSpatial Multivariate Trees\nThis family of methods includes nearest-neighbor Gaussian processes, which limit de- pendence to a small number of neighboring locations (NNGP; Datta et al. 2016a,b), and block-NNGPs (Quiroz et al., 2019). There is a close relation between DAG structure and computational performance of NNGPs: some orderings may be associated to improved ap- proximations (Guinness, 2018), and graph coloring algorithms (Molloy and Reed, 2002; Lewis, 2016) can be used for parallel Gibbs sampling. Inferring ordering or coloring can be problematic when data are in the millions, but these issues can be circumvented by forcing DAGs with known properties onto the data; in meshed GPs (MGPs; Peruzzi et al., 2020), patterned DAGs associated to domain tiling are associated to more eﬃcient sampling of the latent eﬀects. Alternative so-called multiscale or multiresolution methods correspond to DAGs with hierarchical node structures (trees), which are typically coupled with recursive domain partitioning; in this case, too, eﬃciencies follow from the properties of the chosen DAG. There is a rich literature on Gaussian processes and recursive partitioning, see e.g Ferreira and Lee (2007); Gramacy and Lee (2008); Fox and Dunson (2012); in geospatial contexts, in addition to the GMRF-based method of Nychka et al. (2015), multi-resolution approximations (MRA; Katzfuss, 2017) replace an orthogonal basis decomposition with ap- proximations based on tapering or domain partitioning and also have a DAG interpretation (Katzfuss and Guinness, 2021).\nConsiderably less attention has been devoted to process-based methods that ensure scal- ability in multivariate contexts, with the goal of modeling the spatial and/or temporal variability of several variables jointly via ﬂexible cross-covariance functions (Genton and Kleiber, 2015). When scalability of GP methods is achieved via reductions in the condition- ing sets, including more distant locations is thought to aid in the estimation of unknown covariance parameters (Stein et al., 2004). However, the size of such sets may need to be reduced excessively when outcomes are not of very small dimension. One could restrict spatial coverage of the conditioning sets, but this works best when data are not misaligned, in which case all conditioning sets will include outcomes from all margins; this cannot be achieved for misaligned data, leading to pathological behavior. Alternatively, one can model the multivariate outcomes themselves as a DAG; however this may only work on a case-by- case basis. Similarly, recursive domain partitioning strategies work best for data that are measured uniformly in space as this guarantees similarly sized conditioning sets; on the con- trary, recursive partitioning struggles in predicting the outcomes at large unobserved areas as they tend to be associated to the small conditioning sets making up the coarser scales or resolutions.\nIn this article, we solve these issues by introducing a Bayesian regression model that encodes spatial dependence as a latent spatial multivariate tree (SpamTree); conditional independence relations at the reference locations are governed by the branches in a treed DAG, whereas a map is used to assign all non-reference locations to leaf nodes of the same DAG. This assignment map controls the nature and the size of the conditioning sets at all locations; when severe restrictions on the reference set of locations become necessary due to data size, this map is used to improve estimation and predictions and overcome com- mon issues in standard nearest-neighbor and recursive partition methods while maintaining the desirable recursive properties of treed DAGs. Unlike methods based on deﬁning condi- tioning sets based solely on spatial proximity, SpamTrees scale to large data sets without excessive reduction of the conditioning sets. Furthermore, SpamTrees are less restrictive\n3\nPeruzzi & Dunson\n£ BA L LUAL LRUAM 2 \' x DRI \\ve s o Qe e es es e [ ]\nFigure 2: Three SpamTrees on M = 4 levels with depths δ = 1 (left), δ = 3 (center), and δ = 4 (right). Nodes are represented by circles, with branches colored in brown and leaves in green.\nthan methods based on recursive partitioning and can be built to guarantee similarly-sized conditioning sets at all locations.\nThe present work adds to the growing literature on spatial processes deﬁned on DAGs by developing a method that targets eﬃcient computations of Bayesian multivariate spatial regression models. SpamTrees share similarities with MRAs (Katzfuss, 2017); however, while MRAs are deﬁned as a basis function expansion, they can be represented by a treed graph of a SpamTree with full “depth” as deﬁned later (the DAG on the right of Figure 2), in univariate settings, and “response” models. All these restrictions are relaxed in this article. In considering spatial proximity to add “leaves” to our treed graph, our methodology also borrows from nearest-neighbor methods (Datta et al., 2016a). However, while we use spatial neighbors to populate the conditioning sets for non-reference locations, the same cannot be said about reference locations for which the treed graph is used instead. Our construction of the SpamTree process also borrows from MGPs on tessellated domains (Peruzzi et al., 2020); however, the treed DAG we consider here induces markedly diﬀerent properties on the resulting spatial process owing to its recursive nature. Finally, a contribution of this article is in developing self-contained sampling algorithms which, based on the graphical model representation of the model, will not require any external libraries.\nThe article builds SpamTrees as a standalone process based on a DAG representation in Section 2. A Gaussian base process is considered in Section 3 and the resulting properties outlined, along with sampling algorithms. Simulated data and real-world applications are in Section 4; we conclude with a discussion in Section 5. The Appendix provides more in-depth treatment of several topics and additional algorithms.\n2. Spatial Multivariate Trees\nConsider a spatial or spatiotemporal domain D. With the temporal dimension, we have D c R [0,0), otherwise D C R4 A g-variate spatial process is defined as an uncountable set of random variables {w(£) : £ € D}, where w() is a ¢ x 1 random vector with elements w;(£) fori = 1,2, ,q, paired with a probability law P defining the joint distribution of any finite sample from that set. Let {£1, £2, ,,} = L C D beof size n. The ngx 1 random\n4\nSpatial Multivariate Trees\nvector w = (w(€y) ", w(y) ", ,w(£,,)") T has joint density p(w,). After choosing an arbitrary order of the locations, p(w) = — [T p(w(;) |w(£), ..., w(€i1)), where the conditioning set for each w(¥;) can be interpreted as the set of nodes that have a directed edge towards w(;) in a DAG. Some scalable spatial processes result from reductions in size of the conditioning sets, following one of several proposed strategies (Vecchia, 1988; Stein et al., 2004; Gramacy and Apley, 2015; Datta et al., 2016a; Katzfuss and Guinness, 2021; Peruzzi et al., 2020). Accordingly,\nplaw) = w(Pal),\nwhere Pa[;] is the set of spatial locations that correspond to directed edges pointing to £; in the DAG. If Pa[¢;] is of size J or less for all i = 1,...,ng then w(Pa[;])) is of size Jq. Methods that rely on reducing the size of parent sets are thus negatively impacted by the dimension ¢ of the multivariate outcome; if ¢ is not very small, reducing the number of parent ocations J may be insufficient for scalable computations. As an example, an NNGP model has Pa[¢;] = N(£;), where N(-) maps a location in the spatial domain to its neighbor set. It is customary in practice to consider Jq = m < 20 for accurate and scalable estimation and predictions in univariate settings, but this may be restrictive in some multivariate settings as one must reduce J to maintain similar computing times, possibly harming estimation and prediction accuracy.\nWe represent the ith component of the ¢ x 1 vector w() as w(£,&;), where & = 1, € E for some k and Z serves as the k-dimensional latent spatial domain of variables. The g-variate process w(£) is thus recast as {w(,€) : (£,€) € D x E}, with £ representing the latent location in the domain of variables. We can then write (1) as\nnpx plwe) = [] plw(e) | w(Pale])), i=1\nNx where £* = {£;} i 1 £ € Dx = =D* and w(-) is a univariate process on the expanded domain D*. This representation is useful as it provides a clearer accounting of the assumed conditional independence structure of the process in a multivariate context.\n2.1 Constructing Spatial Multivariate DAGs\nWe now introduce the necessary terminology and notation, which are the basis for later detailing of estimation and prediction algorithms involving SpamTrees. The speciﬁcs for building treed DAGs with user-speciﬁed depth are in Section 2.1.1, whereas Section 2.1.2 gives details on cherry picking and its use when outcomes are imbalanced and misaligned.\nThe three key components to build a SPAMTREE are (i) a treed DAG G with branches and leaves on M levels and with depth § < M; (i a reference set of locations S; (41) a cherry picking map. The graph is G = {V E} where the nodes are V = {v, Umy} = AUB, AN B = (. We separate the nodes into reference A and non-reference B nodes, as this will aid in showing that SPAMTREES lead to standalone spatial processes in Section 2.2. The reference or branch nodes are A = {ai,...,am,} = AgU A U--- U Apy_1, where ={a;1,..., Q foralli =0,...,M — 1 and with A;N A; =0 if i # j. The non- reference or leaf nodes are B = {b,...,bm}, AN B = 0. We also denote V, = A, for\n(1)\n(2)\n5\nPeruzzi & Dunson\nr=20,...,M —1and Vj; = B. The edges are E = {Pav] C V : v € V} and similarly Chlv] = {v\' € V : v € Pa[v\']}. The reference set S is partitioned in M levels starting from M—1 M—1 zero, and each level is itself partitioned into reference subsets: S = U T 0 =u 0 Uiy Sri, where S; N S = 0 if # r or i # \' and its complement set of non-reference or other locations U = D* S. The cherry picking map is n : D* — V and assigns a node (and therefore all the edges directed to it in G) to any location in the domain, following a user- specified criterion.\n2.1.1 Branches and Leaves\nFor a given M and a depth § < M, we impose a treed structure on G by assuming that if v e A;and i > M — 0 = M; then there exists a sequence of nodes {v,,, ’ vy, ,} such that v,; € A; for j = Ms,...,i— 1 and Pa[v] = {v,,, vr, O < M—=M then Pafv] = {v;, 1} with v;_; € A, ;. Ag is the tree root and is such that Pafvg] = 0 for all vg € Ag. The depth ¢ determines the number of levels of G (from the top) across which the parent sets are nested. Choosing § = 1 implies that all nodes have a single parent; choosing 6 = M implies fully nested parent sets (i.e. if v; € Pafv;] then Pafv;] C Pa;] for all v;,v; € V). The m; elements of A; are the branches at level i of G and they have i — parents if the current level i is above the depth level Ms and 1 parent otherwise. We refer to terminal branches as nodes v € A such that Chv] C B. For all choices of 4, ve A;,v\' € Aj and v € Pa[v\'] implies i < j; this guarantees acyclicity.\n, . . . , vrk } for some integer As for the leaves, for all v ∈ B we assume Pa[v] = {vrMδ sequence {rMδ , . . . , rk} and vri ∈ Ai with i ≥ Mδ. We allow the existence of multiple leaves with the same parent set, i.e. there can be k and bi1, . . . , bik such that for all i2, . . . , ik, Pa[bih] = Pa[bi1]. Acyclicity of G is maintained as leaves are assumed to have no children. Figure 2 represents the graph associated to SpamTrees with diﬀerent depths.\n2.1.2 Cherry Picking via η(·)\nThe link between G, S and U is established via the map n : D* — V which associates a node in G to any location £* in the expanded domain D*:\nna(€) =ar € Ay if €Sy, ) = ) =be B if £* cl\nThis is a many-to-one map; note however that all locations in S;; are mapped to a;;: by calling n(X) = {n(€*) : € X} then for any = 0,...,M — 1 and any j = 1,...,m; we have 1(S) = na(Si;) = a;. SPAMTREES introduce flexibility by cherry picking the leaves, ie. using np : U — B, the restriction of 7 to U. Since each leaf node b; determines a unique path in G ending in b;, we use 7 to assign a convenient parent set to w(w), u € U, following some criterion.\nFor example, suppose that u = (£,&;) meaning that w(u) = w(¥, & is the realization of the s-th variable at the spatial location £, and we wish to ensure that Paw(u)] includes realizations of the same variable. Denote T\' = {v € A : Ch[v] C B} as the set of terminal s branches of G. Then we find (€, &)opt = arg ming Yen;(T) d(,£) where d(-,-) is the Euclidean distance. Since (£,&)opt € Sij for some 4,j we have na((£ &)opt) = aij. We then set np(u) = by, where Pa[b] = {ai;}. In a sense a;; is the terminal node nearest\n(3)\n6\nSpatial Multivariate Trees\nto w; having defined np in such a way forces the parent set of any location to include at least one realization of the process from the same variable. There is no penalty in using — D* = D x = as we can write p(w(u) | Pajw(u)]) = p(w((,&1) ’ (,)) | Pafw(u)]) = i p(w(, &) w, &), ..., w(, ), Paw(£)])), which also implies that the size of the parent set may depend on the variable index. Assumptions of conditional independence across variables can be encoded similarly. Also note that any specific choice of np induces a partition on U; let U; = {u € U : np(u) = b;}, then clearly U = U my U; with U; N U; = 0 j=1 if 4 # j. This partition does not necessarily correspond to the partitioning scheme used on S np may by designed to ignore part of the tree and result in my < mp. However, we can just drop the unused leaves from G and set Ch[a] = () for terminal nodes whose leaf is inactive, resulting in my = mp. We will thus henceforth assume that my = mp without loss of generality.\n2.2 SpamTrees as a Standalone Spatial Process\nWe define a valid joint density for any finite set of locations in D* satisfying the Kolmogorov consistency conditions in order to define a valid process. We approach this problem analo- gously to Datta et al. (2016a) and Peruzzi et al. (2020). Enumerate each of the mg reference subsets as S; = {s;,, 5 y, } where {i1, n € {1,...,ns}, and each of the my non- reference subsets as U; = {u;,, - Ui, } where {i ’ ’ in} {1,...,n}. Then introduce V= {Vi, ., Viny } where my = mg + my and V; = S; for i = 1,...,mg, Vigi = U; for i=1,...,my. Then take w; = (w(£;,),...,w(£;, )) as the n; x 1 random vector with elements of w(£) for each £ € V;. Denote w) = w(n~Pa[v;])). Then\nM1 Plws) =plwy, ..., w) = [[ plw:lw) Plwy |ws) =[] plwi|w) r=0 i:{v;€A,} i{v,B} M—1 Pws)pwy |ws)=[[ [ pwilwy) plw:|wy) r=0 i:{v;€A,} i{v,€B}\nwhich is a proper multivariate joint density since G is acyclic (Lauritzen, 1996). All locations inside Uj always share the same parent set, but a parent set is not necessarily unique to a single Uj. This includes as a special case a scenario in which one can assume\nmy j Plw | ws) =[] [] p(w(u) | w(n™" (Pafb;]))); j=li=1\nin this case each location corresponds to a leaf node. To conclude the construction, for any ﬁnite subset of spatial locations L ⊂ D we can let U = L \\ S and obtain\ne) = [ | ws)tws) [] dw(s), ;,€\\L\nleading to a well-deﬁned process satisfying the Kolmogorov conditions (see Appendix A).\n(4)\n(5)\n7\nPeruzzi & Dunson\n2.2.1 Positioning of Spatial Locations in Conditioning Sets\nIn spatial models based on sparse DAGs, larger conditioning sets yield processes that are closer to the base process p in terms of Kullback-Leibler divergence (Banerjee, 2020; Pe- ruzzi et al 2020), denoted as K L(p|| The same results cannot be applied directly to SPAMTREES given the treed structure of the DAG. For a given S, we consider the distinct but related issues of placing individual locations into reference subsets (1) at different levels of the treed hierarchy; (2) within the same level of the hierarchy.\nProposition 1 Suppose S = So U S1 where Sy N Sy 0 and Sy S11US12, S11NS12 =0 Take s* ¢ S. Consider the graph {V = {v,v1,v2}, E = {vg — v1,v9 = va}}; denote as p the density of a SPAMTREE using n(Sy U {s*}) = v, (S11) = v1 and (Si2) = va, whereas let p1 be the density of a SPAMTREE with n(Sp) = vo, n(S U {s*}) = v1 and (S12) = v. Then K L(p| KL(p|lpo) >0\nThe proof proceeds by an “information never hurts” argument (Cover and Thomas, 1991) T Denote §* SU{s*}, w* = wg = w(s*) and w (w w*) Then\np0(w∗) = p(w∗ 0)p(w1 | w∗ 0)p(w2 | w∗ 0) = p(w0)p(w∗ | w0)p(w1 | w0, w∗)p(w2 | w∗ 0) p1(w∗) = p(w0)p(w∗ 1 | w0)p(w2 | w0) = p(w0)p(w∗ | w0)p(w1 | w0, w∗)p(w2 | w0),\ntherefore p0(w∗)/p1(w∗) = p(w2 | w∗ 0)/p(w2 | w0); then by Jensen’s inequality\n) llp1 KL(p|po) = { (26 p1(w & w* po(w p( w | p1(w ) (w*)dw — ( w | w ) p(w*)dw* plw | w) = J (o p(w | wo) w1, wy, w)dw dwdwy = p(w 2 | wp) JA e (s (w 2| wo) (wg)dw > 0\nIntuitively, this shows that there is a penalty associated to positioning reference locations at higher levels of the treed hierarchy. Increasing the size of the reference set at the root augments the conditioning sets at all its children; since this is not true when the increase is at a branch level, the KL divergence of p0 from p is smaller than the divergence of p1 from the same density. In other words there is a cost of branching in G which must be justiﬁed by arguments related to computational eﬃciency. The above proposition also suggests populat- ing near-root branches with locations of sparsely-observed outcomes. Not doing so in highly imbalanced settings may result in possibly too restrictive spatial conditional independence assumptions.\nProposition 2 Consider the same setup as Proposition 1 and let p be the density of a SPAMTREE such that n(S12 U {s*}) = v. Let H, be the conditional entropy of base process p. Then Hy(w* | wo, w2) < Hy(w*|wo,w) implies K L(p|p2) < KL(p||p1\n(6)\n8\nSpatial Multivariate Trees\nThe density of the new model is\np(w) = p(wo)p(w | wo)p(w | w) = p(wo)p(w | wo)p(w [ wo)p(w | wo, wa). p1(w*) p(w* [wo,w) Then, noting that p(w | wo) = p(w1 | wo)p(w* | wo, w1), we get p2(w) p(w Jwo, ) and KL(p) = KL = [ og | wni(w)dw — [ 1ogp(u [w, wa)pw)du = H,(w* | w, w) — Hy(w* | w, w).\nWhile we do not target the estimation of these quantities, this result is helpful in designing SpamTrees as it suggests placing a new reference location s∗ in the reference subset least uncertain about the realization of the process at s∗. We interpret this as justifying recursive domain partitioning on S in spatial contexts in which local spatial clusters of locations are likely less uncertain about process realization in the same spatial region. In the remainder of this article, we will consider a given reference set S which typically will be based on a subset of observed locations; the combinatorial problem of selecting an optimal S (in some sense) is beyond the scope of this article. If S is not partitioned, it can be considered as a set of knots or “sensors” and one can refer to a large literature on experimental design and optimal sensor placement (see e.g. Krause et al., 2008, and references therein). It might be possible to extend previous work on adaptive knot placement (Guhaniyogi et al., 2011), but this will come at a steep cost in terms of computational performance.\n3. Bayesian Spatial Regressions Using SpamTrees\nSuppose we observe an -variate outcome at spatial locations £ € D C R?¢ which we wish to model using a spatially-varying regression model:\n— = 1, A = ) + ( w(, &) + ), J\nwhere y;(£) is the j-th point-referenced outcome at £, x;(€) is a p; x 1 vector of spatially d referenced predictors linked to constant coefficients 3;, £;(£) ~ N 0, T is the measurement error for outcome 7, and z;(£) is the k-th (of ¢) covariates for the j-th outcome modeled with spatially-varying coefficient w(,;), £ € D, & € =. This coefficient w(, & corresponds to the k-th margin of a g-variate Gaussian process {w(£) : £ € D} denoted as w(€) ~ GP(0,Cp(-,-)) with cross-covariance Cg indexed by unknown parameters 6 which we omit in notation for simplicity. A valid cross-covariance function is defined as C : D x D — Mq: where My, is a subset of the space of all g x g real matrices R9*9. It must satisfy T C(,£)=C(,) for any two locations £,€ € D, and 31 | >° J =17%; () > 0 for any integer n and finite collection of points {£;, 2, .,£,} and for all z; € 7\\ {0}.\nWe replace the full GP with a Gaussian SPAMTREE for scalable computation considering the g-variate multivariate Gaussian process w(-) as the base process. Since the (,)-th entry of C(£,£\') is C(£,£\');; = Cov(wi(€), w;(£)), i.e. the covariance between the i-th and j-th elements of w(£) at £ and £, we can obtain a covariance function on the augmented domain C* : D* x D* — R as C*((£,),(,¢)) = C(,); where & and ¢ are the\n(7)\n9\nPeruzzi & Dunson\nlocations in = of variables ¢ and j, respectively. Apanasovich and Genton (2010) use a similar representation to build valid cross-covariances based on existing univariate covariance functions; their approach amounts to considering € or | —¢’|| as a parameter to be estimated. Our approach can be based on any valid cross-covariance as we may just set 2 = {1,...,q}. Refer to e.g. Genton and Kleiber (2015) for an extensive review of cross-covariance functions for multivariate processes. Moving forward, we will not distinguish between C* and C. The linear multivariate spatially-varying regression model (7) allows the | outcomes to be observed at different locations; we later consider the case | = g and Z(€) = I, resulting in a multivariate space-varying intercept model.\n3.1 Gaussian SpamTrees\nEnumerate the set of nodes as {v ’ Umy }, my = mg + my and denote w; = wn(vy)), Cij as the n; x n; covariance matrix between w; and wj, C; the n; x J; covariance matrix between w; and wy), C; the n; X n; covariance matrix between w; and itself, and C; the J; x J; covariance matrix between wy;) and itself. A base Gaussian process induces p(wg) = N(w; | Hjw, R;), where\nH j = Cj,[j]C−1 [j] and Rj = Cj − Cj,[j]C−1 [j] C[j],j,\n(8)\nimplying that the joint density p(wgs) is multivariate normal with covariance \' and pre- cision matrix At U we have p(wy | ws) = B N(wj | Hjwy), R;), where H; and R; are as in (8). All quantities can be computed using the base cross-covariance func- tion. Given that the p densities are Gaussian, so will be the finite dimensional distributions.\nThe treed graph G leads to properties which we analyze in more detail in Appendix B and summarize here. For two nodes vi, vj ∈ V denote the common descendants as cd(vi, vj) = ({vi} ∪ Ch[vi]) ∩ ({vj} ∪ Ch[vj]). If vi ∈ Pa[vj] denote H i→j and H \\i→j as the matrix obtained by subsetting H j to columns corresponding to vi, or to Pa[vj] \\ {vi}, respectively. Similarly deﬁne w[i→j] = wi and w[\\i→j]. As a special case, if the tree depth is δ = 1 and {vj} = Pa[vi] then cd(vi, vj) = {vi}, H i→j = H j, and w[i→j] = w[j]. Deﬁne H as the matrix whose (i, j) block is Hij = Oni×nj if vj /∈ Pa[vi], and otherwise Hij = H j→i.\n3.1.1 Precision Matrix\nThe (i, ) block of the precision matrix at both reference and non-reference locations c is denoted by c (4,7), with 4,5 = 1,...,my corresponding to nodes v;,v; € V for some i,; it is nonzero if cd(v;, vj) = 0, otherwise:\n~ 1 (4,) > ( H ;) R ( — Hj) vcd(vi,v) > (Ti — ) "Ry ( — Hg) vcd(v,v;)\n10\n(9)\nSpatial Multivariate Trees\nwhere I;; is the (,7) block of an identity matrix with ngs + ny rows and is nonzero if and only if i = j. We thus obtain that the number of nonzero elements of is\nmy n’ )= + c V , i=1\n1 where n; = [(v;)|, J; = [ (Pa[v;])|, and by symmetry (C ( =C (j,1). If § > 1, the size of C ] is larger for nodes v; at levels of the treed hierarchy farther from Ay;. However suppose v v; are such that Pav;] = {v;} UPav]. Then computing proceeds more cheaply by recursively applying the following:\n—1 +H/R\'H;, —H/R;\' C = = -R;\'H, R;\n3.1.2 Induced Covariance\nDefine a path from vy to v; as = {wi,...,v; } where v;; = wv, v;, = vj, and v;, € Pafv;, ]. The longest path Pj,; is such that if v, € A,, and v; € A then = rj —r + 1. The shortest path is the path from Uk to v; with minimum number of steps. We denote the longest path from the root to v; as Py ; this corresponds to the full set of ancestors of v, and Pa;] C . For two nodes v; and v; we have (Pa[v;]N Palv;]) C ( n . We define the concestor between v; and v; as con(v;,v;) = arg maXy,ev{k : Prsi N Prj # 0} ie. the last common ancestor of the two nodes.\nTake the path in G from a node at Ay, leading to v;. After defining the cross- covariance function K;(€,£) = and denoting K ;(£, s) = K;(£,n (vs)) we can write\nr1 wj; = _. s=ing\nwhere for s > iy the e, are independent zero-mean GPs with covariance K (£,€") and we set K; Mg (,£)=C(,£) and € = Wiy, N(0,C,,). Take two locations £, £ such that v; =(€),v; =n(\') and let v, = con(v;, v;); if Pa[v;] NPalv;] # 0 then the above leads to\nCovz(w(), w(l)) = K (65 VK (5,) +1{ = € }K;(£,), (13) sePav;NPav;]\n= — where K, (£, £) = C(e,). If Pav;]NPalv;] = () take the shortest paths P,_,; we get and P. = {1, r; }; setting Fy = C 4, ,C h 1\nCov(w(), w() = Fy, ---F;,C.F] -- F\nIn particular if 6 = M then Pav;] N Pav;] # 0 for all 4, j and only (13) is used, whereas if § = 1 then the only scenario in which (13) holds is {v.} = Pa[v;] N Pa[v;] in which case the two are equivalent. In univariate settings, the special case in which = M, and hence Ms = 0, leads to an interpretation of (12) as a basis function decomposition; considering all\n(10)\n(11)\n(12)\n(14)\n11\nPeruzzi & Dunson\nleaf paths Pj for vj ∈ B, this leads to an MRA (Katzfuss, 2017; Katzfuss and Gong, 2019). On the other hand, keeping other parameters constant, δ < M and in particular δ = 1 may be associated to savings in computing cost, leading to a trade-oﬀ between graph complexity and size of reference subsets; see Appendix B.5.\n3.1.3 Block-sparse Cholesky Decompositions\nIn recent work Jurek and Katzfuss (2020) consider sparse Cholesky decompositions of co- variance and precision matrices for treed graphs corresponding to the case § = M above in the context of space-time filtering; their methods involve sparse Cholesky routines on reverse orderings of C at the level of individual locations. In doing so, the relationship between and the block structure in § remains somewhat hidden Cholesky decompositions and G, C and sparse Cholesky libraries are typically associated to bottlenecks in MCMC algorithms However we note that a consequence of (9) is that it leads to a direct algorithm, for any , for he block-decomposition of any symmetric positive-definite matrix A conforming to G, i.e with the same block-sparse structure as C\' This allows us to write A (I-L)D(I-L) where I is the identity matrix, L is block lower triangular with the same block-sparsity pat- ern as H above, and D is block diagonal symmetric positive-definite. In Appendix B.2.3 we outline Algorithm 4 which (i) makes direct use of the structure of G, (i) computes the decomposition at blocks of reference and non-reference locations, and (i) requires no ex- ernal sparse matrix library, in no sparse Cholesky solvers. Along with h 5 for the block-computation of (I — L)* it can be used to compute A =(C ) where X is a block-diagonal matrix; it is thus useful in computing the Gaussian integrated ikelihood\n3.2 Estimation and Prediction\nWe introduce notation to aid in obtaining the full conditional distributions. Write (7) as\ny() ( ( +e()\nwhere y(€ ( D = ({e;(0) ~ N(0,D,), D, = diag(r{, X() = l (G The | x ¢ matrix Z(€) B = B ©=1,. ., 1) with = (z(),k =1 q) acts a design matrix for spatial ocation £ n all locations along the j-th margin, we build 7; = {£ s N, } and 7 = U;7;. We then call yli = (y(€), Yj )T and similarly, ) = w) (w( 1 (4) 3 w( ) dZ The full observed data are y, X, Z. Denoting the number of observations as n a1 , Z is thus a n x gn block-diagonal matrix, and similarly w is a gn x 1 vector. We introduce he diagonal matrix D, such that diag(D,,) (3,\nBy construction we may have n(S;) = v; and = v; ‘such that (£,€) € S; and ,¢) € Sj where £ £, & # ¢ and similarly for non-reference subsets. Suppose A C Dx = is a generic reference or non-reference subset We denote A C D x = as the set of all combinations of spatial locations of A and variables i.e. A | x Al where | CDis he set of unique spatial locations in A and are the unique latent variable coordinates\n(15)\n12\nSpatial Multivariate Trees\nBy subtraction we find A_ = A\\ A as the set of locations whose spatial location is in A = — X (A) = — but whose variable is not. Let y(A) = y() = ({y().£ € A X(A) b.diag{ X ( AS values corresponding to unobserved locations will be dealt with by defining D,,(A) as the diagonal matrix obtained from D,, by replacing unobserved outcomes with zeros. Denote Z(A) = b.diag{Z(), £ € Alp} and w(A) similarly. If A includes L unique spatial locations then y(A) is a L [ x 1 vector and X (A) is a L | X pl matrix. In _ particular, Z(A) is a L [ x Lgl matrix; the subset of its columns with locations in A is denoted as Z(A) whereas at other locations we get Z(A_). We can then separate the contribution of w(A) to y(A) from the contribution of w(A_) by writing y(A) = X (A)B+Z(A-)w(A_)+ Z(A)w(A) + e(A), using which we let (A) = — y(A) = X(A)B - Z(A )w(A-)\nWith customary prior distributions β ∼ N (0, V β) and τ 2 j ∼ Inv.Gamma(aτ , bτ ) along with a Gaussian SpamTree prior on w, we obtain the posterior distribution as\np(w, B,{7}=1,0|y) < ply | w, B, {7 }_)p(w] )\nWe compute the full conditional distributions of unknowns in the model, save for θ; iterating sampling from each of these distributions corresponds to a Gibbs sampler which ultimately leads to samples from the posterior distribution above.\n3.2.1 Full Conditional Distributions\nThe full conditional distribution for 3 is Gaussian with covariance 3 = (X "D x) 5 and mean pj = DZ. Forj=1,...,1 p( | B,y, w) = Inv.Gamma( 7 b:,) where 7 = a; + N;j/2 and b7 ; = b, + $ET EV) with ) = y) — X0)g; — Z)ap().\nTake a node v; € V. If v; € A then n\'(v;) = S; and for vj € Chlv;] denote w; = wj — H\\; ,jw\\; ;- The full conditional distribution of w is N(pi, i), where\n(e) S =Z(S) " D(S) \' Z(S) + R+ F i (e) i i = Z(S)) Du(S)\'9(S) + Ry \' H + m i (c) = i—j R\'H, ,; m i H, R {€Chlw]} w;€Chwi]}\nIfv; € Binstead ; = (Z(U;) \' D, (U) \' Z(U+R;) ™\' and p; = (Z(U) " D (U) 9 (Ui)+ R; H Sampling of w at nodes at the same level r proceeds in parallel given the assumed conditional independence structure in G. It is thus essential to minimize the com- putational burden at levels with a small number of nodes to avoid bottlenecks. In particular computing ) and m i (c) can become expensive at the root when the number of children is very large. In Algorithm 3 we show that one can efficiently sample at a near-root node v; by updating F\' ) and ) via message-passing from the children of v;.\n3.2.2 Update of θ\nThe full conditional distribution of —which may include §; for j = 1,...,¢ or equiva- lently d;; = [|€; — &;|| if the chosen cross-covariance function is defined on a latent do- main of variables is not available in closed form and sampling a posteriori can proceed\n(16)\n(17)\n13\nPeruzzi & Dunson\nInitialize: £ = 0; for r € {0 M} do for j: {v; € V,} do I Compute =(C;-C y ] 1) " and (=0 + log |R; Y| - (w; — Hjw;) R (w; — Hjwy; if Chlv;] # 0 then Identify v; € Ch[v] such that v; € V,; -1 Compute and store (possibly via (11));\n// [parallel for]\nResult: exp() o p(w | ) = L N(w; | Hw, R;).\nAlgorithm 1: Computing p(w | θ).\nInput:C; for all j from Algorithm 1; — W, = U w,= U Vs T is even is odd for i € {e,0} do for j: {v; € W;} do // Sample w; ~ N(p;,%;) using (17); (e) _ = Let Pav;] = {v,}, then m, P H\'w;and F) = HR\'H;\n// [parallel for]\nResult: sample from p(wj | w−j, y, β, θ, τ ) for all vj ∈ V .\nAlgorithm 2: Sampling from the full conditional distribution of wi when δ = 1.\nInput: C; for all j from Algorithm 1 Initialize: for all i, m i () = 04,1 and F = Oy for r € {M. s 0} do for j: {v; € V,} do Sample w; ~ N(;,%;) using (17); for p: {v, € Pav;]} do +H _ p— T R;\'H,;; F =FY + H p=J\n// [parallel for]\nResult: sample from p(wj | w−j, y, β, θ, τ ) for all vj ∈ V .\nAlgorithm 3: Sampling from the full conditional distribution of wj when δ = M .\n14\nSpatial Multivariate Trees\nvia Metropolis-Hastings steps which involve accept/reject steps with acceptance probability p(w|0)p(6")q(616") a = min{l, p(w[)p(0)q(6) } In our implementation, we adaptively tune the standard deviation of the proposal distribution via the robust adaptive Metropolis algorithm (RAM; Vihola, 2012). In these settings, unlike similar models based on DAG representations such as NNGPs and MGPs, direct computation via p(w | ) = [[; N(w; | Hwy, R;) is inefficient as it requires computing whose size grows along the hierarchy in G. We thus outline Algorithm 1 for computing p(w | ) via (11). As an alternative we can perform the update using ratios of p(y [3,0,7) = [p(y|w,B,7)p(w|)dw = N(y XZ\'Z + D,,) using Algorithms 4 and 5 outlined in Appendix B.2.3 which require no sparse matrix library.\n3.2.3 Graph Coloring for Parallel Sampling\nAn advantage of the treed structure of G is that it leads to ﬁxed graph coloring associated to parallel Gibbs sampling; no graph coloring algorithms are necessary (see e.g. Molloy and Reed, 2002; Lewis, 2016). Speciﬁcally, if δ = M (full depth) then there is a one to one correspondence between the M + 1 levels of G and graph colors, as evidenced by the parallel blocks in Algorithms 1 and 3. In the case δ = 1, G is associated to only two colors alternating the odd levels with the even ones. This is possible because the Markov blanket of each node at level r, with r even, only includes nodes at odd levels, and vice-versa.\n3.2.4 Prediction of the Outcome at New Locations\nThe Gibbs sampling algorithm will iterate across the above steps and, upon convergence, will produce samples from p(3, w | y). We obtain posterior predictive inference at arbitrary £ € D by evaluating p(y(£)|y). If £ € SUU, then we draw one sample of y(£) ~ N(X(€)"B+Z(£)"w(), D,(£)) for each draw of the parameters from p(83, w | y). — Otherwise, considering that 7(£) = v; € B for some j, with parent nodes Pa[v;], we sample w(£) from the full conditional N (, ), where 5 = (Z(£)Dn () Z () T+ R, ) and p = (Z(£)D 7 (y() X(0)"B) + Ry Hwy)), then draw y() ~ N(X(£)8 + Z() (¢),D,,)\n3.2.5 Computing and Storage Cost\nThe update of 72 and B can be performed at a minimal cost as typically p = D is small; almost all the computation budget must be dedicated to computing p(w | ) and sampling p(w |y, 3, 72). Assume that reference locations are all observed S C T and that all reference subsets have the same size i.e. |S;| = Nj for all . We show in Appendix B.5 that the cost of computing SPAMTREES is O(nN2). As a result, SPAMTREES compare favorably to other models specifically in not scaling with the cube of the number of samples. § does not impact the computational order, however, compared to § = M, choosing 6 = 1 lowers the cost by a factor of M or more. For a fixed reference set partition and corresponding nodes, choosing larger ¢ will result in stronger dependence between leaf nodes and nodes closer to the root this typically corresponds to leaf nodes being assigned conditioning sets that span larger distances in space. The computational speedup corresponding to choosing 6 = 1 can effectively be traded for a coarser partitioning of S, resulting in large conditioning sets that are more local to the leaves.\n15\nPERUZZI & DUNSON\nFull dataset\nObserved dataset\nY4(s), s €D =(0,1 Y,(s), s €D =(0,1 1.00 A ‘¥ 075 ¥ Al ( o 2 - 3 L) . - . v | 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00\nY4(s), s €D =(0,1 Y,(s), s €D =(0,1 1.00 0.75 . 0.50 % 0.25 0.00 f 0.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.00\nFigure 3: Left half: Full data set – a bivariate outcome is generated on 4,900 spatial locations. Right half: Observed data set – the training sample is built via independent subsampling of each outcome.\n4. Applications\nWe consider Gaussian SPAMTREES for the multivariate regression model (15). Consider the spatial locations £,£\' € D and the locations of variables i and j in the latent domain of variables §;,§; € E, then denote h le—, A 1 &ll, and\nexp {—¢llh|exp {3810(1 + a)} } C(h,) = exp {Blog(1+ aA)}\nForj=1,. ,q we also introduce C(h) = exp{—¢;|h||}. A non-separable cross-covariance function for a multivariate process can be defined as\n101C(h, d;) if i # j, Cov(w(,&;), w(,;)) = Cyj(h) =\nwhich is derived from eq. (7) of Apanasovich and Genton (2010); locations of variables in the latent domain are unknown, therefore θ = {σi1, σi2, φi}i=1,...,q ∪ {δij}j<i for a total of 3q + q(q − 1)/2 + 3 unknown parameters. i=1,...,q ∪ {α, β, φ}\n4.1 Synthetic Data\nIn this section we focus on bivariate outcomes (q = 2). We simulate data from model (15), setting β = 0, Z = Iq and take the measurement locations on a regular grid of size 70 × 70 for a total of 4,900 spatial locations. We simulate the bivariate spatial ﬁeld by sampling from the full GP using (18) as cross-covariance function; the nuggets for the two outcomes are set to τ 2 1 = 0.01 and τ 2 2 = 0.1. For j = 1, 2 we ﬁx σj2 = 1, α = 1, β = 1 and independently sample σj1 ∼ U (−3, 3), φj ∼ U (0.1, 3), φ ∼ U (0.1, 30), δ12 ∼ Exp(1), generating a total of 500 bivariate data sets. This setup leads to empirical spatial correlations between the two outcomes smaller than 0.25, between 0.25 and 0.75, and larger than 0.75 in absolute value in 107, 330, and 63 of the 500 data sets, respectively. We introduce misalignment and make the outcomes imbalanced by replacing the ﬁrst outcome with missing values at ≈ 50% of the spatial locations chosen uniformly at random, and then repeating this procedure for the\n(18)\n16\nSpatial Multivariate Trees\nsecond outcome keeping only ≈ 10% of the total locations. We also introduce almost-empty regions of the spatial domain, independently for each outcome, by replacing observations with missing values at ≈ 99% of spatial locations inside small circular areas whose center is chosen uniformly at random in [0, 1]2. As a result of these setup choices, each simulated data set reproduces some features of the real-world unbalanced misaligned data we consider in Section 4.2 at a smaller scale and in a controlled experiment. Figure 3 shows one of the resulting 500 data sets.\nWe consider SPAMTREES with 6 = 1 and implement multiple variants of SPAMTREES with = M in order to assess their sensitivity to design parameters. Table 1 reports implementation setups and the corresponding results in all cases; if the design variable “All outcomes at £7 is set to “No” then a SPAMTREE is built on the D x E domain. If it is set to “Yes” the DAG will be built using D only in other words, the ¢ margins of the latent process are never separated by the DAG if they are measured at the same spatial location. “Cherry pick same outcome” indicates whether the map 7(+) should search for neighbors by first filtering for matching outcomes refer to our discussion at Section 2.1.2. We mention here if the DAG is built using D only, then the nearest neighbor found via cherry picking will always include a realization of the same margin of w(-) Finally, if SPAMTREE is implemented with “Root bias” then the reference set and the DAG are built with locations of the more sparsely observed outcome closer to root nodes of the tree as suggested by Proposition 1.\nSpamTrees are compared with multivariate cubic meshed GPs (Q-MGPs; Peruzzi et al., 2020), a method based on stochastic partial diﬀerential equations (Lindgren et al., 2011) estimated via integrated nested Laplace approximations (Rue et al., 2009) implemented via R-INLA using a 15 × 15 grid and labeled spde-inla, a low-rank multivariate GP method (labeled lowrank) on 25 knots obtained via SpamTrees by setting M = 1 with no domain partitioning, and an independent partitioning GP method (labeled ind-part) implemented by setting M = 1 and partitioning the domain into 25 regions. Refer e.g. to Heaton et al. (2019) for an overview of low-rank and independent partitioning methods. All multivariate SpamTree variants, Q-MGPs, lowrank and ind-part use (18) as the cross-covariance function in order to evaluate their relative performance in estimating θ in terms of root mean square error (RMSE) as reported in Table 1. We also include results from a non- spatial regression using Bayesian additive regression trees (BART; Chipman et al., 2010) which uses the domain coordinates as covariates in addition to a binary ﬁxed eﬀect cor- responding to the outcome index. All methods were setup to target a compute time of approximately 15 seconds for each data set. We focused on comparing the diﬀerent methods under computational constraints because (a) without constraints it would not be feasible to implement the methods for many large simulated spatial datasets; and (b) the diﬀerent methods are mostly focused on providing a faster approximation to full GPs; if constraints were removed one would just be comparing the same full GP method.\nTable 1 reports average performance across all data sets. All Bayesian methods based on latent GPs exhibit very good coverage; in these simulated scenarios, SpamTrees exhibit comparatively lower out-of-sample prediction errors. All SpamTrees perform similarly, with the best out-of-sample predictive performance achieved by the SpamTrees cherry picking based solely on spatial distance (i.e. disregarding whether or not the nearest-neighbor belongs to the same margin). Additional implementation details can be found in Appendix\n17\nPeruzzi & Dunson\nPrediction RMSE relative to independent univariate NNGPs\n110.0% 100.0% 90.0% “ 80.0% 70.0% -10 -05 0.0 05 1.0 Correlation between fully observed outcomes\nFigure 4: Predictive RMSE of the best-performing SpamTree of Table 1 relative to in- dependent univariate NNGP models of the two outcomes, for diﬀerent empirical correla- tions between the two outcomes in the full data. Lower values indicate smaller errors of SpamTrees in predictions.\nC.2.1. Finally, we show in Figure 4 that the relative gains of SpamTrees compared to independent univariate NNGP model of the outcomes are increasing with the magnitude of the correlations between the two outcomes, which are only available due to the simulated nature of the data sets.\n<table><thead><tr><th></th><th>All outcomes at £</th><th>Cherry pick same outcome</th><th>Root bias</th><th>RMSE(y)</th><th>MAE(y)</th><th>COVG(y)</th><th>RMSE(0)</th></tr></thead><tbody><tr><td rowspan="6">SPAMTREES § = ]</td><td>No</td><td>No</td><td>No</td><td>1.078</td><td>.795</td><td>0.95: = &amp;</td><td>4.168</td></tr><tr><td>No</td><td>No</td><td>Yes</td><td>1.065</td><td>07</td><td>0. &amp;</td><td>4.138</td></tr><tr><td>No</td><td>Yes</td><td>No</td><td>1.083</td><td>0. 9</td><td>0.</td><td>4.168</td></tr><tr><td>No</td><td>Yes</td><td>Yes</td><td>1.085</td><td>9</td><td>0.</td><td>4.138</td></tr><tr><td></td><td>Yes</td><td>No</td><td>1.081</td><td></td><td>0.</td><td>4.080</td></tr><tr><td></td><td>Yes</td><td>es</td><td>1.087</td><td>801</td><td>0.</td><td>4.188</td></tr><tr><td>SPAMTREES § = 1</td><td></td><td>Yes</td><td></td><td>1.198</td><td>0.880</td><td>0.956</td><td>4.221</td></tr><tr><td>QMGP</td><td></td><td></td><td></td><td>1.1 5</td><td>.819</td><td>0.951</td><td>39</td></tr><tr><td>IND-PART</td><td>es</td><td></td><td>-</td><td>1 3 4</td><td>1.229</td><td>0.948</td><td>s 4</td></tr><tr><td>LOWRANK</td><td>les</td><td></td><td></td><td>1 2</td><td>1.1</td><td>0.9</td><td>5.647</td></tr><tr><td>SPDE-INLA</td><td>e es</td><td></td><td></td><td></td><td>0.8 2</td><td>.</td><td></td></tr><tr><td>SPAMTREES Univariate</td><td></td><td></td><td></td><td>1.147</td><td>46</td><td>0.953</td><td></td></tr><tr><td>NNGP uvnivariate</td><td></td><td></td><td></td><td>1.129</td><td>.832</td><td>0.</td><td></td></tr><tr><td>BART</td><td></td><td></td><td></td><td>75</td><td>1.036</td><td>0.488</td><td></td></tr></tbody></table>\nTable 1: Prediction and estimation performance on multivariate synthetic data. The four columns on the right refer to root mean square error (RMSE) and mean absolute error (MAE) in out-of-sample predictions, average coverage of empirical 95% prediction intervals, and RMSE in the estimation of θ.\n4.2 Climate Data: MODIS-TERRA and GHCN\nClimate data are collected from multiple sources in large quantities; when originating from satellites and remote sensing, they are typically collected at high spatial and relatively\n18\nSpatial Multivariate Trees\nlow temporal resolution. Atmospheric and land-surface products are obtained via post- processing of satellite imaging, and their quality is negatively impacted by cloud cover and other atmospheric disturbances. On the other hand, data from a relatively small number of land-based stations is of low spatial but high temporal resolution. An advantage of land- based stations is that they measure phenomena related to atmospheric conditions which cannot be easily measured from satellites (e.g. precipitation data, depth of snow cover).\nWe consider the joint analysis of ﬁve spatial outcomes collected from two sources. First, we consider Moderate Resolution Imaging Spectroradiometer (MODIS) data from the Terra satellite which is part of the NASA’s Earth Observing System. Speciﬁcally, data product MOD11C3 v. 6 provides monthly Land Surface Temperature (LST) values in a 0.05 degree lat- itude/longitude grid (the Climate Modeling Grid or CMG). The monthly data sets cover the whole globe from 2000-02-01 and consist of daytime and nighttime LSTs, quality control as- sessments, in addition to emissivities and clear-sky observations. The second source of data is the Global Historical Climatology Network (GHCN) database which includes climate sum- maries from land surface stations across the globe subjected to common quality assurance reviews. Data are published by the National Centers of Environmental Information (NCEI) of the National Oceanic and Atmospheric Administration (NOAA) at several diﬀerent tem- poral resolutions; daily products report ﬁve core elements (precipitation, snowfall, snow depth, maximum and minimum temperature) in addition to several other measurements.\nWe build our data set for analysis by focusing on the continental United States in Oc- tober, 2018. The MODIS data correspond to 359,822 spatial locations. Of these, 250,874 are collected at the maximum reported quality; we consider all remaining 108,948 spa- tial locations as missing, and extract (1) daytime LST (LST_Day_CMG), (2) nighttime LST (LST_Night_CMG), (3) number of days with clear skies (Clear_sky_days), (4) number of nights with clear skies (Clear_sky_nights). From the GHCN database we use daily data to obtain monthly averages for precipitation (PRCP), which is available at 24,066 spatial loca- tions corresponding to U.S. weather stations; we log-transform PRCP. The two data sources do not share measurement locations as there is no overlap between measurement locations in MODIS and GHCN, with the latter data being collected more sparsely—this is a scenario of complete spatial misalignment. From the resulting data set of size n =1,027,562 we remove all observations in a large 3 × 3 degree area in the central U.S. (from -100W to -97W and from 35N to 38N, i.e. the red area of Figure 5) to build a test set on which we calculate coverage and RMSE of the predictions.\nWe implement SpamTrees using the cross-covariance function (18). Considering that PRCP is more sparsely measured and following Proposition 1, we build SpamTrees favoring placement of GHCN locations at root nodes. We compare SpamTrees with a Q-MGP model built on the same cross-covariance function, and two univariate models that make predictions independently for each outcome. Comparisons with other multivariate methods are diﬃcult due to the lack of scalable software for this data size which also deals with misalignment and imbalances across outcomes. Compute times per MCMC iteration ranged from 2.4s/iteration of the multivariate Q-MGP model, to 1.5s/iteration of the univariate NNGP model. The length of the MCMC chains (30,000 for SpamTrees and 20,000 for Q-MGP) was such that the total compute time was about the same for both models at less than 16 hours. Univariate models cannot estimate cross-covariances of multivariate outcomes and are thus associated to faster compute times; we set the length of their MCMC\n19\nPeruzzi & Dunson\n\ni | i L = ) o usona | ot L\nFigure 5: Prediction area\n<table><thead><tr><th colspan="2" rowspan="2">MODIS/GHCN variables</th><th colspan="2">Multivariate</th><th colspan="2">Univariate</th></tr><tr><th>SPAMTREE</th><th>Q-MGP</th><th>| SPAMTREE</th><th>NNGP</th></tr></thead><tbody><tr><td rowspan="2">Clear_sky_days</td><td>RMSE</td><td>1.611</td><td>928</td><td>1.466</td><td>1.825</td></tr><tr><td>COVG</td><td>0.980</td><td>0.866</td><td>0.984</td><td>0.986</td></tr><tr><td rowspan="2">Clear_sky_nights</td><td>RMSE</td><td>1.621</td><td>766</td><td>2.002</td><td>2.216</td></tr><tr><td>COVG</td><td>0.989</td><td>0.943</td><td>0.992</td><td>0.992</td></tr><tr><td rowspan="3">LST_Day_CMG</td><td>RMSE</td><td>1.255</td><td>699</td><td>1.645</td><td>1.666</td></tr><tr><td>COVG</td><td>1.000</td><td>000</td><td>1.000</td><td>1.000</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td rowspan="2">LST_Night_CMG</td><td>RMSE</td><td>1.076</td><td>402</td><td>0.795</td><td>1.352</td></tr><tr><td>COVG</td><td>0.999</td><td>0.999</td><td>1.000</td><td>1.000</td></tr><tr><td rowspan="2">PRCP</td><td>RMSE</td><td>0.517</td><td>0.632</td><td>0.490</td><td>0.497</td></tr><tr><td>COVG</td><td>0</td><td>000</td><td>0.</td><td></td></tr></tbody></table>\nTable 2: Prediction results over the 3 × 3 degree area shown in Figure 5\nchains to 15,000 for a total compute time of less than 7 hours for both models. We provide additional details about the models we implemented at Appendix C.\nTable 2 reports predictive performance of all models, and Figure 6 maps the predictions at all locations from SpamTrees and the corresponding posterior uncertainties. Multivariate models appear advantageous in predicting some, but not all outcomes in this real world illustration; nevertheless, SpamTrees outperformed a Q-MGP model using the same cross- covariance function. Univariate models perform well and remain valid for predictions, but cannot estimate multivariate relationships. We report posterior summaries of θ in Appendix C.2.2. Opposite signs of σi1 and σj1 for pairs of variables i, j ∈ {1, . . . , q} imply a negative relationship; however, the degree of spatial decay of these correlations is diﬀerent for each pair as prescribed by the latent distances in the domain of variables δij. Figure 7 depicts the resulting cross-covariance function for three pairs of variables.\n5. Discussion\nIn this article, we introduced SpamTrees for Bayesian spatial multivariate regression mod- eling and provided algorithms for scalable estimation and prediction. SpamTrees add\n20\nSpatial Multivariate Trees\nMODIS-TERRA GHCN Clear_sky_days Clear_sky_nights LST_Day_CMG LST_Night_CMG 3 n\nFigure 6: Predicted values of the outcomes at all locations (top row) and associated 95% uncertainty (bottom row), with darker spots corresponding to wider credible intervals.\nPRCP vLST_Day_CMG PRCP vLST_Night CMG LST_Day_CMG v Clear_sky_nights 1.00 1.00 0.75 o01 =027 -1 0.50 0.50 0,0y =0.42 -2 o110y =3.41 Cross Covariance 0.25 0.25 -3 == | - Spatial distance in degrees\nFigure 7: Given the latent dimensions δij, the color-coded lines represent C(h, δij) whereas Cij(h) = σ1iσ1jC(h, δij) is shown as a dashed grey line.\n21\nPeruzzi & Dunson\nsigniﬁcantly to the class of methods for regression in spatially-dependent data settings. We have demonstrated that SpamTrees maintain accurate characterization of spatial de- pendence and scalability even in challenging settings involving multivariate data that are spatially misaligned. Such complexities create problems for competing approaches, including recent DAG-based approaches ranging from NNGPs to MGPs.\nOne potential concern is the need for users to choose a tree, and in particular specify the number of locations associated to each node and the multivariate composition of locations in each node. Although one can potentially estimate the tree structure based on the data, this would eliminate much of the computational speedup. We have provided theoretical guidance based on KL divergence from the full GP and computational cost associated to diﬀerent tree structures. This and our computational experiments lead to practical guidelines that can be used routinely in tree building. Choosing a tree provides a useful degree of user-input to reﬁne and improve upon an approach.\nWe have focused on sampling algorithms for the latent eﬀects because they provide a general blueprint which may be used for posterior computations in non-Gaussian outcome models; eﬃcient algorithms for non-Gaussian big geostatistical data sets are currently lacking and are the focus of ongoing research. SpamTrees can be built on larger dimensional inputs for general applications in regression and/or classiﬁcations; such a case requires special considerations regarding domain partitioning and the construction of the tree. In particular, when time is available as a third dimension it may be challenging to build a sparse DAG with reasonable assumptions on temporal dependence. For these reasons, future research may be devoted to building sparse DAG methods combining the advantages of treed structures with e.g. Markov-type assumptions of conditional independence, and applying SpamTrees to data with larger dimensional inputs.\nAcknowledgments\nThis project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 856506). This project was also partially funded by grant R01ES028804 of the United States National Institutes of Health (NIH).\n22\nSpatial Multivariate Trees\nAppendix A. Kolmogorov consistency conditions for SpamTrees\nWe adapt results from Datta et al. (2016a) and Peruzzi et al. (2020) Let w(s),s € D* be the univariate representation in the augmented domain of the multivariate base process {w £ € D C RY. Fix the reference set S C D* and let £ = {£, } C D* and U=L\\S. Then\n(wy | ws)p(ws) )( s,€S\\L l = wy | ws) dw(£;) dw(;) =1, L £;eS\nhence p(w) is a proper joint density. To verify the Kolmogorov consistency conditions take the permutation L, = {£ mand call Uy = Lo\\ S. Clearly Uy = L\\ S = L\\ 8§ =U and similarly S\\ Lr = S\\ £ so that\nwe,) Blwy, |ws)(ws) dus) s,€S\\L ~ [ | ws)tws) T] dus) = pwe) s€S\\L\nimplying\n( (€1) (€)) w(l), s W(L(ny))\nNext, take a new location location £y € D* Call £1 = LU {£}. We want to show that J lwe,)dw(€y) = plw). If €9 € S then £1\\ S = L\\ S =U and hence\ndw(£y) Plwe | ws)p(ws) dw(s i€S\\L = plwe) = [ w | wsptws) T dus s;€S\\L\nIf £y ¢ S we have\n[eten) = [ plws) [] ,€S8\\L1 dw(£p) (wue | ws)ps) dw(s) | dw(o) s, €S\\L (wigy | ws, ws)p(wes | ws)plws dw(s;) | dw(£) ,€S\\L\n23\nPeruzzi & Dunson\n- lwes | w)(ws) dus Bie | ws)dw(o) s, ES\\L ~ [ wes ws)tws) [T duts) s;eS\\L =p(w).\nAppendix B. Properties of Gaussian SpamTrees\nConsider the treed graph G of a SpamTree. In this section, we make no distinction between reference and non-reference nodes, and instead label V i = Ai for i = 0, . . . , M −1 and V M = B so that V = {A, B} = {V 0, . . . , V M −1, V M } and the V M are the leaf nodes. Each wi is ni × 1 and corresponds to vi ∈ V r for some r = 0, . . . , M so that Pa[vi] = {vj1, . . . , vjr } for some sequence {j1, . . . , jr}, and η−1(Pa[vi]) = {Sj1, . . . , Sjr }. Denote the h-th parent of vi as Pa[vi](h).\nB.1 Building the precision matrix\nWe can represent each conditional density N (wi | H iw[i], Ri) as a linear regression on wi:\nw = wy ~ N(0, Ry), w; = w i=1,2,...,M, {jvjePalvil}\nwhere each hij is an ni × nj coeﬃcient matrix representing the regression of wi given w[i], ωi ind∼ N (0, Ri) for i = 0, 1, . . . , M , and each Ri is an ni × ni residual covariance matrix. We set hii = O and hij = O, where O is the matrix of zeros, whenever j /∈ {j1, . . . , jr}. Using this representation, we have H i = [hi,j1, hi,j2, . . . , hi,jr ], which is an ni × Ji block matrix formed by stacking hi,jk side by side for k = 1, . . . , r. Since E[wi | w[i]] = H iw[i] = Ci,[i]C−1 [i] w[i], we obtain H i = Ci,[i]C−1 [i] . We also obtain Ri = var{wi | w[i]} = Ci,i − Ci,[i]C−1 [i] C[i],i, hence all H i’s, hij’s, and Ri’s can be computed from the base covariance function.\nIn order to continue building the precision matrix, deﬁne the block matrix H = {hij}. We can write\no if v; ¢ Palv (Cip ) h) = Hi( if vj = v, € Pafv],\nwhere (·, h) refers to the h-th block column. More compactly using the indicator function 1{·} we have hij = 1{∃h : vj = Pa[vi](h)}(Ci,[i]C−1 [i] )(·, h). If we stack all the hik hor- izontally for k = 0, . . . , MS − 1, we obtain the ni × n matrix H(i, ·), which is i-th block row of H. Intuitively, H(i, ·) is a sparse matrix with the coeﬃcients linking the full w to wi, with zero blocks at locations whose corresponding node is vj /∈ Pa[vi]. The ith block-row of H is of size ni × n but only has r non-empty sub-blocks, with sizes ni × nj for j ∈ {j1, . . . , jr}, respectively. Instead, H i is a dense matrix obtained by dropping all the zero-blocks from H(i, ·), and stores the coeﬃcients linking w[i] to wi. The two are linked as H iw[i] = H(i, ·)w.\n(19)\n(20)\n24\nSpatial Multivariate Trees\nI Since w Hw T w, C var(w) (I —H)\'R(I — H), where R = b.diag{R;} and I — H is block lower-triangular with unit diagonal, hence non-singular. We find the precision matrix as \' =(I-H)YI-H).\nB.2 Properties of ’\nWhen not ambiguous, we use the notation X;; to denote the (4, j) block of X. An exception to this is the (¢, j) block of € which we denote as C (7). In SPAMTREES, ( )is nonzero if i = j or if the corresponding nodes v; and v; are connected in the moral graph G™, which is an undirected graph based on G in which an edge connects all nodes that share a child. This means that either (1) v; € Pav;] or vice-versa, or (2) there exists a* such that {v;,v;} C Pa[a*]. In SPAMTREES, G = G. In fact, suppose there is a node a* € v,+ such that @* € Chlv;] N Chlvg], where v; € v,; and vy € v,. By definition of G there exists a sequence {i ’ ’ i+} such that Paa*] = {v;, ’ ’ v;.} D {v;,v}, and urthermore Pav;, | = {v;,, . for h < r*. This implies that if j = k then v; = vy, whereas if j > k then v € Pa[v;], meaning that no additional edge is necessary to build Ggm\nB.2.1 EXPLICIT DERIVATION OF C (i,\nDenote R™! = U=(I- . and define the “common descendants” as cd(v,vj) = ({v} U Chlv]) N ({v;} U Chlv]). Then consider a; € A,v; € V such hat a; € and denote as H;_,; the matrix obtained by subsetting H; to columns corresponding to a; and note that H;_,; = H ;. The (i, ) block of U is then\nif h ifi=j U (I W = ( —\nThen ’ J) =2 UU and, as in (9), each block of the precision matrix is\n~—1 (& (4,5) > ( Hi ) (I; — H vcd(v,v;) > ( — "Ry ( — vcd(v,v;)\nwhere cd(v;,v;) = 0 implies ( = O and I;; a zero matrix unless i = j as it is the (4,7) block of an identity matrix of dimension n x n.\nB.2.2 Computation of large matrix inverses\nOne important aspect in building ’ is that it requires the computation of the inverse (& [ of dimension J; x J; for all nodes with parents, i.e. at r > 0. Unlike models which achieve scalable computations by limiting the size of the parent set (e.g. NNGPs and their\n(21)\n25\nPeruzzi & Dunson\nblocked variant, or tessellated MGPs), this inverse is increasingly costlier when δ > 1 for nodes at a higher-level of the tree as those nodes have more parents and hence larger sets of parent locations (the same conclusion holds for non-reference nodes). However, the treed structure in G allows one to avoid computing the inverse in O(J 3 i ). In fact, suppose we have a symmetric, positive-deﬁnite block-matrix A and we wish to compute its inverse. We write\nC B ct+coBsiTot cBs— D -Tt |\nwhere S = C\' — BDB is the Schur complement of D in A. If C~! was available, the only necessary inversion is that of S. In SPAMTREES with ¢ > 1, suppose v;, v; are two nodes such that Pa[v;] = {v;} U Pa[v;] this arises for nodes v; € V,7 > M;. Regardless of whether v; is a reference node or not, (Pafv;]) = {Si, S} and\nyc C Cp i] \'C)SI C —1 [i] Ci Ci 7CC [i]\nwhere the Schur complement of Ci is S = Ci − Ci,[i]C−1 [i] C[i],i = Ri. Noting that H i = Ci,[i]C−1 [i] we write\n—1 +H\'R \'H li] ; —HR;\' C -R\'H; Q |\nB.2.3 COMPUTING ( + X)! AND ITS DETERMINANT WITHOUT SPARSE CHOLESKY\nBayesian estimation of regression models requiring the computation of (Z cz + D)and its determinant use the Sherman-Morrison-Woodbury matrix identity to find Z zZ + D =D \' - DZC \' +%)\'Z2" D, where £ = Z\' D\'Z. A sparse Cholesky ctorization of + X can be used as typically X is diagonal or block-diagonal, thus maintaining the sparsity structure of Sparse Cholesky libraries (e.g. Cholmod, Chen et al., 2008), which are embedded in software or high-level languages such as MATLAB™ or he Matrix package for R, scale to large sparse matrices but are either too flexible or too restrictive in our use cases: (1) we know G and its properties in advance; (2) SPAMTREES take advantage of block structures and grouped data. In fact, sparse matrix libraries typically are agnostic of G and heuristically attempt to infer a sparse G given its moralized counterpart. While this operation is typically performed once, a priori knowledge of G implies that reliance on such libraries is in principle unnecessary.\nWe thus take advantage of the known structure in G to derive direct algorithms for computing - + ) and its determinant. In the discussion below we consider § = M, noting here that choosing ¢ = 1 simplifies the treatment as cd(v;, v;) = {v;} if v; = v, and it is empty otherwise. We now show how (21) leads to Algorithm 4 for the decomposition of any precision matrix A which conforms to G i.e. it has the same block-sparse structure as a precision matrix built as in Section B.1. Suppose from A we seek a block lower-triangular matrix L and a block diagonal D such that\nA ij = > (ki — Lyi) " D(I; — Ly). vcd(v,v)\n(22)\n26\nSpatial Multivariate Trees\nStart with v;,v; taken from the leaf nodes, i.e. v;,v; € V Then cd(v,v;) = 0 and we L(j,i) then cd(v;,v;) = {v;} and set L; O=Ay Ifi\n> (I — L " Di(I — Li) = (I L\' Di(I; L(i,)) vy €cd(v;,v;) D; - D;L;; LD,+LD;L;\nwe then set L; = O and get the i-th block of D; simply setting D; = A;;. Proceeding downwards along G, if v; € V N Pav;] we have A; = D(I; = and = — thus set L; We then note that cd(vj,vj) = {v;,v} and obtain Aj; = D; + LDL where L;; and D; have been fixed at the previous step; this results in D=Aj; — LDL;.\nThen, the s-th (of M) step takes v; € V_s N Pav;] and v; € V_g1, implying l = ] h Noting that F* = h - - has been fixed at previous steps since each vy is at level M — s+ 2, we split the sum in (21) and get\nΛij − F ∗ = Di(I ij − Lij) = −DiLij,\nwhere D; has been fixed at step s — 1, obtaining L;; = i (Aij — F*); Dj can be found using the same logic. Proceeding until M — s = 0 from the leaves of G to the root, we ultimately fill each non-empty block in L and D resulting in A = (I — L)\' D(I — L). Algorithm 4 unifies these steps to obtain the block decomposition of any sparse precision matrix A conforming to G resulting in A = (I — L)\' D(I — L), where L is block lower triangular and D is block diagonal. This is akin to a block-LDL decomposition of A indexed on nodes of G. Algorithm 5 complements this decomposition by providing a G-specific block version of forward substitution for computing (I — L)™ with L as above.\nIn practice, a block matrix with K? blocks can be represented as a K? array with rows and columns indexed by nodes in G and matrix elements which may be zero-dimensional whenever corresponding to blocks of zeros. The specification of all algorithms in block notation allows us to never deal with large (sparse) matrices in practice but only with small block matrices indexed by nodes in G, bypassing the need for external sparse matrix libraries. Specifically we use the above algorithms to compute A! = ’ ) and its determinant: Al = I- DI — ) and = / We have not distinguished non- reference and reference nodes in this discussion. In cases in which the non-reference set is arge, we note that the conditional independence of all non-reference locations, given their parents, results in i) being diagonal for all £ € U (i.e. n(€) = v; € B). This portion of the precision matrix can just be stored as a column vector.\nB.2.4 SPARSITY OF C\nWe calculate the sparsity in the precision matrix; considering an enumeration of nodes by evel in G, denote n; = [n(vi;), m; = |V and J;; = [n~(Pa[vy;])|, and noting that by symmetry (C (i,5)) = ~ (j,i), the number of nonzero elements of is\nM m = + <M} +nl{j= , =0i=1\n27\nPeruzzi & Dunson\nInput: A n X n precision matrix conforming to G Initialize L = Opxn, D = Opxn; for r € {M,...,0} do for j: {v; € V,} do Dj; = Ajj; for p: € Pav;]} do Lj, = 7P} for g : {v, € Pafv;]} do Apg = Apg — Agp =A\n// top down from last level // [parallel for]\nResult: Block-lower-triangular L with L;; # O if v; € Pav;], and block-diagonal D such that (I — L)\' D(I — L) = A.\nAlgorithm 4: Precision matrix decomposition given treed graph G with M levels.\nInput: I\' = I — L where L is as in Algorithm 4. Initialize Aj = Oy, , for all 4, j such that v; € Pav;]; for r € {0,...,M} do // bottom up from root of G // [parallel for] for j: {v; € V,} do for p:{v, € do Set chain(vy, v;) = {vp} U ‘] n for g : {v, € chain(v,,v;)} do gp L Ajp =45 Tj\nResult: ∆ = Γ−1.\nAlgorithm 5: Calculating the inverse of I − L with L output from Algorithm 4.\n28\nSpatial Multivariate Trees\nwhere nij1{j = M } refers to the diagonal elements of the precision matrix at non-reference locations.\nB.3 Properties of SpamTrees with δ = M\nWe outline recursive properties of C induced by G when 6 = M. In the case 1 < § < M, these properties hold for nodes at or above level M, using Ay as root. We focus on paths in G. These can be represented as sequences of nodes {vj,,...,v; } such that 05} C Paf ] for 1 < j < k < r. Take two successive elements of such a sequence, i.e. v, v; such that v; — v; in G. Consider Ew; | wy;)] = Hjw;) = and R; = var{w; |w;} = C;; — ]]\' By (22) we can write\n\'+H/R\'H, ; Hjw) =[Cj C ¢ i] ~R;H; R; | [ w; | —1 o Cj 0 (Cii = Ciy) ™" I w; — C;;)C [ =[CiC (Cri-C [i] \'Cl)(C w; — C |\nNow define the covariance function K;(£,£\') = Cp—CyC ij,¢; recalling that the ref- m erence set is S = U J 1j we use a shorthand notation for these subsets: K (S S) = K (h, k). Also denote e; = w; CiC for all . The above expression becomes\n1 -1 = i] i €; | = Hw) + K;(j, 1)) K i —1 (i,9)e;\nwe can use this recursively on {vi0, vi1, . . . , vir } where vi0 ∈ A0 and vir = vj and get\ni1 Hjw =Y K(j,)K; (s, 5)es s=i i Elw; |w] =Y Ee,[w; | ed], s=i\nwhere the expectations on the r.h.s. are taken with respect to the distributions of es which are Gaussian with mean zero and var{e} = K (h,h) this is a compact expression of the conditionals governing the process as prescribed by G. We can also write the above as E(w|w = Y, Ks(, ) K (s,8)(ws — Elw | wy)); using Ele, | w, wy] = 0, for h < k we find\ncov{eh, ek} = E[cov{eh, ek | wh, w[h]}] + cov{E[eh | wh, w[h]], E[ek | wh, w[h]]} = cov{E[eh | wh, w[h]], E[ek | wh, w[h]]} = 0.\n29\nPeruzzi & Dunson\nThe above results also imply C, = — J L 1C C K (j,s)K; " (s,5)K(s,) and suggest an additive representation via orthogonal basis functions:\nr1 w; =Y K (j,K;\'(s,5)es + ¢; s=ip\n(23)\nFinally, considering the same sequence of nodes, recursively introduce the covariance functions F 0(r, s) = Cr,s and for j > 1, F j(r, s) = F j−1(r, s)−F j-1(r, j-1)F −1 We get j-1(j-1, j-1)F j-1(j-1, s).\nF j+1(r, s) = F j(r, s) − F j(r, j)F −1 j (j, j)F j(j, s) using (22) = F j−1(r, s) − F j−1(r, [j-1:j])F −1 j−1([j-1:j], [j-1:j])F −1 j−1([j-1:j], s) = C(r, s) − C(r, [0:j])C−1([0:j], [0:j])C([0:j], s) = C(r, s) − Cr,[j+1]C−1 [j+1]C[j+1],s\nwhich can be iterated forward and results in an additional recursive way to compute co- variances in SpamTrees. Notice that while Kj is formulated using the inverse of Jj × Jj matrix C[j], the F j’s require inversion of smaller nj × nj matrices F j−1(j-1, j-1).\nB.4 Properties of c\nB.4.1 δ = 1\nChoosing depth 6 = 1 results in each node having exactly 1 parent. In this case the path Prj = {vir .,v;,} from vy to v, where v;;, = vy, v;, = vj and {v;,} = Palv;,], is unique, and there is thus no distinction between shortest and longest paths: P_; = Prj = Then denote H,; = H; - H;,_, H;,. Let v, be the concestor be- tween v; and v; i.e. v, = con(v;,v;) = argmaxy,cv{k : Pi N Py; # 0} and the associated paths P,; = {v;,, and P. = {vj,, . where v;, = vj = v., ~ v;,, = v; and v;, = v;. Then we can write w; = w;, = ir, Wiy + Vi, where v; N(0, R;,,) and proceed expanding wi, _ to get wi, = Hi, (H, _ wi, _,+Vi, )+, H; H; w; ,+(H; v, ,+v; ; continuing downwardly along the tree we eventually find w; = H - Hiwi + = H,;w, + v; where is independent of w,. After proceeding analogously with wj, take £;,£; such that (¢;) = v; and (£;) = v;. Then\nCovi(w(;), w(;)) = )(\n(24)\nwhere H . _,;(£;) = C(;, and similarly for (5).\nB.4.2 1 < δ < M\nTake two nodes v,v; € V. If Pav;] N Palv;] # () then we apply the same logic as in B.4.3 using v, = con(v;,v;) as root. If Pafv;] N Pafv;] = and both nodes are at levels below M then we use B.4.1. The remaining scenario is thus one in which v; € A,, r > Ms and\n30\nSpatial Multivariate Trees\nPa[vi] ∩ Pa[vj] = ∅. We take vj ∈ As, s < Mδ for simplicity in exposition and without loss of generality. By (23)\n1 w; = K. (i,s)K (s, s)es + ei, s=ing r—1 = Y K(i,9)K;(s,s)e + Clw + e, S=Mg+1\nwhere vx ∈ AMδ is the parent node of vi at level Mδ. The ﬁnal result of (14) is then achieved by noting that the relevant subgraph linking vx and vj has depth δx = 1 and thus Cov(wx, wj) can be found via B.4.1, then Cov(wi, wj) = CixC−1 Cov(wx, wj) = x F iCov(wx, wj). Notice that F i directly uses the directed edge vx → vi in G; for this reason the path between wi and wz = con(wx, wj) is the actually the shortest path and we have vz → · · · → vx −→ vi.\nB.4.3 δ = M\nTake v;,v; € V and the full paths from the root = {i ’ ’ ir,} and = {o, ,Jr; } respectively. Then using (23) we have\nw; = K, (i,s)Ks,5)es +e; €Poi 2 K, (i,s)Ks,s)es + 2 K (i,s)K (s,s)es + €; 2 K (i,s)Ks,s) + w K,(j, s,)es + e; K (j,8) K7 (s,)es + K (j,s)K; (s,5)es +e; 2 2 \\Poi 2 K (j, ) K \' (s,5) +,\nwhere Cov(€;,€;) = 0. Then since e, are independent and e, ~ N (0, K(s,s)) we find\nK, (i,5)Ks,5)es + e Cov(w;, w) = Cov 2 2 K(.)K s, 9)e + = K (i, ) K (s,5)K(s,§) + Lij{K(i,)}. 2\n(25)\n(26)\n(27)\n31\nPeruzzi & Dunson\nWe conclude by noting that § = M implies Py_,; N Po; = Palv;] N Pafv,]; considering two locations £;,£; € D* such that (¢;) = v; and (£;) = v; we obtain\n= — Covw(), w(£)) 2 K, ) (s,)Ks(s, ;) + 1g,—0 {Ki(i, £} s:{vs€Palv;NPalv;]}\nB.5 Computational cost\nWe make some assumptions here to simplify the calculation of overall cost: first, we assume that reference locations are all observed S C T, and consequently & = 7\\ S. Second we assume that all reference subsets have the same size i.e. |S;| = N; for all 4. Third, we assume all nodes have the same number of children at the next level in G, i.e. if v; € A, with r < M —1, then |Chl[v;]N A,41| = C, whereas if r = M — 1 then |Ch[v;]| = N,,. Fourth, we assume that all non-reference subsets are singletons i.e. if v; € B then |U;| = 1. The latter two assumptions imply (5) We also fix C\'N, N,. As a result, the number of nodes at level r =0 cM N.oM M —1is C", therefore |A| + |B| = er+ N,CM- = Then the sample size is n |71 IS cM+1_q hence M =~ log(n/N;). Starting C with § = M, the parent set sizes J; for a node v; € A, grow with r as J; = r and if v; € B then J; = M N;. The cost of computing p(w | 6) is driven by the calculation of H which is O(r?N) for reference nodes at level r, for a total of O(N? Lrr?). Since o2M _ for common choices of C and M we have M A < L — CMN3 ~ 3 = nN2 then the cost for reference sets is O(nN2). Analogously non reference nodes we get O(CM M2N3) which leads to a cost of O(nN2). The cost of sampling w is mainly driven by the computation of the Cholesky factor of a Ny x Ny matrix at each of C 1 reference nodes, which amounts to O(nN2). For the NC™ non-reference nodes the main cost is in computing H;w;) which is M N for overall cost O(CMMN?2) which is smaller than O(nN2). Obtaining F\'9 at the root of G is associated to a cost O(N C ) —1 which is O(nN) but constitutes a bottleneck if such operation is performed uuy to sampling; however this bottleneck is eliminated in Algorithm 3\nIf 6 = 1 then the parent set sizes J; for all nodes v; € V are constant J; = N; since the nodes at levels 0 to M — 1 have C children, the asymptotic cost of computing p(w | 6) is O(N? ery = O(N? oM ) O(nN?). However there are savings of approximately 1 a factor of M associated to § = 1 in fixed since > torr > McM OM+1 MC Cc-1 Cc-1 I 1o . Fixing C and M one can thus choose larger ( ) N, and smaller , or vice-versa,\nThe storage requirements are driven by the covariance at parent locations C\' for nodes v; with Pav;] # 0 i.e. all reference nodes at level 7 = 1 M —1 and non-reference nodes 6=M, v; is the last parent of v;, meaning = Pa[v;]. Then C; {Si, S} If v; € A, then these matrices are of size (r+1)Ng x (r+1)Ng; h of these is thus O( ) in terms of storage. Considering all such matrices brings the overall storage requirement to O( Cr2N2) which is O(nN) using analogous arguments as above. For § = 1 we apply similar calculations as above. The same number of H; and R must be stored but these are smaller in size and therefore do not affect the overall storage\n(28)\n32\nSpatial Multivariate Trees\nrequirements. The design matrix Z is stored in blocks and never as a large (sparse) matrix implying a storage requirement of O(nq).\nAppendix C. Implementation details\nBuilding a SPAMTREE DAG proceeds by first constructing a base-tree G at depth § = 1 and then adding edges to achieve the desired depth level. The base tree G; is built from the root by branching each node v into |Ch[v]| = ¢? children where d is the dimension of the spatial domain and is a small integer. The spatial domain D is partitioned recursively; after setting D, each recursive step proceeds by partitioning each coordinate axis of D; C D into ¢ intervals. As a consequence D; = U;D;; and N Dyjr = 0 if j # §\'. This recursive partitioning scheme is used to partition the reference set S which we consider as a subset of 1e observed locations. Suppose we wish to associate node v to approximately ng locations where ng = k¢ for some k. Start from the root i.e. v € Ag. Then take Sy = S and partition it via parallel partitioning of each coordinate axis into k intervals. Collect 1 location from each subregion to build Sp. Then set (S) = vo and S = S\\ S. Then, take {D;}; such at UjDy = Do = D. We find Sy via axis-parallel partitioning of S1 N D into k? regions and selecting one location from each partition, as above, and setting So = S\\ {SpUS}. All other reference subsets are found by sequentially removing locations from the reference set, and proceeding analogously as above. This stepwise procedure is stopped when either the ree reaches a predetermined height M, or when there is an insufficient number of remaining ocations to build reference subsets of size ng. The remaining locations are assigned to he leaf nodes via np as defined in Section 2.1 in order to include at least one neighboring realization of the process from the same variable.\nOne speciﬁc issue arises when multivariate data are imbalanced, i.e. one of the margins is observed at a much sparser grid, e.g. in Section 4.2 PRCP is collected at a ratio of 1:10 locations compared to other variables. In these cases, if locations were chosen uniformly at random to build the reference subsets then the root nodes would be associated via η to reference subsets which likely do not contain such sparsely observed variables. This scenario goes against the intuition of 1 suggesting that a naïve approach would result in poor performance at the sparsely-observed margins. To avoid such a scenario, we bias the sampling of locations to favor those at which the sparsely-observed variables are recorded. As a result, in Section 4.2 near-root nodes are associated to reference subsets in which all variables are balanced; the imbalances of the data are reﬂected by imbalanced leaf nodes instead.\nThe source code for SpamTrees is available at https://CRAN.R-project.org/package= spamtree and can be installed as an R package. The spamtree package is written in C++ using the Armadillo library for linear algebra (Sanderson and Curtin, 2016) interfaced to R via RcppArmadillo (Eddelbuettel and Sanderson, 2014). All matrix operations are per- formed eﬃciently by linkage to the LAPACK and BLAS libraries (Blackford et al., 2002; Anderson et al., 1999) as implemented in OpenBLAS 0.3.10 (Zhang, 2020) or the Intel Math Kernel Library. Multithreaded operations proceed via OpenMP (Dagum and Menon, 1998).\n33\nPeruzzi & Dunson\nC.1 On the dependencies on sparse Cholesky libraries\nSPAMTREES do not require the use of external Cholesky libraries because Cholesky-like algorithms can be written explicitly by referring to the treed DAG and its edges. This is unusual for DAG-based models commonly used in geostatistical settings. For example, an NNGP model uses neighbors to build a DAG. The DAG can be used to fill the L and D matrices leading to LDLT = C~!, where C~! is the sparse precision matrix of the latent process. When one then adds measurement error in a regression setting and marginalizes out the latent process, the goal is to find the Cholesky decomposition of C~ + 72I,,. The original NNGP DAG used for L an D is not useful for this purpose hence the need for NNGP to use sparse Cholesky libraries in collapsed samplers (Finley et al., 2019). On the other hand, with SPAMTREES we can still look at the original DAG to “update” L and D by using Algorithm 4. We included it in the Appendix as our software package implements he Gibbs sampler in the main article, which does not involve Cholesky decompositions of arge sparse precision matrices.\nFurthermore, one of the initial steps in Cholesky algorithms for sparse symmetric positive- deﬁnite matrices involves ﬁnding “good” reordering rows and columns. These reorderings simplify the (undirected) graphical model that corresponds to the sparsity structure in the matrix. Once a simple-enough graphical model is found heuristically, it is used for the decomposition. On the other hand, the sparse Cholesky algorithm for SpamTrees can be written explicitly using the underlying DAG, without any intermediate step, because it is ﬁxed and with a convenient treed structure. It might be possible to write software that out- performs excellent libraries such as CHOLMOD (Chen et al., 2008) at decomposing the matrices needed in collapsed sampling algorithms for SpamTrees.\nRegarding a more general perspective about dependencies on well established software libraries, we should clarify that the software for SpamTree does take advantage of highly eﬃcient libraries such as BLAS/LAPACK provided in Intel MKL 2019.5, OpenMP (Dagum and Menon, 1998) for parallelizing the algorithms as described in the main article. These libraries are optional and our code does not strictly depend on them. For example, noting that it is considerably more diﬃcult to compile OpenMP code on Macs, one can just disable OpenMP and let the Accelerate BLAS (rather than OpenBLAS or Intel MKL) deal with all matrix algebra for Apple computers, at the cost of some performance in big data settings. Our code also has R package dependencies for data pre-processing, but these are peripheral to the proposed methods and algorithms. Any improvement in the upstream libraries we used for coding spamtree will positively impact the performance of our software.\nC.2 Applications\nC.2.1 Simulated datasets\nSpamTrees with full depth are implemented by targeting reference subsets of size nS = 25 and tress with c = 4 additional children for each branch. The tree is built starting from a 2 × 2 partition of the domain, hence there are 4 root nodes with no parents in the DAG. The cherry-pickying function η is set as in Section 2.1; with these settings the tree height is M = 3. For SpamTrees with depth δ = 1 we build the tree with reference subsets of size nS = 80 and c = 4. Multivariate Q-MGPs are implemented via axis-parallel partitioning using 57 intervals along each axis. The multivariate SPDE-INLA method was implemented\n34\nSpatial Multivariate Trees\n<table><thead><tr><th>3</th><th>Ti1</th><th>Ti2</th><th>i</th><th>«</th></tr></thead><tbody><tr><td>LST_Day_CMG</td><td>99, 0.840</td><td>8.328 7.8071 96</td><td>8 460</td><td>0.0696, 0.1248</td></tr><tr><td>LST_Night_CMG</td><td>-1 3530</td><td>7.07</td><td>054 !</td><td>5</td></tr><tr><td>Clear_sky_days</td><td>0.86! 189 7</td><td>15 23</td><td>9| 6 5 1</td><td>0.1258,0.2203 )54</td></tr><tr><td>Clear_sky_nights</td><td>3138, 5 9306</td><td>0.9194, 0114</td><td>6</td><td></td></tr><tr><td>PRCP</td><td>i —0.3348, — i 02</td><td>0.6466, 7200</td><td>6. 55</td><td>s 6079</td></tr></tbody></table>\n<table><thead><tr><th>ij</th><th>LST_Day_CMG</th><th>LST_Night_CMG</th><th>Clear_sky_days</th><th>Clear_sky_nights</th></tr></thead><tbody><tr><td colspan="5">LST_Night_CMG 0.0608;</td></tr><tr><td>Clear_sky_days</td><td>7</td><td>371 705!</td><td></td><td></td></tr><tr><td>Clear_sky_nights</td><td>0.0221,0.0395</td><td>Bl 0.896 94</td><td>03 ) 1. ki 7380</td><td></td></tr><tr><td>PRCP</td><td>0.2039, 0.2878</td><td>J 0.914 7000</td><td>3 72 0.0490, 0.0643</td><td>0.4010, § 468</td></tr><tr><td></td><td>y</td><td></td><td></td><td>P</td></tr></tbody></table>\nFigure 8: Posterior means and 95% credible intervals for components of θ for SpamTrees.\nfollowing the examples in Krainski et al. (2019), Chapter 3, setting the grid size to 15×15 to limit the compute time to 15 seconds when using 10 CPU threads. BART was implemented on each dataset via the wbart function in the R package BART; the set of covariates for BART was built using the spatial coordinates in addition to a binary variable representing the output variable index (i.e. taking value 1 whenever yi is of the ﬁrst outcome variable, 0 otherwise).\nC.2.2 MODIS-TERRA and GHCN\nThe implemented SpamTrees are built with 36 root nodes and c = 6 additional children for each level of the tree, 25 reference locations for each tree node, and for up to M = 5 levels of the tree and δ = 5 (i.e. full depth). The non-reference observed locations are linked to leaves via cherry-pickying as in Section 2.1. Multivariate models were run on an AMD Epyc 7452-based virtual machine with 256GB of memory in the Microsoft Azure cloud; the SpamTree R package was set to run on 20 CPU threads, on R version 4.0.3 linked to the Intel Math Kernel Library (MKL) version 2019.5-075. The univariate models were run on an AMD Ryzen 5950X-based dedicated server with 128GB of memory, on 16 threads, R version 4.1.1 linked to Intel MKL 2019.5-075. The univariate NNGP model was implemented using R package spNNGP (Finley et al., 2020) using 20 neighbors for all outcomes and a “latent” algorithm. The univariate SpamTree was implemented on each outcome with full depth, c = 16 additional chidren for each level of the tree, and 25 reference locations for each node.\nThe MGP model was implemented via the development package at github.com/mkln/ meshgp targeting a block size with 4 spatial locations, resulting in an eﬀective average block dimension of 20. Caching was unavailable due to the irregularly spaced PRCP values. Fewer MCMC iterations were run compared to SpamTrees to limit total runtime to less than 16h.\n35\nPeruzzi & Dunson\nReferences\nS. Ambikasaran, D. Foreman-Mackey, L. Greengard, D. W. Hogg, and M. O’Neil. Fast direct methods for Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(2):252–265, 2016. doi:10.1109/TPAMI.2015.2448083.\nE. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen. LAPACK Users’ Guide. Society for Industrial and Applied Mathematics, Philadelphia, PA, third edition, 1999.\nT. V. Apanasovich and M. G. Genton. Cross-covariance functions for multivariate random ﬁelds based on latent dimensions. Biometrika, 97:15–30, 2010. doi:10.1093/biomet/asp078.\nS. Banerjee. High-dimensional Bayesian geostatistics. Bayesian Analysis, 12(2):583–614, 2017. doi:10.1214/17-BA1056R.\nS. Banerjee. Modeling Massive Spatial Datasets Using a Conjugate Bayesian Linear Modeling Frame- work. Spatial Statistics, in press, 2020. doi:10.1016/j.spasta.2020.100417.\nS. Banerjee, A. E. Gelfand, A. O. Finley, and H. Sang. Gaussian predictive process models for large spatial data sets. Journal of the Royal Statistical Society, Series B, 70:825–848, 2008. doi:10.1111/j.1467-9868.2008.00663.x.\nS. Banerjee, A. O. Finley, P. Waldmann, and T. Ericsson. Hierarchical spatial process models for multiple traits in large genetic trials. Journal of American Statistical Association, 105(490): 506–521, 2010. doi:10.1198/jasa.2009.ap09068.\nL. S. Blackford, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, J. Demmel, J. Dongarra, I. Duﬀ, S. Hammarling, G. Henry, et al. An updated set of basic linear algebra subprograms (BLAS). ACM Transactions on Mathematical Software, 28(2):135–151, 2002.\nY. Chen, T. A. Davis, W. W. Hager, and S. Rajamanickam. Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. ACM Trans. Math. Softw., 35(3), 2008. doi:10.1145/1391989.1391995.\nH. A. Chipman, E. I. George, and R. E. McCulloch. BART: Bayesian additive regression trees. Annals of Applied Statistics, 4(1):266–298, 2010. doi:10.1214/09-AOAS285.\nT. M. Cover and J. A. Thomas. Elements of information theory. Wiley Series in Telecommunications and Signal Processing. Wiley Interscience, 1991.\nN. Cressie and G. Johannesson. Fixed Rank Kriging for Very Large Spatial Data Sets. Journal of the Royal Statistical Society, Series B, 70:209–226, 2008. doi:10.1111/j.1467-9868.2007.00633.x.\nL. Dagum and R. Menon. OpenMP: an industry standard api for shared-memory programming. Computational Science & Engineering, IEEE, 5(1):46–55, 1998.\nA. Datta, S. Banerjee, A. O. Finley, and A. E. Gelfand. Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets. Journal of the American Statistical Association, 111:800–812, 2016a. doi:10.1080/01621459.2015.1044091.\nA. Datta, S. Banerjee, A. O. Finley, N. A. S. Hamm, and M. Schaap. Nonseparable dy- namic nearest neighbor gaussian process models for large spatio-temporal data with an appli- cation to particulate matter analysis. The Annals of Applied Statistics, 10:1286–1316, 2016b. doi:10.1214/16-AOAS931.\n36\nSpatial Multivariate Trees\nD. Eddelbuettel and C. Sanderson. RcppArmadillo: Accelerating R with high-performance C++ linear algebra. Computational Statistics and Data Analysis, 71:1054–1063, March 2014. doi:10.1016/j.csda.2013.02.005.\nJ. Eidsvik, B. A. Shaby, B. J. Reich, M. Wheeler, and J. Niemi. Estimation and prediction in spatial models with block composite likelihoods. Journal of Computational and Graphical Statistics, 23: 295–315, 2014. doi:10.1080/10618600.2012.760460.\nM. A. Ferreira and H. K. Lee. Multiscale Modeling: A Bayesian Perspective. Springer Publishing Company, Incorporated, 1st edition, 2007. ISBN 0387708979, 9780387708973.\nA. O. Finley, A. Datta, B. D. Cook, D. C. Morton, H. E. Andersen, and S. Banerjee. Eﬃcient Algorithms for Bayesian Nearest Neighbor Gaussian Processes. Journal of Computational and Graphical Statistics, 28:401–414, 2019. doi:10.1080/10618600.2018.1537924.\nA. O. Finley, A. Datta, and S. Banerjee. R package for Nearest Neighbor Gaussian Process models. 2020. arXiv:2001.09111.\nE. B. Fox and D. B. Dunson. Multiresolution Gaussian processes. In Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, NIPS’12, page 737–745, Red Hook, NY, USA, 2012. Curran Associates Inc. https://dl.acm.org/doi/10.5555/ 2999134.2999217.\nR. Furrer, M. G. Genton, and D. Nychka. Covariance Tapering for Interpolation of Large Spatial Datasets. Journal of Computational and Graphical Statistics, 15:502–523, 2006. doi:10.1198/106186006X132178.\nA. Gelfand, P. Diggle, M. Fuentes, , and P. Guttorp. Handbook of Spatial Statistics. CRC Press, Boca Raton, FL, 2010.\nM. G. Genton and W. Kleiber. Cross-Covariance Functions for Multivariate Geostatistics. Statistical Science, 30:147–163, 2015. doi:10.1214/14-STS487.\nC. J. Geoga, M. Anitescu, and M. L. Stein. Scalable Gaussian process computations using hierarchical matrices. Journal of Computational and Graphical Statistics, 29:227–237, 2020. doi:10.1080/10618600.2019.1652616.\nE. Gilboa, Y. Saatçi, and J. P. Cunningham. Scaling multidimensional inference for structured gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(2): 424–436, 2015. doi:10.1109/TPAMI.2013.192.\nR. B. Gramacy and D. W. Apley. Local Gaussian Process Approximation for Large Com- puter Experiments. Journal of Computational and Graphical Statistics, 24:561–578, 2015. doi:10.1080/10618600.2018.1537924.\nR. B. Gramacy and H. K. H. Lee. Bayesian Treed Gaussian Process Models With an Application to Computer Modeling. Journal of the American Statistical Association, 103:1119–1130, 2008. doi:10.1198/016214508000000689.\nR. Guhaniyogi, A. O. Finley, S. Banerjee, and A. E. Gelfand. Adaptive Gaussian predictive process models for large spatial datasets. Environmetrics, 22:997–1007, 2011. doi:10.1002/env.1131.\nJ. Guinness. Permutation and grouping methods for sharpening gaussian process approximations. Technometrics, 60(4):415–429, 2018. doi:10.1080/00401706.2018.1437476.\n37\nPeruzzi & Dunson\nM. J. Heaton, A. Datta, A. O. Finley, R. Furrer, J. Guinness, R. Guhaniyogi, F. Gerber, R. B. Gramacy, D. Hammerling, M. Katzfuss, F. Lindgren, D. W. Nychka, F. Sun, and A. Zammit-Mangion. A case study competition among methods for analyzing large spatial data. Journal of Agricultural, Biological and Environmental Statistics, 24(3):398–425, Sep 2019. doi:10.1007/s13253-018-00348-w.\nH. Huang and Y. Sun. Hierarchical low rank approximation of likelihoods for large spa- tial datasets. Journal of Computational and Graphical Statistics, 27(1):110–118, 2018. doi:10.1080/10618600.2017.1356324.\nM. Jurek and M. Katzfuss. Hierarchical sparse cholesky decomposition with applications to high- dimensional spatio-temporal ﬁltering, 2020. arXiv:2006.16901.\nM. Katzfuss. A multi-resolution approximation for massive spatial datasets. Journal of the American Statistical Association, 112:201–214, 2017. doi:10.1080/01621459.2015.1123632.\nM. Katzfuss and W. Gong. A class of multi-resolution approximations for large spatial datasets. Statistica Sinica, 30:2203–2226, 2019. doi:10.5705/ss.202018.0285.\nM. Katzfuss and J. Guinness. A general framework for Vecchia approximations of Gaussian processes. Statistical Science, 36(1):124–141, 2021. doi:10.1214/19-STS755.\nC. G. Kaufman, M. J. Schervish, and D. W. Nychka. Covariance Tapering for Likelihood-Based Estimation in Large Spatial Data Sets. Journal of the American Statistical Association, 103: 1545–1555, 2008. doi:10.1198/016214508000000959.\nE. T. Krainski, V. Gómez-Rubio, H. Bakka, A. Lenzi, D. Castro-Camilo, D. Simpson, F. Lindgren, and H. Rue. Advanced Spatial Modeling with Stochastic Partial Diﬀerential Equations Using R and INLA. CRC Press/Taylor and Francis Group, 2019.\nA. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in Gaussian processes: Theory, eﬃcient algorithms and empirical studies. Journal of Machine Learning Research, 8: 235–284, 2008. http://www.jmlr.org/papers/v9/krause08a.html.\nL. Lauritzen, S. Graphical Models. Clarendon Press, Oxford, UK, 1996.\nR. Lewis. A guide to graph colouring. Springer International Publishing, 2016. doi:10.1007/978-3-319-25730-3.\nF. Lindgren, H. Rue, and J. Lindström. An explicit link between Gaussian ﬁelds and Gaussian Markov random ﬁelds: the stochastic partial diﬀerential equation approach. Journal of the Royal Statistical Society: Series B, 73:423–498, 2011. doi:10.1111/j.1467-9868.2011.00777.x.\nJ. Loper, D. Blei, J. P. Cunningham, and L. Paninski. General linear-time inference for Gaussian processes on one dimension, 2020. arXiv:2003.05554.\nK. H. Low, J. Yu, J. Chen, and P. Jaillet. Parallel Gaussian process regression for big data: Low- rank representation meets Markov approximation. In Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, page 2821–2827, 2015. http://hdl.handle.net/1721.1/ 116273.\nM. Molloy and B. Reed. Graph colouring and the probabilistic method. Springer-Verlag Berlin Heidelberg, 2002. doi:10.1007/978-3-642-04016-0.\n38\nSpatial Multivariate Trees\nK. R. Moran and M. W. Wheeler. Fast increased ﬁdelity approximate Gibbs samplers for Bayesian Gaussian process regression, 2020. arXiv:2006.06537.\nD. Nychka, S. Bandyopadhyay, D. Hammerling, F. Lindgren, and S. Sain. A multiresolution gaussian process model for the analysis of large spatial datasets. Journal of Computational and Graphical Statistics, 24:579–599, 2015. doi:10.1080/10618600.2014.914946.\nM. Peruzzi, S. Banerjee, and A. O. Finley. Highly scalable Bayesian geostatistical modeling via meshed Gaussian processes on partitioned domains. Journal of the American Statistical Associa- tion, 2020. in press. doi:10.1080/01621459.2020.1833889.\nZ. C. Quiroz, M. O. Prates, and D. K. Dey. Block Nearest Neighboor Gaussian processes for large datasets, 2019. arXiv:1604.08403.\nJ. Quiñonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research, 6:1939–1959, 2005. https://www.jmlr.org/ papers/volume6/quinonero-candela05a/quinonero-candela05a.pdf.\nH. Rue and L. Held. Gaussian Markov Random Fields: Theory and Applications. Chapman & Hall/CRC, 2005. doi:10.1007/978-3-642-20192-9.\nH. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B, 71:319–392, 2009. doi:10.1111/j.1467-9868.2008.00700.x.\nC. Sanderson and R. Curtin. Armadillo: a template-based C++ library for linear algebra. Journal of Open Source Software, 1:26, 2016.\nH. Sang and J. Z. Huang. A full scale approximation of covariance functions for large spatial data sets. Journal of the Royal Statistical Society, Series B, 74:111–132, 2012. doi:10.1111/j.1467-9868.2011.01007.x.\nE. Snelson and Z. Ghahramani. Local and global sparse Gaussian process approximations. In Proceedings of the Eleventh International Conference on Artiﬁcial Intelligence and Statistics, vol- ume 2 of Proceedings of Machine Learning Research, pages 524–531, 2007. http://proceedings. mlr.press/v2/snelson07a.html.\nM. L. Stein. Limitations on low rank approximations for covariance matrices of spatial data. Spatial Statistics, 8:1–19, 2014. doi:doi:10.1016/j.spasta.2013.06.003.\nM. L. Stein, Z. Chi, and L. J. Welty. Approximating likelihoods for large spa- tial data sets. Journal of the Royal Statistical Society, Series B, 66:275–296, 2004. doi:10.1046/j.1369-7412.2003.05512.x.\nY. Sun, B. Li, and M. Genton. Geostatistics for large datasets. In J. Montero, E. Porcu, and M. Schlather, editors, Advances and Challenges in Space-time Modelling of Natural Events, pages 55–77. Springer-Verlag, Berlin Heidelberg, 2011. doi:10.1007/978-3-642-17086-7.\nA. V. Vecchia. Estimation and model identiﬁcation for continuous spatial pro- cesses. Journal of the Royal Statistical Society, Series B, 50:297–312, 1988. doi:10.1111/j.2517-6161.1988.tb01729.x.\nM. Vihola. Robust adaptive Metropolis algorithm with coerced acceptance rate. Statistics and Computing, 22:997–1008, 2012. doi:10.1007/s11222-011-9269-5.\n39\nPeruzzi & Dunson\nL. Wu, A. Miller, L. Anderson, G. Pleiss, D. Blei, and J. Cunningham. Hierarchical inducing point Gaussian process for inter-domain observations. In Proceedings of the 24th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2021. arXiv:2103.00393.\nX. Zhang. An Optimized BLAS Library Based on GotoBLAS2., 2020. URL https://github.com/ xianyi/OpenBLAS/.\n40', "Document 4: Fake Answer.doc\nORACLAIM PARTNERS LLP\nEmelia C. Zarzuela (SBN 002456)\nRobert T. Brown (SBN 005778)\n335 Elm Street, Suite 350\nSan Francisco, CA  94111\nTelephone No.: 415-555-4600\nFacsimile No.: 415-555-4601\nAttorneys for Defendants\nFAIRS ARE US, INC. and COUNTY OF SIERRA\nSUPERIOR COURT OF THE STATE OF CALIFORNIA\nCOUNTY OF SIERRA\n<table><tr><td>JOHN DOE Plaintiff, vs. FAIRS ARE US, INC., COUNTY OF SIERRA, and DOES 1 through 25, inclusive, Defendants.</td><td/><td>Case No. CGG-24-555555 Action Filed: January 5, 2024 Trial Date: TBD DEFENDANT FAIRS ARE US INC.’S ANSWER TO PLAINTIFF’S COMPLANT</td></tr></table>\nBy way of general denial pursuant to Cal. Code Civ. Proc. § 430.31, defendant FAIRS ARE US, INC. (“Defendant”) hereby answers plaintiff JOHN DOE (“Plaintiff”) Complaint and each cause of action contained therein as follows:\nGENERAL DENIAL\nDefendant denies generally each and every allegation contained in the Complaint, and denies that Plaintiff was damaged in any of the sums mentioned in the Complaint, or in any other sums, or at all, by reason of any action or omission on the part of the Defendant.\nFIRST AFFIRMATIVE DEFENSE\n(Failure to State a Cause of Action)\nAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that neither Plaintiff’s Complaint, nor any of the alleged causes of action therein, state facts sufficient to constitute a cause of action against the Defendant.\nSECOND AFFIRMATIVE DEFENSE\n(Comparative Negligence)\nAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that Plaintiff was negligent in and about the matters alleged in the Complaint and in each cause of action therein, and that such negligence contributed directly and proximately to the accident and damages, if any, alleged therein.\nTHIRD AFFIRMATIVE DEFENSE\n(Contributory Negligence)\nAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that the injuries and damages complained of by Plaintiff, if any there were, were directly and proximately caused, either wholly or in part, by the negligence of persons or entities other than Defendant, and that such negligence is either imputed to Plaintiff by reason of the relationship between Plaintiff and such persons or entities, or comparatively reduces the proportion of negligence and corresponding liability of Defendant.\nFOURTH AFFIRMATIVE DEFENSE\n(Failure to Mitigate Damages)\nAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that Plaintiff has unreasonably failed to act in such a manner as to mitigate the damages of which he complains, if any there were.\nFIFTH AFFIRMATIVE DEFENSE\n(Statute of Limitations)\n\tAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that Plaintiff’s Complaint and claims against Defendant are barred by the applicable statutes(s) of limitation.\nSIXTH AFFIRMATIVE DEFENSE\n(Unclean Hands)\n\tAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that Plaintiff is precluded from recovery against Defendant pursuant to the doctrine of unclean hands. \nSEVENTH AFFIRMATIVE DEFENSE\n(Liability for Non-Economic Damages)\nAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that if Defendant is ultimately found to have been negligent in this case, which supposition is denied and merely stated for the purpose of this affirmative defense, its liability, if any, for Plaintiff’s non-economic damages shall be several, pursuant to the Fair Responsibility Act of 1986 (Proposition 51), including but not limited to California Civil Code §1430 et seq., so that Defendant shall be liable only for the amount of non-economic damages allocated to Defendant in direct proportion to Defendant’s percentage of fault, if any, and a separate judgment, if any, shall be rendered against Defendant for that amount.\nEIGHTH AFFIRMATIVE DEFENSE\n(Laches)\nAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that Plaintiff has unreasonably delayed notifying Defendant of the claims alleged in the Complaint and has unreasonably delayed prosecuting said claims after the alleged causes of action arose; that due to said delay, the recollections of witnesses have become unclear, witnesses have become unavailable, and for other reasons, said delay was therefore unreasonable; and that said delay has prejudiced Defendant in preparing and presenting its defenses herein; and that by reason of the premises, Plaintiff's claims and actions in the Complaint are barred by the doctrine of laches.\nNINTH AFFIRMATIVE DEFENSE\n(Failure to Exhaust Remedies Under a Collective Bargaining Agreement)\nAS A SEPARATE AND DISTINCT AFFIRMATIVE DEFENSE, Defendant alleges that Plaintiff has failed to look to any applicable collective bargaining agreement for redress of any grievance or injury preempting the plaintiff’s causes of action and claims made in this Complaint and limiting his remedies to those under that agreement, if any.\nREQUESTED RELIEF\nWHEREFORE, Defendant hereby requests that Plaintiff take nothing by virtue of the Complaint on file with this Court, that the Complaint and each cause of action therein be dismissed with prejudice, that judgment be entered in favor of Defendant, including its costs of suit herein, and for such other and further relief as the Court deems just and proper.\nDated: January 30, 2024 \t\t\tORACLAIM PARTNERS LLP\nAttorneys for Defendant FAIRS ARE US, INC. \n     \tBy:___________________________________\n\t\t\t\t\t\t\tRobert T. Brown\n-4-\nDEFENDANT’S ANSWER TO PLAINTIFF’S COMPLAINT", 'Document 5: Fake Complaint.docx\nCOMPLAINT FOR DAMAGES\nPlaintiff John Doe alleges as follows:\nPARTIES\nPlaintiff: John Doe ("Plaintiff") is, and at all times relevant herein was, an individual residing in the County of Sierra, State of California.\nDefendant Sierra County: Defendant Sierra County ("Sierra County") is, and at all times relevant herein was, a governmental entity operating within the State of California.\nDefendant Fairs Are Us: Defendant Fairs Are Us ("Fairs Are Us") is, and at all times relevant herein was, a business entity engaged in the operation and management of fairs and amusement rides, including the Sierra County Fair.\nDefendants DOES 1-10: Plaintiff is ignorant of the true names and capacities of Defendants sued herein as DOES 1 through 10, inclusive, and therefore sues these Defendants by such fictitious names. Plaintiff will amend this Complaint to allege their true names and capacities when ascertained. Plaintiff is informed and believes, and thereon alleges, that each of the fictitiously named Defendants is responsible in some manner for the occurrences herein alleged, and that Plaintiff’s damages as herein alleged were proximately caused by such Defendants.\nJURISDICTION AND VENUE\nJurisdiction: This Court has jurisdiction over this matter pursuant to California Code of Civil Procedure § 410.10.\nVenue: Venue is proper in this Court pursuant to California Code of Civil Procedure § 395(a) because the injury occurred in Sierra County, California, and Defendants conduct business within this county.\nFACTUAL ALLEGATIONS\nOn or about January 23, 2024, Plaintiff attended the Sierra County Fair with his two nine-year-old daughters and several co-workers. The fair was operated and managed by Defendants Sierra County and Fairs Are Us.\nWhile exiting the Tilt-A-Whirl ride at approximately 3:00 PM, Plaintiff tripped over a metal spike that was protruding from the ground in the exit path of the ride.\nAs a result of the fall, Plaintiff suffered a broken left ankle, which required immediate surgery, as well as back spasms and neck strain. Plaintiff was transported to Sierra Vista Medical Center for treatment.\nPlaintiff has been unable to return to his job as a construction worker due to the severity of his injuries and is uncertain when he will be able to resume work.\nThe spike posed an unreasonable hazard to fairgoers, and Defendants failed to warn of or eliminate this danger despite their duty to maintain safe premises.\nFIRST CAUSE OF ACTION: PREMISES LIABILITY\n(Against All Defendants)\nPlaintiff re-alleges and incorporates by reference paragraphs 1 through 11 as though fully set forth herein.\nDefendants owned, leased, occupied, and/or controlled the premises of the Sierra County Fair.\nDefendants had a duty to maintain the premises in a reasonably safe condition and to warn of any dangerous conditions.\nDefendants breached their duty by failing to eliminate the hazard posed by the spike and failing to warn Plaintiff and other fairgoers of the danger.\nAs a direct and proximate result of Defendants\' negligence, Plaintiff sustained serious injuries, resulting in medical expenses, lost wages, loss of earning capacity, and other damages.\nSECOND CAUSE OF ACTION: NEGLIGENCE\n(Against All Defendants)\nPlaintiff re-alleges and incorporates by reference paragraphs 1 through 16 as though fully set forth herein.\nDefendants owed Plaintiff a duty of care to ensure the fairgrounds were safe for visitors.\nDefendants breached their duty of care by negligently maintaining the premises, allowing a dangerous spike to protrude in the exit path of a ride.\nAs a direct and proximate result of Defendants\' negligence, Plaintiff suffered severe injuries, incurred medical expenses, lost wages, and experienced pain and suffering.\nPRAYER FOR RELIEF\nWHEREFORE, Plaintiff prays for judgment against Defendants as follows:\nFor general damages in an amount to be proven at trial;\nFor special damages, including but not limited to medical expenses, lost earnings, and loss of earning capacity, according to proof at trial;\nFor costs of suit incurred herein;\nFor prejudgment interest as permitted by law; and\nFor such other and further relief as the Court may deem just and proper.\nDEMAND FOR JURY TRIAL\nPlaintiff hereby demands a trial by jury on all issues so triable.\nDated: _______________\nRespectfully submitted,\n[Plaintiff’s Attorney’s Name]\n[Law Firm Name]\nBy: ______________________\n[Plaintiff’s Attorney’s Name]\nAttorneys for Plaintiff John Doe', 'Document 6: A_process_model_for_industrial_new_product_development.pdf\n2\nIEEE T R A N S A C T I O NS ON ENG INEERING M A N A G E M E N T, V O L. EM-30, N O. 1, F E B R U A RY 1 9 83\nA Process Model for Industrial New Product Development\nR O B E RT G . C O O P ER\nAbstract—What steps should the industrial new product manager take to improve new product performance? This article pulls together the findings from the many research studies into what makes a new product a success. Six important lessons for managers are developed from those studies. The lessons point to a flow model approach to the development and commercialization of a new product—a step-by-step approach to successful product innovation. A seven-stage model, de signed to move a product from the idea stage to product launch, is described. Actual case histories illustrate how each stage of the model can be implemented.\nINTRODUCTION\nW HAT STEPS should t he industrial new product manager take to improve new product performance? Twenty years of research into reasons for new product success and causes of failure have yielded valuable clues to more effective new prod uct management. Yet many firms continue to make t he same mistakes in their new p r o d u ct programs. This article tackles the problem, first by reviewing recent research into new prod- i;r>f f i i f p o cc ΓίπΗ f a i l u f f* ί ΐ Γ ΐΗ i R P c o f i f! hv iran-lstirî" the findings into a process model as a normative guide for managers.\nThe last two decades have witnessed a plethora of research into industrial new products. We have probed such issues as: how firms develop and launch new industrial p r o d u c t s; the a n a t o my of new product success; why new products fail; and failures. Whether what separates successes from this new found information has had m u ch impact on management prac tice or on product performance is doubtful. New product fail ure rates remain high; and t he same reasons for failure are re peated. F or example, a recent Conference Board study into why new products fail identified exactly the same set of fail ure reasons as a study d o ne 17 years before [ 1 8 ], [ 2 7 ].\nOne reason why firms have been slow to act on the research findings is the way the findings have been presented [ 4 ]. Rea sons for failure or success have been reported on a variable-by- variable basis, whereas managers tend to think in terms of ges- talts or scenarios [ 2 4 ], [ 2 5 ]. More critical is that the research has n ot been translated into meaningful and tangible guides to action. F or example, o ne n o t ed study of new p r o d u ct sue cess factors, Project SAPPHO, found that " an understanding of users\' needs" was the n u m b er one factor in success, a find ing of interest from an academic standpoint. But what steps should the engineering-oriented new product manager take to become more "aware of user n e e d s ?" The answer is n ot pro vided by the research.\nWhat is missing is a shaping of the research conclusions into a managerial guide. The most commonly proposed managerial\nManuscript received April 2 1, 1982.\nThe author is with the Faculty of Management, McGill University, Montreal, Canada H3A 1G5.\nnew product framework is t he process model—a stepwise se quence of activities designed to move t he p r o d u ct from t he idea stage through to successful launch. A clear need exists to redesign the process model to include these n ew research findings.\nC L U ES TO NEW PRODUCT SUCCESS\nDiagnosis of Product Failures\nResearch into the p r o d u ct R&D process has generally fo cused on what can be learned from our previous experiences. Diagnosis of past failures has been one important research di rection. Such studies are premised on the belief that a post mortem of previous failures should identify pitfalls and o b stacles, that m a n a g e m e nt can then take steps to avert or over come in future projects.\nThree Conference Board studies, in 1964, \' 7 1, and \' 8 0, all identified marketing variables as the major weaknesses in firms\' new product failures [ 1 8 ], [ 1 9 ], [ 2 7 ]. Inadequate mar ket analysis was cited as t he n u m b er one reason for failure, and by a considerable margin over other causes.1 Also import ant were product defects, lack of an effective marketing effort, higher costs than e x p e c t e d, competitive strength, bad timing, and salesforce or distribution weaknesses. Most of these failure causes were marketing related. But the prime cause involved a lack of market information, while deficiencies in the market ing, promotion, a nd selling efforts at the launch phase were only secondary reasons.\nRecommendations of these studies called for more and bet ter marketing research and marketing efforts (including selling and promotion efforts), careful product positioning, more ef fective concept testing, and better test marketing. Also a need for better evaluation was n o t e d, including early screening of new product proposals. Finally, t he Conference Board studies pointed to b e t t er venture management, improvements in coor dination and internai c o m m u n i c a t i o n, and a m o re planned ap proach to venture management.\nThe causes of industrial new product failure were investi gated by Cooper for a sample of 114 actual produ^* cases [ 5 ]. The major reasons were again marketing d o m i n a t e d:\n1) underestimated competitive strength a n d / or competitive position in market (36.4 percent);\n2) overestimated n u m b er of potential users ( 2 0 .5 percent);\n3) product\'s price set t oo high (18.2 percent); and\n4) technical difficulties/deficiencies with p r o d u ct (20.5 per cent).\n*45 percent of respondents cited inadequate market analysis, versus 29 percent for product defects and 25 percent for poor marketing efforts.\n0 0 1 8 - 9 3 9 1 / 8 3 / 0 2 0 0 - 0 0 0 2 S 0 1 . 00 © 1983 IEEE\nCOOPER: P R O C E SS M O D EL F OR I N D U S T R I AL NEW P R O D U CT D E V E L O P M E NT\n3\nSix scenarios of new product were identified [ 3 ]. The most c o m m on scenario was the " b e t t er mousetrap no one w a n t e d" (an innovative product that did not serve a market need, 28 percent of cases), followed by the "me-too product meeting a competitive brick wall" (similar to c o m p e t i t o r s\' products with no differential advantage, 24 percent of cases) [ 3 ].\nThe study also pinpointed areas of weakness within the firm and t he new product process. In almost two-thirds of the cases, a lack of marketing research skills or personnel was thought to have contributed significantly to the failure, while the detailed market study phase was the most poorly under taken activity of the new product process. Again, this research points to a lack of a market and marketing orientation as the culprit in industrial new products.\nSuccesses New Product\nStudies of new product success stories have focused on these successes shared in common. An ex identifying what innovations by tensive descriptive study of 567 successful Myers and Marquis showed that most were market-derived (market pull) ventures and only 21 percent were technology push [ 2 6 ]. Correct identification of an existing demand was t he critical c o m m on innovations. ingredient among these Other lessons learned were that internal sources of information were the most important to the innovation process, pcinting to the need to foster internal communication. External infor mation via nonstructured channels also played a key rolt. A descriptive process model, consisting of five major steps and eight activities, outlined the actual process followed in these innovations.\nIn contrast, the ingredients of success in Globe\'s study of ten radical innovations were dominated by technological an<i internal variables, with external factors least important [ 1 5 ]. Success factors included:.\n1) a recognition of a technical opportunity;\n2) a need (market) recognition;\n3) proficient internal R&D management;\n4) well-executed venture decisions;\n5) ample development funds;\n6) a technical entrepreneur.\nBoth internal (technological) and external (market) vari ables helped decide the fates of six successful innovations at GE Labs studied by Roberts and Burke [30] :\n1) market needs were recognized and R&D was targeted at satisfying these needs;\n2) when a technical success did not have a specific market identified to suit the need, the product was adapted need;\nthe possibility of a 3) research managers communicated technical breakthrough clearly to other departments, the identification of a market need; which facilitated\n4) communication existed between engineers and scien tists in the operating departments.\nIn another investigation, detailed case studies o f i h r ee high technology, significant industrial new products revealed an in tricate and balanced new product process as the key to infor\nmation acquisition and risk management [ 6 ]. The entire new product process could be viewed as a goal directed stepwise process, involving a series of information acquisition activi ties and evaluation points. The process was characterized by a "incremental c o m m i t m e n t ," where re called phenomina sources were committed to t he project such that uncertainty and a m o u n ts c o m m i t t ed were balanced. The process itself was mul- tidisciplinary and integrated, with inputs from marketing, en gineering, R&D, and production. Extensive market studies, especially near the beginning of each project, were common to all three p r o d u c t s, and played a key role in shaping the success of the p r o d u c t s.\nTownsend\'s study of one radical innovation and subsequent innovations incremental innovations showed that successful depend on intimate collaboration between user and innovator, and that a well defined market need facilitates the innovation\'s success [ 3 7 ]. Other key factors included the existence of a infor "technical c h a m p i o n ;" interdisciplinary exchange of mation and internal communication; and highly developed testing and screening procedures.\nGronhang analyzed 36 products in 15 small- and medium- sized Norwegian firms [ 1 6 ]. Highly novel products tended to originate from outside the company, while medium and low novelty products came from within. The most successful were medium novelty products. The magnitude of R&D input was positively correlated with number of new product ideas, de veloped p r o d u c t s, and successful p r o d u c t s. Finally a strong led to less novel products, but a competitive environment m o re systematic search for ideas. An effective internal and ex ternal communication system was felt essential to successful innovation.\nSuccesses versus Failures\nMost recent research has focused on contrasting successes and failures in order to find those variables or characteristics that discriminate between the t w o. The British study, Project SAPPHO, identified a pattern of differences between a paired sample of 43 successful and unsuccessful innovations [ 3 1 ], [ 3 5 ]. Of the 122 variables measured, 41 were found to dis criminate between successes and failures, and five underlying factors that separated successes from failures were identified:\n1) understanding of users\' needs;\n2) efficiency of development;\n3) characteristics of management and managers;\n4) effectiveness of communications (internal and exter nal);\n5) magnitude of marketing efforts.\nOther conclusions were that many variables leading to product outcomes were, to a large extent, amenable to better manage m e nt control, particularly in the area of marketing. But prod uct outcomes—success versus failure—could rarely be reduced to a single variable. Finally, the investigation yielded a rank order list of what variables appear i m p o r t a nt in deciding new product results.\nA similar b ut smaller scale study was undertaken in the Hungarian electronics industry [ 3 3 ]. The sample size was lim ited to 12 product pairs and featured a different pairing tech-\n4\nI E EE T R A N S A C T I O NS ON E N G I N E E R I NG M A N A G E M E N T, V O L. EM-30, N O. 1, F E B R U A RY 1 9 83\nnique. In spite of t he differences in research design and t he ob vious contrasts between the t wo countries, t he Hungarian re- suits were strikingly similar to the British SAPPHO. The fol lowing characteristics were highly associated with successful innovations:\n1) market need satisfaction;\n2) effective communication (internal and external);\n3) efficient development;\n4) a market orientation;\n5) the role of "key individuals."\nAnother success/failure study undertaken by Kulvik in Fin land yielded similar results to the two studies above [ 2 1 ]. But several additional success facilitators were identified, including the effective utilization of company potentials (good p r o d u c t/ company fit); the utilization of technical " k n ow h o w" of the company; and the exploitation of market opportunities. Fa miliarity with technology and markets both were determinants of success, with market familiarity having a more pronounced effect. An i m p o r t a nt finding of the Finnish study was that the result of a few critical factors, b ut in failures are often order to succeed, competence must be demonstrated in a wide range of tasks.\nA similar study undertaken on European and Japanese firms measured the impact of the external environment and in dustry maturity on the innovation process [ 3 9 ]. Successful in novations, when compared to failures:\n1) had no initial difficulties in marketing;\n2) had a real p r o d u ct advantage;\n3) had market needs recognized prior to a solution;\n4) had more customer c o n t a c t;\n5) involved top management initiation.\nAdvance planning, the use of outside consultants, the absence of patent protection, and responses to government actions were all positively related to success.\nA West German study investigated products that were com [ 1 3 ]. The source mercialized versus those that were canceled of the i d e a - w h e t h er market pull or technology push—had a marked impact on commercialization, with market pull prod ucts fairing better. All products were incremental innovations.\nThe SAPPHO researchers recently reported the results of a five country study of innovation in the textile industry [ 3 2 ]. Incremental innovations were found important to short term prosperity. Firms employing qualified scientists and R&D engi neers were more able to produce the successful breakthroughs, and. more radical innovations stemmed from those firms with a technically qualified chief executive. Successful firms had superior marketing programs and frequent customer contact in this textile industry study. Successful firms understood users\' needs better, and were able to assess whether these needs could be filled economically. Specific sales strategies were matched to market requirements. An open and flexible man agement structure, the existence of a "product champion," ef fective external communication, and several other controllable variables were all related to positive new product outcomes.\nFifty-four significant facilitators for success were identi fied in Rubenstein etal.\'s study of American new p r o d u c t s, but no single characteristic of success or failure could be detected [ 3 6 ]. F u r t h e r, one person\'s facilitator could be someone else\'s barrier. Some of the important facilitators included:\n1) existence of a strong product champion;\n2) marketing factors, such as need recognition;\n3) strong internal communications;\n4) superior techniques for data gathering, analysis, and decision-making;\n5) planned approaches to venture management.\nProject NewProd contrasted almost 2 00 randomly selected Canadian industrial new p r o d u c t s - h a lf successes, half fail ures—on each of 77 characteristics [ 7 ] - [ 1 0 ]. The three most critical determinants of new product success were:\n1) having a unique or superior product in the eyes of the customer: one that was innovative; had unique features for the customer; met customer needs better t h an com peting products; did a unique task; lowered customer costs; and was of high quality;\n2) having marketing knowledge and proficiency: properly undertaking preliminary market assessment, market re search, and test markets; having a sound understanding of the m a r k e t - n e e ds and wants, price sensitivity, buyer behavior, market size, and competition; and executing the iaunch well;\n3) having technical and production synergy and proficiency: having a good fit between the product and the company in terms of R&D, engineering, and production; having " i n - h o u s e" technical and production knowledge; and un dertaking the technical and production activities well.\nA n o t h er eight factors were also related to success, but less strongly.Subsequently, new product scenarios were developed, each with its own likelihood of success [ 4 ].\nOther i m p o r t a nt conclusions of NewProd were that there influenced new product success. were m a ny variables that Most were amenable to management action, while success was not nearly so dependent on the new product situation or en vironment. Finally, weaknesses exist in many firms\' new prod uct processes\' many firms were developing " m e - t o o" p r o d u c t s: often products were new and different but offered no new benefits to customers; typically vital market information was lacking in the process; and the market launch itself was fre quently poorly planned or executed.\nWHAT NEW PRODUCT RESEARCH HAS T A U G HT US\nthemes or messages begin to A n u m b er of underlying emerge as one reviews the research into new product perform ance. Consider some of the more evident lessons we can learn from a synthesis of these studies.\nC O O P E R: PROCESS M O D EL F OR I N D U S T R I AL NEW P R O D U CT D E V E L O P M E NT\n5\nLesson 1. For Industrial New Products, a Much Stronger is Needed Market Orientation\nNeed recognition, understanding user needs, undertaking market assessment and market research, and having a sound knowledge of the marketplace were prevalent findings, com mon to virtually every study. The ultimate success of a new product is determined in the marketplace, and market infor mation must play a critical role in the shaping of t he p r o d u ct and the launch strategy. Most successful projects are market pull, relying heavily on market information. In the case of suc- cesful technology push products, innovators determine the ex istence of a market need before proceeding, and then deter mine precisely user needs and interpret them in the design of the new product [ 3 4 ].\nEqually clear is that m a ny firms are still not performing the research and market studies essential to successful market is under product development. T oo little market research taken, and it is often t oo late in t he process. Moreover, import ant, often vital, market studies tend to get omitted altogether. A lack of market research remains the most frequent cause of failure.\nThe ideal new product process would see technical and en gineering research balanced with extensive market research. Market information should be integrated into every stage of the process, and not just as an afterthought or towards the launch phase. Whether a technology push or parket pull pro ject, market information should be used, not only in the evalu ation decisions, but most importantly, as an input to the prod uct design, engineering, and product development activities.\nLesson 2. New Product Success is Largely Amenable to Management Action\nMost of the studies concur that the actions of the people in the fates of the ventures. volved in these projects decided innovators are n ot victims of circumstance or pris Product oners of the environment: the o u t c o me of a venture depends not so much on the nature of the market or nature of the project, b ut rather on what people do about it. The role of key players (for example, t he product champion) and the impact of certain activities and procedures (for example, market re search) underscore this point. As Rothwell notes: "-..while chance and uncertainty can upset even the best laid s c h e m e s. responsibility for the success or failure of innovations ulti mately rests firmly in the hands of the innovating companies\' own management" [ 3 4 ].\nThis is a provocative finding. The traditional new p r o d u ct literature is preoccupied with project selection, with relatively little focus on the activities of the new product process. The assumption appears to be that "selecting the right project" is p a r a m o u n t. Not so according to recent research, where success appears to depend m o re on execution rather than selection. The design and implementation of an appropriate new prod uct p r o c e s s - t he key activities from idea to l a u n c h - b e c o m es critical to success.\nLesson 3. There is No Easy Explanation for What Makes a New Product a Success\nSuccess does not depend on one or a few variables. Most studies identified a large number of characteristics that ex plained success. An accepted conclusion is that t he "success e q u a t i o n" is a complex o n e; and that success depends on doing many things well, while failure can result from a single error.\nThe new product manager is therefore faced with the com plex task of managing a highly uncertain endeavor where many things must be done properly and a single miscue can spell dis is that a carefully developed activity aster. One implication plan or process model would help in ensuring that no vital activity or information-seeking step is overlooked. A second concerns the need to organize for varied inputs to t he process the firm: a multidisciplinary ap from many sources within proach.\nLesson 4. The Product Itself-A Unique Product with Central to Success Real Customer Advantages-is\nThe central role of t he product itself-its design, features, advantages, and benefits—in creating success should come as no surprise. But several studies point out that tired products and " m e - t o o" designs are the rule rather than the exception in most firms\' new product efforts.\nEqually clear is that merely having an innovative p r o d u c t- unique or totally new—is not sufficient. Rather, t he product and must be unique and superior in the eyes of the customer not just in the opinion of t he R&D department.\nThe desire to deliver a "better product" in terms of meet ing customer needs, parallels t he need to be market oriented. Clearly the firm must possess the tehnological skills to develop a "superior" product: the product may be better because it uses newer technology or is better designed and engineered than competing products. But in order that the p r o d u ct de liver significant advantages to the customer, a clear under standing of the customers\' needs, wants, preferences, choice criteria, and use practices is essential before serious product development begins. The new product process must marry the technical side of product design and engineering to the needs of the marketplace to ensure that the product does indeed de liver unique benefits to the customer.\nLesson 5. A Well-Conceived Properly Executed Launch is Vital to Success\nA strong marketing effort, a well targeted selling approach, effective aftersales service, and sound marketing communica tions were c o m m on themes in m u ch of the research. But a well integrated and properly targeted launch effort does n ot hap pen by chance. It is the result of a fine-tuned marketing plan backed up by proper e x e c u t i o n. The marketing planning pro cess, itself a complex process, must therefore be built into the new product activity plan. For example, selection of target m a r k e t, a key step in marketing planning, should logically pre- ceed product design and development. Critical to an effective information: how customers b u y; marketing plan is market their choice criteria; sources of information ; and competitive\n6\nI E EE T R A N S A C T I O NS ON E N G I N E E R I NG M A N A G E M E N T, V O L. EM-30, N O. 1, F E B R U A RY 1 9 83\npractices. Once again we see a need for market research, b ut this time research designed to provide information essential to the design of a launch plan.\nand Coordination Lesson 6. Internal Communication Successful Between Internal Groups Greatly Fosters Innovation\nMany of the research studies spoke of interfaces between R&D and marketing; of coordination between key internal inputs to t he new product groups; and of multidisciplinary process. Product innovation is n ot simply a matter of R&D, but consists of many varied activities that ideally should be un dertaken by different groups within t he firm: market research, R&D, sales, engineering, industrial design, p r o d u c t i o n, and ad vertising. Multiple inputs must be coordinated and integrated throughout the process. This calls n ot only for a systematic approach to the new p r o d u ct process, but also one where vari ous groups are logically brought together in the process.\nCONCEPTUALIZING THE NEW PRODUCT PROCESS\nThe lessons learned from new p r o d u ct research suggest needed changes in the process many firms use to develop new industrial products. Consider the new product process or flow diagram model. A process model is a useful approach to orga nizing and controlling the various activities involved in the de velopment of a new product, and provides a skeleton around which each project manager can build his/her own P E RT or critical path plan specific to any one project.\nNew product process models can be categorized as descrip tive versus nomative, and further by industrial product versus consumer product. Descriptive process models have evolved from empirical studies of the new p r o d u ct case histories. Booz, Allen,and Hamilton, [ 2 ], Myers and Marquis [ 2 6 ], Utterback [ 3 8 ], Rothwell [ 3 1 ], and Little [22] all propose flow charts that identify the steps of the new product process. Myers and is typical: a Marquis\' model five-stage descriptive model, whose stages include recognition, idea, search, solution, and implementation. Such models, however, were never intended as normative guides for managers: while conceptually correct, they lack the detail and precision necessary for use as a norma tive model. Moreover, since descriptive models are based on actual practice, there is no guarantee that such practice itself is ideal and could stand as a guide to others.\nThe most specific and detailed process normative models are in the field of non-industrial goods. Klompmaker\'s "ide alized process for new product d e v e l o p m e n t" outlines a 27- step model for consumer goods, that specifies details such as "select a n a m e ," "design the package," and "create copy t h e m e" [ 1 0 ]. Similarly, Hanan presents a detailed 24-activity flow diagram, but it t oo appears most appropriate for con sumer goods [ 1 7 ]. Management Decision System Inc. outlines a detailed process for new pharmaceutical products, that even suggests how some steps should be carried o ut (for example, "concept screening" using focus groups) [ 2 3 ].\nNormative models for the industrial new product process suffer by comparison. Several models in use by companies are reviewed by the National Industrial Conference Board (NICB) [ 2 8 ]. Of the three that are most developed, t wo omit marketing\nr t i v i t i e* altogether, and the other lacks specificity, consisting of a few major steps, not unlike a descriptive model. In con trast, Giss^r\'s "new product introduction p r o g r a m" consists of a 67-step recine to new product success, and is designed for use as a computer planning model [ 1 4 ]. Finally, Cooper and More\'s " m o d u le b r o a c h" is not a piocess model at all, but rather an approach to constructing a flow model [ 1 2 ].\nThe engineering literature mak > several notable contribu tions to the industrial new product process. But such models, typified by Albala\'s "stage approach activities," tend to view the new product process from a technological perspective, and focus attention on the "front e n d" stages (with little emphasis on commercialization) and on engineering activities [ 1 ]. Albala\'s model, for example, provides only passing m e n t i on of preliminary market assessment and no reference to marketing research in all its seven stages.\nAN IMPROVED PROCESS MODEL\nWhat are the requirements of an ideal process model for industrial new products? First, it must be sufficiently specific and detailed to act as an action guide to managers, yet not to pedantic so as to discourage it must be its use. Second, strongly market oriented, building in market research and mar keting planning throughout the process. A constant concern must be the ideal of developing a product with a differential advantage in the marketplace: one that delivers unique bene fits to the customer. The model must be multidisciplinary and foster internal communication amongst key groups. And it must recognize the high failure rates and risks of new prod ucts, building in timely evalutionand "bail o u t" points through out the process.\nThe stage model presented below is designed to meet these requirements, it was conceived on the basis of the recent re search findings, an analysis of previous normative models, and a review of 60 flow charts of case histories of new product projects.\nThe normative model consists of seven stages. Each stage contains a number of activities, for a total of 16 activities and seven evaluation points (see Fig. 1). The seven stages are:\nI. IDEA\nII. PRELIMINARY ASSESSMENT\nIII. CONCEPT\nIV. DEVELOPMENT\nV. J L u O i l i N VJ\nVI. TRIAL\nVII. LAUNCH.\nEach stage is separated from the previous (or following) stage by an evaluation point or GO/KILL decision n o d e. Thus the process can be truncated at the completion of any one of the stages.\nStage I. Idea\nThe process begins with the definition of a product idea. An idea results when technological possibilities are matched with an expected market d e m a n d. Ideas for a new product may be spotted in the marketplace: a competitor\'s p r o d u c t, recognition of unsatisfied customer needs, or direct requests the from customers. Such " m a r k et pull" projects represent\nO O σ ? W β D ) W W 3 Ο m D D Ο > π H C Ά w Ο Ο 0 C ο 2 Τ S P P Ρ / ο s * r • C 7 r\nI  H   I  E V  C  N  G  U  A  A  T  L  S  I   V  L  E  t  A  e  k  G  t  I  a  r  R  a  e  A  M  T  T  T  »  S  ·  g  n  n  o  i  l  t  t  e  a  k  z  r  l  a  l  n  a  M  a  n  l  l  H  P  o  1  1  [  G  V  S  N  E  G  I  T  A  S  *  h  r  e   E  T  e  p  t  m  y  T  i  S  t  W  o  o  t  t  t  s  a  o  u  e  r  C  P  T  j  1  1  J  T  N  E  V  M   I  E  P  t  g  n  G  O  e  n  m  i  A  t  L  e  p  E  k  T  o  r  l  a  S  V  e  n  M  v  a  E  e  f  l  D  P  o  D  *  O  G  !  ^  J  \'  y  \'  #  n  o  t  i  p  t  e  a  c  u  n  l  a  o  v  C  E  f  l  *  *  T  I  P  I  t  I  E  e  E  k  C  r  G  t  p  a  N  e  M  A  y  c  d  O  t  n  T  a  u  o  e  t  C     S  S  C     T  )  n  l  o  a  c     i  t  t  i  p  a  n  e  r  h  e  c  c  n  n  e  y  n  e  o  T  G  o  C  d  (  i  u  t  a  t  S  c  i  t  f  e  i  k  t  n  r  e  a  d  M  I  1  O  û  H  \\  Y  1     O  y  G  n  r     a  0  Y  n  o  4  T  i  1  i  m  t  R  N  a  i  u  l  A  E  e  I  l  a  r  E I  P  M  v  N  E  I  S     G  /  M  S  A  \\  I  *  L  E  T     E  l  S  S  l     n  y  S  R  y  t  r     r  m  a  l  A  a  a  P  n  n  c  M  i  i  i  m  t  m  n  e  M  h  i  k  i  l  c  l  r  e  e  i  e  a  r  A  r  P  T  M  P  -  *     0  »  -     t  Y  I  f  \\  /  g     0  n  0  i        w  O  l  a  N  i  e  t  i  r  n  c  I  S   I   E     A  I  G  E     A  Ι  D        n  T  /  I  l  S     o  a  t  M  o  i  c  t  t  c  l  i  i  n  t  e  t  u  l  l  v  k  h  v  d  l  r  l  c  o  t  a  t  c  c  e  r  M  P  A  A  T \nA flow diagram of the seven-stage new product process model.\nFig. 1\nN\n8\nI E EE T R A N S A C T I O NS ON E N G I N E E R I NG M A N A G E M E N T, V O L. EM-30, N O. 1, F E B R U A RY 1 9 83\nmajority of industrial new product projects. But "technology p u s h" p r o j e c t s - w h e re the product idea comes from basic re technological discovery-also play a role, par search or a ticularly in the case of radical innovations or breakthrough products, which are important to long term success [ 3 2 ].\nis the first evaluation of the new product idea, Screening and represents the initital decision to c o m m it resources to the idea. At this point, if G O, it becomes a project. The screening decision should be viewed as a "tentative positive" decision, allowing the project to proceed to the next stage, where it will again be reviewed in the light of m o re and b e t t er information. Note that because information is so lacking at this early stage, the decision can only be a tentative o n e; at the same time, the resources c o m m i t t ed by the decision are also small: a limited amount of time and m o n ey to be spent on preliminary assess this incremental commitment ap in Stage II. Using ment proach, resources and uncertainty are balanced; and risk is kept to a tolerable level.\nIdeally, screening is a multilevel culling process. Two ob vious criteria must be met before any product idea receives serious consideration:\n1) Does the proposed product fit with t he company\'s new product guidelines or mission? This culling question under scores the need for a sharply defined new product strategy or "innovation c h a r t e r" for the firm.\n2) Is the project " d o - a b l e" by the company? Does the com pany have the needed resources? Could these resources be readily acquired? And is the project feasible technologically?\nThe third criterion pertains to the attractiveness of the ven ture. Here screening amounts to an investment decision made in the absence of financial data. As a result, nonfinancial prox ies that gauge the attractiveness of the venture must be used. Checklist and scoring models, based on lists of criteria, are often used to rate the attractiveness of alternate projects. Such a scor ng model has been proposed by O M e a ra [29] and an empirically derived industrial product model is also available [ 1 1 ].\nStage II. Preliminary A ssessment\nPreliminary assessment the is first stage where signifi cant resources are spent to gather information regarding the feasibility and attractiveness of the project. This preliminary stage, which includes market assessment and technical assess ment, should be deliberately limited, in terms of time and man power or expense, to a prespecified ceiling.\nPreliminary market assessment involves a quick market study, using in-house (for example, the sales- information force); relying on secondary data (for example, published statistics or research reports); and accessing outside sources (such as several knowledgeable potential customers or industry the information concerns an overview of experts). Desired market; identification of possible segments; market size; and likely prospects for the new product.\nPreliminary technical assessment a m o u n ts to exposing the firm\'s idea the to technical s t a f f - R &D or engineering-for technical appraisal. The key questions concern the technical viability of t he idea and some indication regarding the resources required to develop and produce the p r o d u c t.\nFollowing these two preliminary information steps—market and technical—an evaluation is performed. At this point, pre liminary financial analysis may be possible; but it is likely that qualitative issues will continue to play a major role in the GO/KILL decision. If the decision is G O, t he commitment is to move the project to the next stage, Concept.\nAn illustration:\nA major chemical company has streamlined and com puterized the first two stages of the model as follows: ideas are actively solicited from a number of product sources, and are stored as written proposals in a com puter memory bank. This is called the "white stage." Initial screening is next, and involves t he use of a check list questionnaire, completed by independent evaluators within the firm. Responses are combined and weighted by computer to yield a product score for each project.\nIf an acceptable score is achieved, the project moves to "green stage," where preliminary technical and the market assessment is undertaken. With more complete information, a second computerized evaluation is car ried out, again based on a checklist scoring model, but with more precise information requirements. If G O, the project moves to the "blue stage," where extensive market and technical studies begin.\nStage III. Concept\nThe purpose of the concept stage is to b e t t er define exactly what the product is, who it is aimed at, and h ow it will b* positioned vis a vis market segments and competitive products. This concept definition stage is frequently ommitted by in dustrial product firms, often with disastrous results.\nidentification. The first activity is a market study, concept This market study of potential users or buyers first seeks to identify a hole in the marketplace: a segment of customers who are dissatisfied with what is currently available; a vul nerable or poorly designed competitor p r o d u c t; or a niche where a new technology or new design can gain a competitive advantage. Second, the study identifies what must be done in order to achieve success in this market: for example, the de sired benefits or features sought in a new and "winning" product, and how the product should be positioned.\nAn example:\ntrucks A major manufacturer of high quality highway sought entry into the urban market; namely, d u mp trucks for construction applications. T he product idea was: "let\'s build a high quality d u mp t r u c k ," and a pre liminary investigation revealed that the company could indeed design and build such a vehicle, and that the mar ket was a large and growing one. But t he details of the the product would be po product concept and how sitioned were missing.\nidentification market study was commis A concept sioned. D u mp truck fleet owners were personally inter to determine their choice criteria; attitudes to viewed various OEM\'s products; likes, dislikes, and preferences; reasons for these; areas of discontent; design improve ments desired; etc.\nthe Following truck it became clear that interviews,\nCOOPER: P R O C E SS M O D EL F OR I N D U S T R I AL NEW P R O D U CT D E V E L O P M E NT\ndowntime was a major concern and a costly problem to fleet operators. In fact, many users kept spare ve hicles simply to ensure that they could meet their own customers\' demands. So the new product concept be came: "a truck that can be repaired and back on the road overnight—in 12 h - no matter how serious the b r e a k d o w n ."\nThe design specifications for what constitutes a "better product for t he c u s t o m e r" are defined from the concept iden tification market s t u dy (above). Next comes concept develop ment. Here the m a r k et requirements are translated into an op erational concept, o ne that is technically feasible:\nStage IV. Development\nArmed with the desired concept of a "minimum down time truck," engineers conceived a modular vehicle. Every major " t r o u b le c o m p o n e n t" in the truck was en gineered to be easily removed a nd replaced quickly and easily—engine, transmission, rear axle, electrics, radiator, etc. And standard parts, available from a variety of local suppliers, were used. The minimum downtime concept was indeed technically feasible and at the right price.\nThe final activity in t he concept stage is a concept test: a test of the likely acceptance of the concept by the marketplace. Another market study of potential buyers or users is required; but unlike the c o n c e pt iueninication, the concept test nas something specific to show respondents: sketches, diagrams, models, or descriptions of the proposed product. The object of this study is to obtain a gauge of market acceptance of the new product: interest, liking, preference, and intent to pur chase. Additional information might involve suggested modi fications to the proposed concept, or in the event of rejection, reasons why the c o n c e pt is unacceptable to the buyer:\nStage V. Testing\nAnother example:\nBefore proceeding with commercial development, a manu facturer of health care products conducted a concept test of a total health testing unit aimed at executives in corporations. T he concept was for a trailer to be equip ped with advanced equipment to perform a complete in minutes. The trailer would be medical examination to t he corporate premises where executives brought would be examined on site. T he benefits to the company were thought to be better executive health and no need t W O - H ay a n n u al r h p f k r- n nc jn a hosbltaî. ίΟΓ i n c o n v e n i e nt For the market concept test, the proposed product was described using artists sketches, simulated promotional literature, and a model of the van\'s interior. The visuals, descriptions, and price information were displayed to de cision makers in t he potential client firm, and their re a c t i o n - i n t e r e s t, liking, and purchase i n t e n t - w as gauged.\nThe two market studies in this concept stage do more than identify and test a p r o d u ct concept. They also begin the mar keting planning process. These initial market studies in effect the target m a r k e t; specify the core element of the identify marketing m i x, t he product; and result in a product posi tioning statement. So prior to actual product development, the marketing planning process has been initiated.\nStage VI. Trial\nA concept evaluation decision is n ow m a d e. Note that the concept test m a r k et study provides intent to purchase data\n9\nestimates of market acceptance and expected that permit sales. Similarly, t he concept development, on the technical time in the side, provides estimates of costs. F or the first project, a reasonable financial analysis can be performed. A reliable evaluation is essential at this point before moving the project to the next and more expensive stage: development.\nThe actual product now begins in earnest. development Here technological r e s o u r c e s - R &D and engineering and in dustrial d e s i g n - a re mostly involved. A p r o t o t y pe or product sample is the usual outcome of this stage.\nParalleling the development of the product is the develop m e nt of a formal and complete marketing plan. Here the re sults of the concept stage—target market selection, product strategy, and product positioning—are shaped into a market ing plan. N e x t, t he supporting elements of the marketing r n i x- pricing, distribution, advertising, saiesforce strategy, and ser vice—are decided. These supporting elements may require an ad ditional market study on buyer b e h a v i o r - h ow customers buy the product; w ho the purchase influencers are; sources of product information; e t c . - in order to design an effective mar keting plan.\nThe testing stage is a validation of the product\'s design and features in use. Product prototypes are tested within the com pany to determine that no technical flaws exist. In parallel, a test of t he product is conducted. Prototype samples customer are placed with potential customers for trial to test the pro duct\'s design. The object is to identify design defects, and, in particular, modifications needed to improve customer accep tance.\nDuring t he development of a dial-in-hand t lephone, a telephone manufacturer a s s e m b le 100 proto major type units. Fifty of these went for in-\\ouse t e s t i n g - t he usual reliability and durability t^st. that a n ew tele p h o ne must pass. The other 50 -vent into nontechnical employees\' homes for customer testing.\nThe latter test proved crucial to the product\'s eventual success. Here a potentially disastrous flaw in the de sign was uncovered, one that had been overlooked in t he lab setting: in t he wall p h o ne design, the receiver would îall off in a if a n e a r by d o or w e re s l a m m ed t he h o ok stud-wall construction house. A minor design change to the receiver overcame the problem before millions of phones with faulty receivers would have found their way to telephone companies and eventually into households [ 6 ].\nFollowing t he in-house and customer tests to verify product design, an evaluation is made. A GO decision moves the prod uct to t he trial stage.\ntrial stage represents a " d ry r u n" of all commercial The facets of the project: production, product design, and market ing. Before these trials begin, however, both the product de finalized. sign and the marketing plan must be completed and The customer and in-house tests from the previous stage pro-\n10\nIEEE T R A N S A C T I O NS ON E N G I N E E R I NG M A N A G E M E N T, V O L. EM-30, N O. 1, F E B R U A RY 1 9 83\nvide the inputs to finalize the product design, while the de velopment of a marketing plan has been proceeding since the concept stage.\nrun tests the production m e t h od A trial or pilot production t h at will eventually be used for full scale production. Modifica tions to the final production facilities or methods are often re quired in order to alleviate problems uncovered in the pilot p r o d u c t i o n. In addition, more accurate estimates of produc tion times, throughputs, and costs are obtained in the trial r u n. A test market-selling the product using the proposed marketing plan, but to a limited n u m b er of customers or in a limited geographic area—tests not only the product, but all the elements of the marketing mix together. The identification of needed adjustments to the marketing plan and a final estimate of market share and expected sales are two results of the test m a r k e t:\nT he telephone manufacturer used a trial production run of 1000 of the new dial-in-hand phones to obtain true estimates of production costs and to spot and correct then production problems. These 1000 phones were sold in a test market in one smaller but typical city with the cooperation of the manufacturer\'s own cus t o m e r, the telephone company. All the elements of the marketing mix were tested, including the pricing strategy the communications strategy (a monthly premium); (newspaper, radio, and announcements in the monthly p h o ne bill); and personal selling (use of installers to pro the premium product in new installations). The m o te test market confirmed most elements, but revealed that the manufacturer had sales would be double what t h o u g h t. A revised national launch p l a n - on a region by region basis-was quickly devised, so that production could meet demand [ 6 ].\nA final precommercialization business analysis and evalua financial data from the test tion is m a d e, based on concrete m a r k et and trial production.\nStage VII: Launch\n.\nThe launch stage involves startup of full or commercial p r o d u c t i o n, and the implementation of the marketing plan in the full market area. Note that all facets of the launch have b e en well tested prior to this stage. If the tests have been well carried o u t, and barring any unforeseen or new circumstances in the market, the launch should b? Β simple matter of execut ing a well-designed plan of action.\nPostlaunch evaluation or control points at predesignated times after launch provide benchmarks to gauge whether the p r o d u ct is " on target." Such benchmarks include market share, sales volume, production costs per unit, etc. Post- to control the product and launch evaluations are essential to signal the implementation of corrective schemes to move t he p r o d u ct back on course.\nCONCLUSION\nNew product development will always be a high risk under taking. But m u ch can be learned about effective new product management from a review of the experiences in past new p r o d u ct projects and in other firms. Many of these insights have been incorporated into the seven-stage process model\npresented in this article. No product project will necessarily follow t he model religiously; certainly unforeseen events a nd special circumstances will dictate additional steps, recycling through previous steps, or deletion of certain activities. The in tent was merely to provide a normative guide to managers to ensure that many of the critical steps in t he process are n ot overlooked.\nThe benefits of implementing the model are m a n y. One re sult is that the process becomes more multidisciplinary: n o te that technical and production activities were denoted along the top of the model, and the market oriented activities along the b o t t om (Fig. 1). The balance between the internal versus external orientation becomes obvious. A second payoff is that interaction between varied groups is encouraged: the many evaluation nodes demand diverse inputs from different groups in the c o m p a n y. Also, one activity tends to feed another, often in a different functional a.ea within the firm. Notice for example, t he amount of criss-crossing back and forth between technical activities t h r o u g h o ut the model. A marketing and is the incremental c o m m i t t m e nt nature of the third benefit process: expenditures tend to be balanced with certainty level; each stage involves progressively better information and con currently entails progressively higher expenditures; and risk is managed. Further, decision nodes and bail-out points are pro vided at each stage. Finally, the process is decidedly market oriented, providing for ample market information and mar towards launch phase, b ut the keting planning, not only throughout the entire process.\nNew p r o d u ct success can never be guaranteed. But more thoughtful attention and a systematic approach to the way we proceed to develop and launch products can surely help us avoid m a ny of the pitfalls that have plagued product de velopment in the past.\nR E F E R E N C ES\n\'Stage approach tor the evaluation and selection of [1] A. Albala, R<*D projects." IEEE Trans. Eng. Manag.. voi. E M - 2 2. no. 4, Nov. 1975.\n|2] Booz. Allen, and Hamilton, Management of New Products. New York: Boo/. Allen, and Hamilton. 1968.\nfor 131 R. Calantone and R. G. Cooper, A discriminant model identifying scenarios of industrial new product failure.*\' J. Acad. Marketing Sci.. vol. 7. no. 3. 1979.\n14] R. Calantone and R. G. Cooper, "New product scenarios: Pros pects for success." J. Marketing, vol. 45, Spring 1981.\nR. G. Cooper. "Vv\'hv new Industrial industrial products fail." \\5\\ Marketing Manage., vol. 4. pp. 315-326, 1975.\n. introducing successful new products," European J. Mar 16] keting. MCB Monographs, Bradford. England. 1976.\n[7] . "Identifying industrial new product success- Project New 124-135. Industrial Marketing Manas*., vol. 8. pp. Prod." 1979.\n[8J . "The dimensions of industrial new product success and fail ure," J. Marketing, vol. 4 3. no. 3, Summer 1979.\n. "Project NewProd: Factors in new product success," Euro (9] pean J. Marketing, vol. 14, no. 5/6, 1980.\n. "The myth of the better mousetrap: What makes a new prod [101 uct a success?" Business Quart., vol. 46, no. 1, Summer 1979.\n[11] ^ «Απ empirically derived new product project selection m o d e l ," IEEE Trans. Eng. Manag., vol. EM-28, no. 3. Aug. 1981.\n|12] R. G. Cooper and R. More, "Strategic planning for successful technological innovation." Business Quart.. 1978.\n{13] A. Gerstenfeld. A study of successful projects, unsuccessful projects, and projects in process in West Germany." IEEE Trans. Eng. Manag.. vol. EM-23. pp. 1 16-123, Aug. 1976.\n11\nCOOPER: P R O C E SS M O D EL F OR I N D U S T R I AL NEW P R O D U CT D E V E L O P M E NT\n[14] P. Gisser, "Taking the \'chances\' out of product introduction," industrial Marketing, May 1965.\n[15] S. Globe, G. W. Levy, and C. M. Schwartz, "Key factors and events in the innovation process," Research Manag.. pp. 8-15. July 1973.\n[ 16] K. Gronhang, "Product development in small firms: Some findings and practical implications." Manag. Decision, pp. 67-77. Spring 1973.\n[ 17] M. Hanan. "Effective coordination of marketing with research and Victor of Modern Marketing, in Handbook development," Buell, Ed. New York: McGraw-Hill. 1970.\n[18] D. S. Hopkins. "New product winners and losers." Research Manag.. pp. 12-17, May 1981.\n132]\n[19] D. S. Hopkins and E. L. Bailey, "New-product pressures." ConJ. Board Record, pp. 16-24. 1971.\n133]\n[20] J. E. Klompmaker, G. D. Hughes, and R. 1. Haley. "Test mar in new product development." Harvard Bus. Rev., pp. keting 128-138, May -June 1976.\n[34]\n[21] H. Kulvik, "Factors underlying the success or failure of new products." Univ. of Technology, Helsinki, Finland. Rep. 29, 1977.\nj22j Β Little. "Characterizing the new product for better evaluation and planning." Working Paper Series, no. 21. Univ. W. Ontario. London. Canada. July 1970.\n[23] Management Decision System. Inc., "Growth and development. New product development process." Manag. Decision Syst., Tech. Rep. 15. 1977.\n[37]\n[24) H. Mint/berg. "Patterns of strategy formulation." Manag. Sci.. vol. 24. pp. 934-945, 1978.\n[25] D. Miller and P. Freisen, "Strategy making in context: Ten empirical arehetvpes." J. Manag. Studies, vol. 24. pp. 251-280. 1977.\ni-oj\ninuusinai 3. M y e rs anu u. o. Marquis, "ouccessiui innova tions." National Sci. Foundation. Tech. Rep. NSF 6 9 - 1 7, 1969.\n[27] National Industrial Conference Board, "Why new products fail," Conf. BocrdRec. 1964.\n[28] , "Appraising the marked for new industrial products," ch. 1, National Industrial Conf. Board, Inc., 1967.\n[29] J. T. O\'Meara, Jr.. "Selecting profitable products," Harvard Bus. Rev., pp. 8 3 - 8 9, Jan-Feb 1961.\n[30] R. W. Roberts and J. E. Burke, "Six new products—What made them successful," Research Manag., vol. 19. no. 4. July 1976.\ninnovations." [31] R. Rothwell, "Factors for success in industrial Project SAPPHO-A Comparitive Study of Success and Failure in Industrial Innovation. S.P.R.U., 1972.\n. "Innovation in textile machinery: Some significant factors in success and failure." SPRU Occasional Paper Series, Brighton, Sussex, U. K., no. 2, June 1976.\n. "The Hungarian \'Sappho\': Some comments and compari s o n ." Res. Policy 3. pp. 30-38, 1974.\n, "The characteristics of successful innovations and tech nically progressive firms (with some comments on innovation re search)." R&D Manag.. vol. 7. no. 3, pp. 191-206, 1977\n[35] Rothwell, Freeman, Horsley, Tervis, Robertson, and Townsend, "SAPPHO updated-project SAPPHO phase II," Res. Policy 3, pp. 2 5 8 - 2 9 1, 1974.\n|36] Rubenstein, Chakrabarti, O\'Keefe, Souder, and Young, "Factors influencing innovation success at the project level," Res. Manag., pp. 15-20, May 1976.\nJ. F. Townsend. "Innovation in coal machinery: "The Anderson Shearer loader"—The role of the NCB and the supply industry in its development." SPRU Occasional Paper Series. Brighton. Sussex. L . K. no. 3, Dec. 1976.\n[38] J. M. Utterback. "The process of technological innovation within the firm." Acad. Manag. J.. pp. 75-88, Mar. 1971.\n(39] Utterback. Allen. Holloman. and Sirbu, "The process of innova tion in five industries in Europe and Japan," IEEE Tran:,. Eng. Manag., vol. EM-23, no. 1, pp. 3-9, Feb. 1976.', "Document 7: Fake Defense Deposition.docx\nDeposition Transcript of Herman Bogie\nDeposition Date: April 1, 2024\nLocation: Law Offices of Smith & Associates\nCase: John Doe v. Sierra County and Fairs Are Us\nDeposition of: Herman Bogie\nTaken by: Counsel for Plaintiff\nPresent:\nPlaintiff's Counsel: Jane Smith, Esq.\nDefendants' Counsel: Robert Brown, Esq.\nCourt Reporter: Mary Johnson\nCourt Reporter: Please raise your right hand. Do you swear or affirm to tell the truth, the whole truth, and nothing but the truth?\nHerman Bogie: I do.\nCourt Reporter: Please state your name for the record.\nHerman Bogie: Herman Bogie.\nPlaintiff's Counsel: Good morning, Mr. Bogie. My name is Jane Smith, and I represent John Doe in this matter. Could you please state your full name and position for the record?\nHerman Bogie: My name is Herman Bogie, and I am the general manager of Fairs Are Us.\nPlaintiff's Counsel: How long have you been in this position?\nHerman Bogie: I've been the general manager for about five years now.\nPlaintiff's Counsel: Were you present at the Sierra County Fair on January 23, 2024, the day of the incident involving John Doe?\nHerman Bogie: Yes, I was on site that day.\nPlaintiff's Counsel: Can you describe your responsibilities as the general manager on the day of the incident?\nHerman Bogie: My responsibilities include overseeing the operations of the fair, ensuring safety protocols are followed, managing staff, and addressing any issues that arise during the event.\nPlaintiff's Counsel: Does Fairs Are Us have an Injury and Illness Prevention Policy, or IIPP?\nHerman Bogie: Yes, we do.\nPlaintiff's Counsel: Can you briefly describe the key components of the IIPP?\nHerman Bogie: The IIPP includes regular inspections for hazards, procedures for addressing and remediating hazards, training for employees on hazard recognition and reporting, and maintaining records of all safety-related activities.\nPlaintiff's Counsel: Was the IIPP followed on the day of the incident?\nHerman Bogie: To the best of my knowledge, yes, the IIPP was followed on that day.\nPlaintiff's Counsel: How do you know that the IIPP was followed on January 23, 2024?\nHerman Bogie: We have a structured process for daily inspections, and all activities are documented. I reviewed the inspection reports for that day, and everything was conducted as per our standard procedures.\nPlaintiff's Counsel: Can you provide more detail about the inspection process that was carried out on the morning of the incident?\nHerman Bogie: Sure. Every morning, before the fair opens, our safety officers conduct a comprehensive walk-through of the entire fairgrounds. They use a detailed checklist to identify any potential hazards. These checklists are then submitted to me for review and filed for recordkeeping.\nPlaintiff's Counsel: Did you personally review the inspection report from the morning of January 23, 2024?\nHerman Bogie: Yes, I did. I review all inspection reports daily to ensure that any identified hazards are addressed immediately.\nPlaintiff's Counsel: Were there any hazards identified during the inspection on the morning of the incident?\nHerman Bogie: According to the inspection report, no significant hazards were identified in the area of the Tilt-A-Whirl ride or its exit path.\nPlaintiff's Counsel: If a hazard had been identified, what would the procedure have been to address it?\nHerman Bogie: If a hazard is identified, it is marked immediately, and the necessary remediation steps are taken as soon as possible. The action taken is then documented in a follow-up report.\nPlaintiff's Counsel: Thank you. In the aftermath of the incident involving John Doe, did anyone report that Mr. Doe had been acting drunk?\nHerman Bogie: Yes, one of our staff members, Julie Hing, mentioned that she thought Mr. Doe appeared to be intoxicated.\nPlaintiff's Counsel: What actions, if any, were taken based on Ms. Hing's observation?\nHerman Bogie: We did not take any specific actions based on that observation at the time, as our primary focus was on providing immediate medical assistance to Mr. Doe.\nPlaintiff's Counsel: Thank you, Mr. Bogie. I have no further questions at this time.\nDefendants' Counsel: I have a few questions, Mr. Bogie. Can you confirm that Fairs Are Us maintains comprehensive records of all safety inspections and hazard reports?\nHerman Bogie: Yes, we maintain comprehensive records as part of our commitment to safety and compliance with our IIPP.\nDefendants' Counsel: And to your knowledge, were all required inspections and safety measures in place and documented on the day of the incident?\nHerman Bogie: Yes, to my knowledge, all required inspections and safety measures were in place and documented.\nDefendants' Counsel: Thank you, Mr. Bogie. I have no further questions.\nCourt Reporter: The deposition is concluded at 10:45 AM.", 'Document 8: 2020-02_fp.pdf\nETH\nEidgendssische Technische Hochschule Ziirich Swiss Federal Institute of Technology Zurich\nSeminar for Applied Mathematics\nSAM\nDeep ReLU Neural Network Expression Rates for Data-to-QoI Maps in Bayesian PDE Inversion\nL. Herrmann and Ch. Schwab and J. Zech\nResearch Report No. 2020-02 January 2020\nSeminar für Angewandte Mathematik Eidgenössische Technische Hochschule CH-8092 Zürich Switzerland\nFunding: JZ is supported by the Swiss National Science Foundation under Early Postdoc.Mobility Fellowship 184530.\nDeep ReLU Neural Network Expression Rates for Data-to-QoI Maps in Bayesian PDE Inversion∗\nLukas Herrmann†, Christoph Schwab†, and Jakob Zech‡\nSeminar for Applied Mathematics, ETH Z¨urich, R¨amistrasse 101, CH–8092 Z¨urich, Switzerland. lukas.herrmann@sam.math.ethz.ch, christoph.schwab@sam.math.ethz.ch ‡Department of Aeronautics and Astronautics, MIT, 02139 Cambridge, MA, USA. jzech@mit.edu\nJanuary 9, 2020\nAbstract\nFor Bayesian inverse problems with input-to-response maps given by well-posed partial diﬀerential equations (PDEs) and subject to uncertain parametric or function space input, we establish (under rather weak conditions on the “forward”, input-to-response maps) the parametric holomorphy of the data-to-QoI map relating observation data δ to the Bayesian estimate for an unknown quantity of interest (QoI). We prove exponential expression rate bounds for this data-to-QoI map by deep neural networks with rectiﬁed linear unit (ReLU) activation function, which are uniform with respect to the data δ taking values in a com- pact subset of RK . Similar convergence rates are veriﬁed for polynomial and rational approximations of the data-to-QoI map.\nKey words: Deep ReLU neural networks, Bayesian inverse problems, approximation rates, exponential convergence, Uncertainty Quantiﬁcation Subject Classiﬁcation: 41A25, 41A10, 41A46\n1 Introduction\nIn recent years, computational Bayesian inversion of partial diﬀerential equations (PDEs) sub- ject to uncertain inputs from function spaces (“distributed random inputs”), subject to various function space prior probability measures has received considerable attention. We refer for ex- ample to [21, 5, 6] and to the references there. The currently most widely used computational method for numerical Bayesian inversion with assimilation of noisy observation data is the Markov Chain Monte Carlo (MCMC) algorithm, and its variants (e.g. [15, 14]). In practice, it is obstructed by the low Monte Carlo (MC) convergence rate (at most 1/2 in terms of the number of MCMC proposals) and the need to numerically solve a forward PDE problem of each MCMC proposal, or also by a possibly extended burn-in phase of MCMC to reach asymptotic convergence.\nThese arguments remain valid, in part, also for multilevel variants of MCMC, see e.g. [15, 14] and the references there. Therefore, in recent years, alternative numerical methods have been\n∗JZ is supported by the Swiss National Science Foundation under Early Postdoc.Mobility Fellowship 184530. CS acknowledges stimulating discussions at the RICAM WS on Optimization under uncertainty in November 2019 at RICAM, Linz, Austria, and at the WIAS WS on Deep Learning for PDEs at the Weierstrass Institute Berlin, Germany, 2-6 December 2019.\n1\nproposed which oﬀer the possibility to circumvent the burn-in phase, and which aﬀord po- tentially higher convergence rates than 1/2; see, e.g. [29, 8, 7, 12] and the references there. Parametrization of the function space of uncertain PDE inputs, for example by means of a (Riesz- or Schauder) basis, and constructing a prior measure on the corresponding coordinate domain converts the Bayesian inverse problem (BIP) for the forward PDE with uncertain func- tion space input into a parametric PDE inverse problem on high- or even inﬁnite-dimensional parameter spaces, rendering the BIP amenable to deterministic numerical methods. The high- dimensionality of the parameter spaces obstructs the use of standard numerical methods and has, classically, been addressed computationally by adopting MC-based numerical methods, such as MCMC and its variants, for the numerical solution of PDE BIPs.\n1.1 Previous Work\nIn recent years, eﬃcient deterministic numerical methods capable of overcoming the men- tioned curse of dimensionality in Bayesian PDE inversion and of providing higher (dimension- independent) convergence rates than the rate 1/2 aﬀorded by MC-based methods have been developed. We mention in particular Quasi-Monte Carlo (QMC) (see, e.g., [8, 7]), and Sparse- grid, resp. (adaptive) Smolyak-type numerical integration schemes, see, e.g., [29, 9, 37] and the references there for an analysis of these methods in the presently considered forward and Bayesian inverse uncertainty quantiﬁcation. The mentioned numerical methods do retain their signiﬁcance in the context of training algorithms for deep neural network surrogates (DNNs) for data-to-QoI maps which are numerically approximated by “standard” schemes such as MCMC methods (see, e.g., [15] and the references there) as we will analyze in [11].\n1.2 Contributions\nIn the present paper, we show that the data-to-QoI map which results from Bayesian inversion of a (well-posed) PDE with uncertain input data from function spaces and subject to addi- tive, centered Gaussian observation noise can be expressed by deep neural networks (DNNs) with rectiﬁed linear unit (ReLU) activation function, and certain other, multivariate approx- imation methods, with exponential rate which is independent of the number of coordinates in the parametrization of the uncertain input from function spaces. These mathematical results are based on the strong, regularizing eﬀect of the Gaussian weight in the high-dimensional in- tegration in Bayesian posterior expectation. Due to Bayes’ theorem, the appearance of the Gaussian in the Bayesian posterior expectation is a consequence of the (assumed) centered, Gaussian law of the observation noise in the data. As we show here, the strong smoothing property of a convolution with a Gaussian (or, equivalently, under a heat-ﬂow) will imply ex- ponential expression rates of the corresponding data-to-QoI maps in (Bayesian) inverse UQ. Importantly, this is valid under rather weak assumptions on the parameter-to-response map in forward UQ. Similar smoothing eﬀects have, earlier, been identiﬁed by some of us to fa- cilitate high approximation rates for statistical moments of in general discontinuous solution of nonlinear conservation laws [30]. As a “byproduct” of the present expression rate analysis, we also obtain quantitative bounds on the expression of the data-to-QoI maps by multivariate polynomial and rational surrogate maps in Section 5.2. These approximation rate bounds are of independent interest, as they also justify other approximations (diﬀerent from the presently considered, DNN-based constructions of surrogates, such as tensor-structured surrogates) of these maps. The approximation of the map x 1/x by ReLU NNs analyzed in Appendix C 7→ (needed in our analysis) could also be of independent interest.\nThe proven expression rate bounds by rational models will imply generalization error bounds in either the worst-case or in the mean-square sense. In “learning” data-to-QoI maps, there arises the practically signiﬁcant question of how the DNN (or the mentioned alternative archi- tectures) should be “trained”. I.e., calibrated on a set of (possibly synthetic) “training data”\n2\nand observables of varying levels of ﬁdelity.\nOur analysis will yield, in particular, guidelines for in a sense minimal sets of synthetic training data which are suﬃcient for the calibration of the (polynomial, tensor-structured, or deep ReLU NN) surrogates. In the task of Bayesian PDE inversion considered here, “exact” Bayesian expectations for training the surrogate architectures are usually not available. As mentioned, reference values for surrogate training are rather assumed to be furnished by a numerical algorithm for Bayesian PDE inversion which, being based on PDE discretization and approximate posterior sampling, incurs modeling and discretization errors. Typically, then, several levels of accuracy (or “ﬁdelity”) of the reference values are accessible numerically, at corresponding cost. These extensions will be developed in [11].\n1.3 Outline\nThe outline of the present paper is as follows. In Section 2 we present the general setting for the presently considered class of Bayesian PDE constrained inverse problems. We recapitulate abstract results from [6] and from the references there to delineate suﬃcient conditions for its well-posedness. We distinguish uncertain inputs from ﬁnite and from inﬁnite-dimensional spaces. In Section 3, we present examples of Bayesian inverse problems for two exemplary PDEs (elliptic, with level-set models for uncertain coeﬃcient interfaces, and nonlinear hyperbolic PDEs with uncertain ﬂux functions) which we show to ﬁt into the abstract setting. The holomorphy of the mappings relating observation data δ to the Bayesian posterior expectation and to the normalization constant is shown, for nondegenerate Gaussian observation noise, in Section 4.2. In Section 5.1 we introduce the DNNs considered in the ensuing expressive power bounds. Section 5.2 discusses polynomial and rational approximation of data-to-QoI maps, and Section 5.3 contains the statement and the proof of our main result: exponential expression rate bounds for deep ReLU NNs for the data-to-QoI maps in Bayesian inverse UQ for partial diﬀerential equations. Section 6 contains some conclusions and straightforward generalizations of the present results, in particular an exponential expression rate bound for the ﬁnite-dimensional setting in Section 2.1 and additive noise distributed according to a Lipschitz density ρ with respect to Lebesgue measure. In Appendix C, Lemma C.1, we prove a novel 1 bound for the error of expressing the map [x x ] by ReLU DNNs.\n7→ In [11], we will address bounds on the DNN generalization error for observables in forward UQ and for unobservable quantities of interest in Bayesian inverse UQ constrained by forward PDE models with uncertain inputs from function spaces. There, we also furnish an error analysis of DNN training based on generic, randomized “coaching” routines for Bayesian inversion, such as the mentioned multilevel MCMC algorithms (e.g., [15]) or QMC integration with randomly shifted lattice rules (e.g. [10]).\n1.4 Notation\ndenote the Euclidean norm on RK, K N. For r > 0 We adopt standard notation. Let | · | ∈ we denote by Br(0) the closed ball with radius r in either RK (with respect to the Euclidean norm) or in a Banach space X (which case is meant shall be clear from the context). By π0 we shall generically denote a prior probability measure on a (assumed polish) space of uncertain PDE inputs u. Observation data will be denoted by the symbol δ and is assumed to take values in RK for some ﬁnite value of K. The symbols Z, Z ′ shall denote certain quantities in the Bayesian estimate and, with various sub- and superscripts, bounds on these.\n2 Bayesian Inverse Problems\nIn order to develop the holomorphic dependence of the Bayesian estimate on the observation data (vector) in some generality, we present an abstract setting of BIP, accommodating in\n3\nparticular forward problems given by PDEs with random ﬁeld (“distributed”) uncertain input from function spaces. A reference on the mathematical setting and the well-posedness can be [6] and the references therein. We brieﬂy recapitulate its mathematical setting, as our subsequent analysis will be based on its properties. For ease of presentation we ﬁrst address the ﬁnite- dimensional case, before generalizing to inﬁnite dimensions, as required by PDE constrained Bayesian inversion with uncertain function space inputs.\n2.1 Finite-Dimensional Case\nRn from noisy observation data δ RK. We assume We wish to infer uncertain input data u ∈ ∈ that the noiseless response δ is related to the uncertain input u by a data-to-observable map δ = G(u). Assuming δ is only accessible up to additive, centered observation noise denoted by a mean-zero random variable η Q0, we postulate\nδ = G(u) + η , η Q0.\nRn is a random We model uncertainty in the input by furthermore assuming that u ∈ variable (RV) whose law admits a Borel measurable prior density ρ0 w.r. to the Lebesgue measure λn, i.e.,\nu π0 := ρ0(u)λn .\nWe assume in (1) that the forward map [G : Rn RK : u δ] is Borel measurable and that → 7→ Q0 is independent of the uncertain input u. In the case that the law the observation noise η ∼ Rn RK is a RV with product density Q0 of η admits a density ρ w.r. to λK, the pair (u, δ) ∈ × δ (read “u given observation data δ”) is then G(u))ρ0(u). The distribution of the RV u ρ(δ − | given by the following result.\nTheorem 2.1 (Bayes’ Theorem) Assume that the data δ RK is such that\nZ = Z(δ) := ZRn ρ(δ − G(u))ρ0(u)du > 0 .\nThen, u density | δ is a RV on Rn distributed according to the posterior πδ. The posterior πδ admits the\nρδ(u) = 1 Z ρ(δ − G(u))ρ0(u) , u ∈ Rn\nwith respect to the Lebesgue measure λn on Rn.\nThe expression (u, δ) ρ(δ G(u)) in (3), (4) is also referred to as the likelihood. The negative 7→ − log-likelihood, denoted by Φ, will be referred to as Bayesian potential, i.e.\nΦ(u; δ) := log ρ(δ G(u)) .\nDenoting for δ ∈ RK the measure with density ρδ in (4) as πδ, we may write (3), (4) as\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\ndπδ dπ0 = 1 Z exp( − Φ(u; δ)), Z := Zu∈Rn exp( − Φ(u; δ))π0(du) .\nRemark 2.2 The assumptions imply that the map [δ Z(δ)] is a probability density: the RV 7→ (δ, u) admits the joint density ρ(δ G(u))ρ0(u). The marginal density [δ Z(δ)] is given by 7→ − Z(δ) = G(u))ρ0(u) du. ρ(δ\n4\nExample 2.3 (Gaussian observation noise) Assume that in (1), the observation noise η is centered, nondegenerate Gaussian observation noise on RK. Then Q0 ∼ RK×K ric, positive deﬁnite covariance matrix Σ sym , so that ∈ N (0, Σ) with symmet-\nρ : ( RK ζ 7→ R → (2π)−K/2det(Σ)−1/2 exp( − ζ ⊤Σ−1ζ/2),\nwhence\nΦ(u; δ) = − log ρ(δ − G(u)) = 1 2 (δ − G(u))⊤Σ−1(δ − G(u)) + 1 2 log((2π)Kdet(Σ)),\ni.e. the Bayesian potential is the negative log-likelihood, respectively the (observation noise) covariance-weighted data-to-prediction misﬁt functional.\nFor any given, measurable QoI φ : Rn the data δ RK, is → R, the expected value under the posterior, given\ndr® 1 dmg A exp(—®(u; ))(u)mo(du)\n2.2 Inﬁnite-dimensional Case\nWe denote by X and Y real, separable Banach spaces, equipped with the Borel sigma-algebra. In the ﬁnite-dimensional setting of the preceding section, X = Rn denotes the space of uncertain inputs and Y = RK denotes the data space. Here, we retain Y = RK ﬁnite-dimensional, but admit X to be a real, separable Banach space corresponding to uncertain function space input for PDEs.\nThe forward (“input-to-observation”) map will again be denoted by G : X G be measurable and consider again the BIP: given noisy observation data δ such that → ∈ Y . We assume Y , ﬁnd u ∈ X\nδ = G(u) + η .\nHere, η denotes a Y -valued RV which describes additive observation noise on the data δ. As- sumption (9) renders (u, δ) X Y a RV with respect to the product sigma algebra.\n∈ × δ. To calculate it, we place a (Bayesian) prior probability We are interested in the law of u | (X)), and a probability measure Q0 on (Y, (Y )) corresponding to the measure π0 on (X, B B distribution of η. We assume Q0 to be centered, and the RVs u and η to be independent. Then, the product probability measure ν0 = π0 ⊗ (u, δ) Y , we observe that given u X X, δ × ∈ ∈ | Q0 translated by G(u). We assume Q0 is well-deﬁned. To derive the law of u is a RV taking values in Y with law Qu being\nQu ≪ Q0 π0-a.e. u ∈ X .\nX the nonnegative Radon-Nikodym derivative dQu This assumption implies that for π0-a.e. u ∈ R, i.e. Φ( Φ(u; δ)) with the log-likelihood ; δ) : X exists and we denote it by exp( dQ0\ndQu dQ0 = exp( − Φ(u; δ)) .\nThen, for π0-a.e. u X, Φ(u, ) : Y ∈ → · more, the law of the RV (u, δ) is ν = π0 ⊗ R is measurable with EQ0 [exp( Qu and ν ≪ ν0 with − Φ(u; ))] = 1. Further- ·\ndν dν0 = exp( − Φ(u; δ)) .\n(7)\n(8)\n(9)\n(10)\n(11)\n5\nTheorem 2.4 Assume that Φ : X Y R is ν0-measurable with\nZ := ZX exp( − Φ(u; δ))π0(du) > 0 Q0-a.e. δ ∈ Y .\nThen the law of u (u, δ) holds δ, denoted as πδ, exists for Q0-a.e. δ, and πδ | ≪ π0. Moreover, for ν-a.e.\ndπδ dπ0 = 1 Z exp( − Φ(u; δ)) .\nWe refer to [6, Theorem 3.4] for a proof.\n(12)\n(13)\n3 Examples\nThroughout this section we use several times the following: if the forward solution operator G : X ′ RK is continuous, then G is Borel-Borel measurable and thus the map (u, δ) G(u)) ρ(δ 7→ − → is Borel-Borel measurable provided the density ρ : RK R is Borel-Borel measurable. Since ρ → is a Lebesgue density, ρ is in general only Lebesgue-Borel measurable. However, in all examples considered below, ρ is actually continuous and thus also Borel-Borel measurable.\nWith this prerequisite in mind, the preceding, abstract setting accommodates a wide range of Bayesian inverse problems. Before addressing DNN expression rate bounds, we illustrate in this section the scope of the present setting by verifying the above, general assumptions for a selection of parametric PDE problems with uncertain PDE inputs from (subsets X ′ of) function spaces X. The prior probability measure π0 in (10) will, in this case, be a pushforward of a ), to a separable subset X ′ of a Banach space probability measure P on a measurable space (Ω, F X of admissible inputs for the PDE under consideration. Speciﬁcally, we suppose that u : Ω X is strongly measurable. This implies by Pettis’ theorem that there exists a measurable subset → Ω′ Ω such that P(Ω′) = 1 and Ω′ u(ω) : ω is separable in X, cf. [36, Theorem V.4]. { ∈ ⊂ } We refer to [5, Section 2] for a detailed derivation of such priors π0 for linear, well-posed elliptic PDEs with uncertain coeﬃcients. Rather than covering the most general case, we opt for developing two PDE models and also discuss examples of priors, which we construct as the law of a strongly measurable random ﬁeld u. More PDE problems are admissible in our framework, for example the problem to recover the unknown conductivity from noisy boundary measurements in Calder´on problems, see [1] and the references therein.\n3.1 PDE models\nWe will consider forward data-to-solution maps which are realized through the solution of a governing PDE for uncertain function space input. Generally, the uncertain function space input is denoted by u X, which should be constrained such that the PDE under consideration ∈ is well-posed for this input data. For that reason, we may restrict the function space X to a subset X ′. The unique solution given input u is denoted by q V and the forward solution ∈ map is denoted by (u) V , where V is a Banach space. q = , i.e., u\n∈ S S 7→ In numerical Bayesian inversion of a PDE, we aim at computing a conditional expectation V ∗, which is here assumed to be a linear of a Quantity of Interest (“QoI” for short) φ ∈ (V ∗)K, (u) + η where functional. To this end, we assume at hand (noisy) observations O ∈ O ◦ S RK is a RV on RK whose law admits a Borel measurable density ρ. In and, as before, η ∈ this case the input-to-observation map G in (9) is given by G = . We assume that the O ◦ S X ′, which is strongly measurable with respect prior π0 is the law of a random ﬁeld u : Ω → : X ′ V to the topology of X. Moreover, we assume that the forward solution operator → S is continuous. Then, as a composition of two continuous maps the data-to-observation map : X ′ Y is also continuous. The strong measurability of u : Ω X ′ implies that the\n6\nobservable [G : Ω Y : u )(u)] is a RV, i.e. measurable with respect to the Borel ( 7→ → O ◦ S sigma algebra of Y = RK. Let us assume that\nEπ0 [ | φ | ] = ZX | (φ ◦ S )(u) | π0(du) < ∞ .\nThen, given noisy observation data δ form ∈ Y , the posterior expectation of the QoI φ takes the\nEπδ [φ] = 1 Z ZX (φ ◦ S )(u)ρ(δ − G(u))π0(du) .\nThis expression is well-deﬁned by the (assumed) measurability of the density ρ with respect to the Borel sigma-algebra.\n3.1.1 Diﬀusion equations\nRd, given a (assumed uncertain) coeﬃcient u In a bounded Lipschitz domain D ⊂ and a deterministic (i.e., deterministic assumed known with certainty) source term f as a forward model, we are interested in ﬁnding q H 1 0 (D) such that ∈ ∈ L∞(D) L2(D),\nf + ∇ · (u ∇ q) = 0 in H −1(D) , q |∂D = 0 .\nL∞(D) such that ess inf x∈D u(x) > 0, the forward problem (16) As is well-known, for every u ∈ H 1 0 (D). Here V = H 1 admits a unique variational solution q 0 (D). For ﬁxed f in (16), the ∈ input-to-solution map\nS : { u ∈ L∞(D) : ess inf x∈D u(x) > 0 } → V : u 7→ q\ninduced by (16) satisﬁes\nkS (u) kV ≤ f kV ∗ k ess inf x∈D{ u(x) } .\nThe map is Lipschitz continuous which implies measurability of the likelihood as follows. For S any u, u′ L∞(D) : ess inf x∈D u(x) > 0 W 1,r(D) for some r such that u (u) [2, } ∈ S ∈ { ∈ ∈ there holds ∞ ),\n(u) kLr(D) u′ (u′) k∇S u kL2r/(r−2)(D) . (u) kV ≤ u′(x) ess inf x∈D{ kS − k − S }\nbeing (Borel) measurable, we endow X ′ with For X ′ L∞ : ess inf x∈D{ u(x) u > 0 ⊂ { ∈ } } the L∞(D)-norm and suppose that X ′ is separable with respect to the L∞(D)-norm. In this case, r′ = 2r/(r W 1,r(D) is satisﬁed by (18). Thus, and r = 2. Note that 2) = (u) S ∞ − ∈ : X ′ by (19), the forward operator V is Lipschitz continuous. But the veriﬁcation of the → S condition (14) becomes non-trivial and shall be discussed in the particular construction of the prior (see ahead Section 3.2.3).\nSuppose there exists C > 0 such that\nX ′ = { u ∈ L∞(D) : C −1 ≤ ess inf x∈D{ u(x) } ≤ k u kL∞(D) ≤ C } .\n(W 1,r/(r−1) X ′ if also f W 1,r(D) for every u Then, there exists r > 2 such that (u) 0 ∈ S ∈ ∈ cf. [2, Proposition 1] (the conditions of [2, Proposition 1] are veriﬁed for example by [20, (D))∗, Theorems 1.1 and 1.3]). Note that the earlier assumption f (W 1,r/(r−1) (D))∗ for d = 1, 2 and for d > 2 if r < 2d/(d 0 Lr′ (D)-norm for r′ = 2r/(r : X ′ 2). Thus, by (19), − → S case the condition (14) is always satisﬁed, which follows by (18). L2(D) implies that f ∈ 2). We endow X ′ with the − V is Lipschitz continuous. In this ∈\n(14)\n(15)\n(16)\n(17)\n(18)\n(19)\n7\n3.1.2 Scalar hyperbolic conservation law\nWe consider the Cauchy problem for the scalar, nonlinear hyperbolic conservation law\n∂tq + ∂x(u(q)) = 0 , q |t=0 = q0 .\nL1(R) has bounded variation and is assumed known, The initial condition q0 ∈ i.e., de- terministic. Denote by M := q0kL∞(R) and note the maximum principle satisﬁed by the k (unique) entropy solutions, cf. e.g. [16, Theorem 2.14(i)]. The Lipschitz continuous ﬂux func- W 1,∞([ tion u M, M ]) in (20) is assumed to be uncertain. For any ﬁxed realization ∈ − W 1,∞([ M, M ]) of the ﬂux function u in (20), there exists a unique entropy solution u − ∈ q to (20) by [16, Theorem 2.14]. For ﬁxed t > 0, let us denote by St the operator u 7→ St(u). The L1(R)-norm of the entropy solution at time for ﬁxed initial data q0, i.e., q(t) = t > 0 is bounded in terms of the data. Speciﬁcally, by [16, Theorem 2.14(vi)] and the triangle q(t), inequality\nkSt(u) kL1(R) ≤ k q0kL1(R) + tTV(q0) k ∂xu kL∞([−M,M ]).\nFurthermore, the entropy solution q depends Lipschitz continuously on the ﬂux function u: by [16, Theorem 2.13], for any two Lipschitz ﬂux functions u, ˜u every t > 0 holds ∈ W 1,∞([ − M, M ]) and for\nkSt(u) − St(˜u) kL1(R) ≤ tTV(q0) k ∂x(u − ˜u) kL∞([−M,M ]).\nSt : W 1,∞([ Thus, for every t > 0, the forward (“ﬂux-to-entropy solution”) map M, M ]) − L1(R) is continuous. Hence, we are in the abstract setting for Bayesian inversion with X = → W 1,∞([ M, M ]) and V = L1(R).\n3.2 Priors\nWe present several constructions of parametric function space priors, for which our results apply. These constructions are by no means meant to be exhaustive; they are merely listed, with references, in order to illustrate the scope of applicability of our principal results on DNN expression rate bounds for PDE constrained Bayesian inverse problems.\n3.2.1 Level set priors\nWe shall discuss in some detail a class of uncertain u such that u is piecewise constant and attains known values (or levels) at uncertain locations in the spatial domain D, i.e.,\nn u = ui✶Di i=1\nfor certain numbers ui ∈ (0, ), i = 1, . . . , n, and uncertain subsets Di of D such that D = ∞ n = i′. Suppose that g : Ω C 0(D) is a strongly measurable i=1 Di and Di ∩ for i Di′ = → ∅ , P). For constants = c0 < Gaussian random ﬁeld on an auxiliary probability space (Ω, −∞ A S , deﬁne the function F : R c1 < . . . < cn = ) by (0,\nn F = ui✶ [ci−1,ci). i=1\nIt follows that u := F (g) satisﬁes (23) with the uncertain sets Di deﬁned by\nDi := { x ∈ D : ci−1 ≤ g(x) < ci} , i = 1, . . . , n.\n(20)\n(21)\n(22)\n(23)\n8\nLr(D) deﬁned by ω Lemma 3.1 The level set random ﬁeld u : Ω → strongly measurable with respect to the topology of Lr(D) for every r 7→ [1, u(ω) := F (g(ω)) is ).\nProof. By deﬁnition of strong measurability, cf. e.g. [36, Deﬁnition V.4.1], there exist functions gN i ∈ as N F ( N ✶ i=1 gN C 0(D) and measurable, disjoint sets ΩN g Ω such that ΩN i ⊂ j kC0(D) → i k − N ✶ i=1 gN P-a.s. This is to say that is “ﬁnitely valued”. We observe that P ΩN i → ∞ j N N ✶ i )✶ i=1 gN is also ﬁnitely valued, since the sets ΩN i=1 F (gN ) = i , i = 1, . . . , N , are P ΩN ΩN i j j 0 disjoint.\nP P v is a mapping from C 0(D) to Lr(D) for every r F We state the fact that v ). [1, ∈ ∞ ◦ 7→ C 0(D) if and only if This map is continuous at some v D : v(x) = ci} is a nullset with x ∈ ∈ { 1, cf. e.g. [19, Proposition 2.6]. respect to the Lebesgue measure for every i = 2, . . . , n\n− By [19, Proposition 7.2], P( x D : g(ω)(x) = ci}| Ω : ω = 0 ) = 1, i = 1, . . . , n 1. ∈ − ∈ |{ { } N i )✶ i=1 F (gN 0 as N F (g) This implies with the aforementioned fact that ΩN j kLr(D) → − k → ∞ ✷ g is strongly measurable with respect to Lr(D). P-a.s. Thus, the composition F P\n\nDiﬀusion equations\nIn the setting of Section 3.1.1 we let X := L∞(D), X ′ := L∞(D) : min u1, . . . , un} ≤ u { { ∈ and V := H 1 0 (D). There exists r′ a.e. x D ) in dependence [2, max u1, . . . , un} u(x) ∈ ∞ ∈ } ≤ { : X ′ of min and max V is continuous, see Section 3.1.1, u1, . . . , un} such that u1, . . . , un} S → { { where we endowed X ′ with the Lr′ X ′ is strongly measurable (D)-norm. By Lemma 3.1, u : Ω → g)−1(A)) for all Borel and we deﬁne the prior on X ′ as the law of u, i.e., π0(A) = P((F ◦ X ′. measurable A\n3.2.2 Aﬃne parametric priors\nFor m 0, 1 ﬁxed, we assume at hand a nominal coeﬃcient u0 ∈ ∈ { } X = W m,∞(D) such that ψj}j≥1 ⊂ representation system Ψ = { W m,∞(D) and a countable\nXj≥1 k ψjkW m,∞(D) < ∞ .\nThen we consider the parametric coeﬃcient u given by\nu(x, y) = u0(x) + Xj≥1 yjψj(x) , x ∈ D, y = (yj)j≥1 ∈ Ω := [ − 1, 1]∞ .\nΩ the inﬁnite series (25) converges in W m,∞(D). We By our assumptions, for every ﬁxed y ∈ denote by X ′ X the set of all limits in X of the parametric expansion (25), X ′ = u : u = { ⊂ j≥1 yjψj for some y Ω . u0 +\n∈ } We now construct a prior measure π0 on the set X ′ X. Again, we start by introducing an ⊂ P 1, 1]∞, , P), where Ω = [ = auxiliary probability space (Ω, ([ 1, 1]) is the product ⊗j∈N A A − B − Borel sigma algebra, and P := ⊗j≥1λ/2 is the (countable) product probability measure, where λ/2 denotes the scaled Lebesgue measure on [ 1, 1]. The measure π0 is deﬁned as the pushfor- − X ′. The X ′ as in (25), i.e. π0(A) = P(u−1(A)) for all measurable A ward of P under u : Ω → ⊆ 1, 1]∞ is compact sigma-algebra on X ′ X is again the Borel sigma-algebra. The set Ω = [ ⊆ − when equipped with the product topology, by Tychonoﬀ’s theorem (e.g. [28, Theorem A3]). In this case the Borel-sigma algebra on Ω coincides with the product sigma-algebra ⊗j∈N B ([ − 1, 1]).\nDiﬀusion equations\nFor the diﬀusion equation (16), we let m = 0 so that X = L∞(D) and V = H 1 0 (D). Fur- L∞ : ess inf x∈D{ thermore, we impose X ′ u(x) to ensure well-posedness of the > 0 u } ∈ ⊂ { }\n(24)\n(25)\n9\nforward problem. Suppose in addition that u0 satisﬁes\ness inf x∈D u0 ≥ umin > 0.\nThen\ninf u∈X ′ ess inf x∈D u = inf y∈Ω ess inf x∈D u(x, y) ≥ (1 − κ)umin > 0 ,\nwhich implies X ′ ⊂ { satisﬁed for every QoI φ u ∈ L∞ : ess inf x∈D u(x) > (1 V ∗. − κ)umin} . By (18), condition (14) is\nScalar conservation laws\nFor the scalar conservation law in (20), we let m = 1, X = W 1,∞(D) and D = [ M, M ], − q0kL∞(R) and q0 is the initial data. Furthermore V = L1(R). The where we recall that M = k V ∗. condition (14) is satisﬁed as a consequence of (21) and (24) for every QoI φ\n3.2.3 Log-Besov parametric priors\nLet m 0, 1 . Consider now u taking log-aﬃne form, i.e.\nu(x, y) = u0(x) + exp yjψj(x) Xj≥1 x ∈ D.\nX = W m,∞(D) satisﬁes ess inf x∈D{ where u0 ∈ the function system Ψ = (ψj)j≥1, i.e., u0(x) } ≥ 0. We assume again summability of\nXj≥1 k ψjkW m,∞(D) < ∞ .\nFor p [1, 2], we deﬁne P as the product measure\nP(dy) := Oj≥1 p 2p1/pΓ(1/p) e− |yj |p p dyj\non Ω := R∞, where Γ denotes the Gamma function. We suppose that y in (26) is distributed according to P, and Ω is equipped with the product Borel sigma algebra. As a consequence [10, Proposition 3.2], it holds that\ness inf x∈D { u(x, y) } > 0 P − a.e. y ∈ Ω,\nwhere for p = 1 we require supj≥1 k ψjkL∞ < 1.\nW m,∞(D) : u = u0 + exp( Here, X ′ = u ∈ { tion 3.2], in the case p (1, 2] j≥1 yjψj), P − a.e. y ∈ Ω } . By [10, Proposi-\nZΩ k u(y) kW m,∞(D)P(dy) < ∞ ,\nwhich also holds for p = 1 if supj≥1{k ψjkW m,∞(D)} < 1. Also by [10, Proposition 3.2], u : X ′ is strongly measurable. We construct the Besov prior π0 as the law of u, i.e., π0(A) := Ω → X ′. P(u−1(A)) for every measurable A\nDiﬀusion equations\nFor the PDE model (16), m = 0, X = L∞(D) and V = H 1 by (27) and (18) for every QoI φ V ∗. 0 (D). The condition (14) is satisﬁed\n(26)\n(27)\n10\nScalar conservation laws\nFor the PDE model (20), m = 1, X = W 1,∞(D) and D = [ M, M ], where we recall that − q0kL∞(R) and q0 is the initial condition. Furthermore V = L1(R). The condition (14) is M = k V ∗. satisﬁed by (27) and (21) for every QoI φ\nRemark 3.2 For p = 2 and u0 ≡ 0, we recover the widely used case of parametric log-Gaussian diﬀusion coeﬃcients u as a special case of the constructed log-Besov priors. Hence, BIPs with Gaussian priors are also covered by the present results.\n4 Regularity of the Data-to-QoI Map\nEπδ We now investigate the regularity of the data-to-QoI map δ [φ]. This regularity is crucial, 7→ on the one hand, for the ensuing DNN expression rate analysis and, on the other hand, will be seen to determine to some extent the DNN architecture. As we show, this regularity is strongly dependent on the regularity of the density ρ of the observation noise η in the additive model (1).\n4.1 Lipschitz Regularity\nWe assume we are in the ﬁnite-dimensional case described in Theorem 2.1 and that the density function ρ in (3), (4) is globally Lipschitz continuous.\nExample 4.1 Consider the function ρ : RK R : ζ exp( 7→ → . Then ρ is globally Lipschitz, since for ζ, ζ ′ k1 = ζ ζ1| ζK| + ... + k | | −k ∈ ζ k1) where, for ζ RK holds ∈ RK,\nρ(ζ) | − ρ(ζ ′) | = | exp( −k ζ k1) − exp( −k ζ ′ k1) | ≤ |k ζ k1 − k ζ ′ k1| ≤ k ζ − ζ ′ k1 .\nThe Lipschitz property of ρ is inherited by the data-to-QoI map δ Eπδ [φ] in (8).\nLip(RK) with respect to some Proposition 4.2 In the setting of Theorem 2.1, assume that ρ ∈ on RK. Suppose in addition that the QoI φ satisﬁes φ L1(X ′, π0). norm\n∈ k ◦ k Eπδ [φ]] Then, for every r > 0 the map [δ Lip(Br(0)), i.e, there exists a constant ∈ 7→ C(r, φ) > 0 such that\nEπδ′ Eπδ δ, δ′ δ′ [φ] δ [φ] Br(0) : C . − ≤ k − ∈ ∀ k\nProof. Let for the moment 8,0’ € RX be arbitrary realizations of the observation data. Fur- thermore, denote Z(8) := Z(§)E™ [¢] and Z\'(\') := [¢]. Then,\nZ\'(9) — (%) = 12\'() = Z\'( | |Z\'()Z) — Z ()] t Z(9) Z() Z(9) 2()Z(\')\nBy assumption the density p is globally Lipschitz on RX with constant Cri, > 0. Thus by definition of Z in\nZ(δ) − Z(δ′) | ≤ CLipk δ − δ′ k ZX π0(du) = CLipk δ − δ′ k\nSimilarly, Z ′(δ) | − Z ′(δ′) | ≤ CLipEπ0 [ | φ | ] k δ − δ′ k . For every r > 0, deﬁne\nZmin,r := inf δ′′∈Br(0) Z(δ′′) and Z ′ max,r := sup δ′′∈Br(0) Z ′(δ′′).\n(28)\n11\nZ ′(δ)] are Since we have proven already that the nonnegative mappings [δ Z(δ)] and [δ → → Lipschitz continuous on RK, they achieve their minimum and maximum on the compact set Br(0) RK so that 0 < Zmin,r, Z ′ max,r < . The assertion now follows with\nC = CLip Eπ0 [ φ | Zmin,r | ] + Z ′ max,r Z 2 min,r ! .\n4.2 Holomorphy\nWe now establish one core result of the present paper, namely the holomorphy of the data- to-QoI map which results from the expectation under the Bayesian posterior, for the negative log-likelihood Φ in (5) being a quadratic. This corresponds to the important assumption that the additive observation noise η in (1) is centered, Gaussian.\nRK for some integer K As we admit vector-valued data δ 1 and as the QoI φ is assumed ∈ ≥ to be scalar, taking values in R (or, upon complexiﬁcation, in C), this amounts to verifying holomorphy of a scalar function of K complex variables. Based on standard results (e.g. [17]) on functions of several complex variables, we shall verify this ﬁrst in the univariate case (i.e., for K = 1) and subsequently infer holomorphy of the multivariate map by means of Hartogs’ theorem.\nWe shall use the following technical result on averages of holomorphic maps, which is asser- tion C3 of the Theorem in [23].\nProposition 4.3 Let (Ω, the functions f : Ω G , π) be a measure space and let G A C satisﬁes ⊂ C be an open set. Suppose that\n(i) [Ω ω f (ω, z)] is measurable with respect to for every z G.\n(ii) [G z f (ω, z)] is holomorphic for every ω Ω.\n(iii) for every z0 ∈ G, there is δ > 0 such that supz∈G,|z−z0|≤δ Ω | f (ω, z) | π(dω) < ∞ .\nThen, [G ∋ z 7→ Ω f (ω, z)π(dω)] is holomorphic.\nR Proof. This is assertion C3 of the theorem in [23].\n4.2.1 Univariate Data (K = 1)\nLemma 4.4 In the setting of Theorem 2.4 let K = 1 and assume that φ Φ(u; δ) = (δ G(u))(δ G(u))/(2σ2) for some σ > 0. ∈ L1(X ′, π0) and\nThen the map\nδ 7→ ZX ′ φ(u) exp( − Φ(u; δ))π0(du)\nis holomorphic on C.\nProof. We shall verify assumptions (i), (ii) and (iii) in Proposition 4.3. Since u Φ(u; δ) is 7→ X ′, the same is true C and δ Φ(u; δ) is holomorphic for π0-a.e. u measurable for every δ ∈ 7→ ∈ for the map (u, δ) exp( Φ(u; δ)). This completes the veriﬁcation of assumptions (i) and (ii) 7→ − in Proposition 4.3.\nIt remains to show that X ′ φ(u) exp( Φ(u; δ))π0(du) is locally bounded for every δ − 2δG(u) + G(u)2. For some δ0 ∈ holds 2σ2Φ(u; δ) = δ2 C, consider arbitrary δ R ∈ − δ δ0| ≤ 1. By the triangle inequality, − | C. It ∈ C such that\n| exp( − Φ(u; δ)) | ≤ | exp( − δ2/(2σ2)) | exp( − (G(u)2 − 2 | G(u) [ | | δ0| + 1])/(2σ2)).\n✷\n12\nBy maximizing the quadratic polynomial G(u) 7→ − G(u)2 + 2G(u)( δ0| | + 1) we get\n(160] + 1) | exp(—(u))] < — 202 )\nwhere we used that G(u) ∈ R for u ∈ X ′. Thus, for every δ0 ∈ C,\nsup 5EC,|5—0|<1 o exp(us) a(an) < (1] + 1) = sup 202 €EC,[6—o|<1 |exp(=/(20%)) | exp I < 0.\nπ0(dy) is This veriﬁes assumption (iii) in Proposition 4.3, i.e., δ φ(u) Φ(u; δ)) exp( X ′ | 7→ − | || C. The assertion of the lemma is then implied Proposition 4.3. locally bounded for every δ ∈ R ✷\n4.2.2 Multivariate Case (K > 1)\nN and assume that φ Lemma 4.5 In the setting of Theorem 2.4 let 1 < K ∈ G(u))⊤Σ−1(δ RK×K. G(u))/2 for some SPD matrix Σ Φ(u; δ) = (δ ∈ L1(X ′, π0) and\n~ Then, for each fired € RE1, the mapping\nδ 7→ ZX ′ φ(u) exp( − Φ((δ; δ); u))π0(du)\nis holomorphic on C.\nProof. The assertion is proven similarly to Lemma 4.4 and is also a consequence of Proposi- = +√δH δ the modulus with CK we denote by tion 4.3. In the ensuing argument, for δ δ ∈ | | respect to the Euclidean norm on CK and, for a K K real, symmetric matrix Σ, we denote × by Σ its spectral norm.\nk k The mapping C X ′ exp( Φ(u; δ))π0(du) satisﬁes assumptions (i) and (ii) in Propo- δ → − ∋ sition 4.3.\nR CK For the veriﬁcation of assumption (iii) in Proposition 4.3, let us denote δ := (δ; δ) ∈ 2δ⊤Σ−1G(u) + CK−1. We observe that 2Φ(u; δ) = (δ⊤Σ−1δ C and δ for any δ − ∈ ∈ G(u)⊤Σ−1G(u)). Moreover, it holds that e\ne exp(δ⊤Σ−1G(u)) | = exp(δ⊤ ReΣ−1G(u))) ≤ exp( k Σ−1 k| δRe|| G(u) ). |\nCK−1 and δ0 ∈ C be arbitrary and denote δ0 := (δ0; δ Let ∈ eigenvalue of Σ1/2. For every δ C such that δ δ0| ≤ 1, − | ∈ δ). Denote by σ > 0 the largest\nΦ(u; δ)) exp( − | δ⊤Σ−1δ/2) 2/(2σ2) + exp( exp( G(u) − ≤ | −| || | k e Σ−1 k ( | δ | + δ0| | + 1) | G(u)\n)\n|\n|\n.\nSimilar as in the proof of Lemma 4.4, maximizing the polynomial s δ0| | + 1)s, we get e 7→ − s2/(2σ2) + k Σ−1 k ( | ˜δ | +\nexp( − Φ(u; δ)) | ≤ | exp( − δ⊤Σ−1δ/2) | exp ( | δ | + δ0| | + 1)2σ2 2 k Σ−1 k 2 !\n|\nThus\nsup δ∈C,|δ−δ0|≤1 ZX ′ | φ(u) || exp( − Φ(u; δ)) | π0(du) < ∞ ,\nwhich establishes the local boundedness of δ X ′ φ(u) exp( Φ(u; δ))π0(du), i.e., assumption → − (iii) in Proposition 4.3. The assertion of this lemma then follows from Proposition 4.3. ✷\n13\nN and assume that φ Proposition 4.6 In the setting of Theorem 2.4 let 1 < K ∈ G(u))⊤Σ−1(δ RK×K. and Φ(u; δ) = (δ G(u))/2 for some SPD matrix Σ ∈ L1(X ′, π0)\nThen the map\nis holomorphic on CK.\nδ 7→ ZX ′ φ(u) exp( − Φ(u; δ)π0(du)\nProof. Lemma 4.5 implies holomorphy in every coordinate δi, i = 1, . . . , K. The assertion now follows by Hartogs’ theorem, cf. [17, Theorem 2.2.8]. ✷\nCorollary 4.7 In the setting of Theorem 2.4 and Proposition 4.6, and for every ﬁnite r > 0, the map\n[ r, r]K δ Eπδ [φ] R\nadmits a holomorphic extension to some open set such that [ r, r]K CK.\nProof. By Proposition 4.6 the maps\nδ 7→ Z ′(δ) = ZX ′ φ(u) exp( − Φ(u; δ))π0( du), δ 7→ Z(δ) = ZX ′ exp( − Φ(u; δ))π0( du),\nadmit (unique) holomorphic extensions to all of CK. Furthermore Z(δ) > 0 for Q0-a.e. δ RK. ∈ RK, we Since Q0 is a Gaussian measure and Z is continuous (even analytic) as a function of δ ∈ CK strictly containing RK. Hence there is a bounded set conclude Z(δ) > 0 for every δ ∈ E ⊂ Eπδ . ✷ r, r]K such that the map δ [φ] = Z ′(δ)/Z(δ) admits an holomorphic extension to [\n5 Exponential DNN Expression Rate\nEπδ Using the holomorphy of the data-to-QoI map [δ [φ]] established in Section 2 (for addi- 7→ tive, centered Gaussian observation noise η in (1)), in this section we prove for this map and deep ReLU NNs an exponential expression rate bound in term of the overall NN size. As a byproduct of the proof, we also show exponential convergence rates for polynomial and rational approximations.\nEπδ The approximation of the data-to-QoI map [δ [φ]] by ReLU NNs will be developed for 7→ observation data δ in compact subsets of RK. We immediately point out that in the (assumed) observation noise model (1), i.e. δ = G(u) + η, the RV δ can take arbitrarily large values with positive probability. This may be due to the unboundedness of the uncertain input u (see for example Section 3.2.3) or due to the unboundedness of the additive noise η, e.g. additive Gaussian noise. However, bounds on the tails of these distributions entail that the RV δ takes values in a compact set with high probability. Speciﬁcally, in the case of a prior measure r, r]K decays with bounded support, the probability of data δ outside of a compact box [ − r2K/(2λmax)), where λmax is the largest double exponentially, i.e., upper bounded by C exp( − eigenvalue of the covariance matrix of the additive Gaussian observation noise and C > 0 is a constant that does not depend on r, cf. [18, Theorem 1]. The approximation of the data-to-QoI map by ReLU NNs will be developed on compact subsets of RK.\nThe structure of the section is as follows: in Section 5.1, we recapitulate notation and deﬁne the architecture for the ReLU DNN approximations to be analyzed. The main reference here is [25], and also [26, 31]. We remark in passing that alternative, more involved architectures could aﬀord better expression rate bounds; we refer to [35] and to the discussion in Section 6 ahead. Next, in Section 5.2 we show exponential convergence of polynomial and rational approximations, with a slightly better result in the latter case. Finally, in Section 5.3, we infer the expression rate bounds, based on holomorphy (δ 7→ on all of CK) of the data-to-QoI map in Bayesian inversion. Z ′(δ) and δ 7→ Z(δ) are holomorphic\n(29)\n14\n5.1 Deﬁnitions and Architecture of Deep ReLU NNs\nWe consider feed-forward deep neural networks (DNNs). These DNNs are obtained as iterated compositions of linear transformations followed by a nonlinearity. This nonlinearity is speciﬁed via the so-called activation function σ : R R of the DNN. The architecture of the DNN → N, numbers Nℓ ∈ N of computation nodes in comprises a ﬁxed number of hidden layers L ∈ , the map f : RN0 RNL+1 is said to be realized by a feedforward neural layer ℓ 0, . . . , L ∈ { → } R it holds for all x = (xi)N0 network, if for certain weights wℓ R, and biases bℓ i,j ∈ j ∈ i=1\nN0 z1 j = σ i=1 w1 i,jxi + b1 j ! , j ∈ { 1, . . . , N1} ,\nand\nNℓ zℓ+1 j = σ i=1 wℓ+1 i,j zℓ i + bℓ+1 j ! , ℓ ∈ { 1, . . . , L − 1 } , j ∈ { 1, . . . , Nℓ+1} ,\nand ﬁnally\nNL NL+1 f (x) = (zL+1 j )NL+1 j=1 = i=1 wL+1 i,j zL i + bL+1 j ! j=1 .\nX\nIn this case n = N0 is the dimension of the NN input, and m = NL+1 is the dimension of the output. Furthermore zℓ j denotes the output of unit j in layer ℓ. The weight wℓ i,j has the interpretation of connecting the ith unit in layer ℓ 1 with the jth unit in layer ℓ. We do not − distinguish between the network (which is deﬁned through σ, the wℓ i,j and bℓ j) and the function f : RN0 RNL+1 it realizes although such distinction is mathematically at times mandatory → (we refer to the discussion in [26, Deﬁnition 2.1]. The number of hidden layers L of a NN is referred to as depth of the DNN. We shall in particular consider DNNs with the so-called ReLU activation σ = σ1 given by x by w and of biases by b. 7→ σ1(x) := max { 0, x } . Let us also denote the tensor of weights\n5.2 Polynomial and Rational Approximation\nWe have seen in the previous sections that the data-to-QoI map with respect to the unnor- malized posterior density, i.e., δ X ′ φ(u) exp( Φ(u; δ))π0(du) can be extended to an entire 7→ − function on CK under the assumption of additive, nondegenerate Gaussian observation noise. R Holomorphy implies fast convergence of Taylor expansions as we recall this in the next theorem. The proof (which is based on standard arguments) is provided in Appendix A.\nN, and assume that f : CK C is holomorphic. Then, for every Theorem 5.1 Let K ∈ → N it holds κ > 1 and every r > 0, there exists Cκ,f,r > 0 such that for every n\nsup f@ 9 f(0) 8 < C exp(—kn). S[—rr] > ! [lgee <n\nNote that {v € N : ||= < n} has cardinality ¢, := (n + 1)X. With respect to the number ¢y, of terms in the Taylor expansion, the error in ) thus decreases exponentially, namely like —n 1/K expl )\n− In the following we show two approximation results for the data-to-QoI map [δ As earlier, for every δ RK we denote 7→ Eπδ [φ]].\nZ ′(δ) := Eπδ [φ]Z(δ) and Z(δ) := ZX ′ exp( − Φ(u; δ))π0(du).\n15\n(30)\n(31)\n(32)\n(33)\n(34)\nIt is classical, that holomorphic (but not necessarily entire) functions can be approximated at an exponential rate with polynomial functions (see for example [24, Theorem 3.5]). As a consequence of Corollary 4.7 we thus have the following statement.\nProposition 5.2 Suppose the setting of Theorem 2.4 and Section 4.2 (i.e. the observation N. Then there exist constants κ > 0 and noise η in (1) is Gaussian). Let r > 0 and K ∈ N there Cr > 0 (depending also on the observation noise covariance Σ) such that for every n ∈ yν : ν such that n exists a polynomial pn ∈ span kℓ∞ ≤ { k }\nEπδ sup [φ] pn(δ) Cr exp( κn). − ≤ − δ∈[−r,r]K\nUsing Th- we can improve this statement using a rational rather than a polynomial approximation (note that, contrary to Proposition [5 K in be arbitrarily large)\nProposition 5.3 Suppose the setting of Theorem 2.4 and Section 4.2 (i.e. the observation N. For every κ > 0 there exists Cr,κ > 0 noise η in (1) is Gaussian). Let r > 0 and K ∈ N such that for all n (depending also on the observation noise covariance Σ) and n0 ∈ n0 ≥\n-1 5 2\'(0) O Z(0) sup E [¢] - > ! 1) > ) < C exp(—kn) S[—r] [g <n [g <n\nProof. Recall that E™ [¢] \'(6)/2 where Z\'() and Z() are defined in By Propo- sition neN and h for every k > 0, there exists a constant C,. > 0 such hat for every\nZ( ) " Z(0) id sup 7\'() > &\\ +|2(5) - > < exp(—kn) se(—rr [llese <n llese <n\nSince Z() > 0 for every € RX | it holds that Zy, := infse_, x Z(8) > 0, where we used that [RX 3§ — Z()] is continuous by Proposition Thus by there exists n € N such that for every n > ng and every € [—r,r\nZmin 2 ≤ Xkνkℓ∞ ≤n ∂ν Z(0) ν! δν .\nThus,\n-1 Z\'(0) 8 (0) ™ [¢] > 1 > llese <n lles <n 1Z\'( 2 <n 92(0) s T [ 92\'(0) s T Iz ( y <n 9Z(0) P | ) Zin Z\n(35)\nwhich follows similarly as (28). The asserted estimate follows now by (35).\n16\n5.3 Deep ReLU Approximation\nThe following result is an improvement of the exponential convergence rate in [25, Theorem 3.7] for the (smaller) class of entire, analytic functions. The proof is similar to the argument in [25], being mainly based on the NN approximation results of [34]. For convenience of the reader, we provide a proof in Appendix B.\nN and assume that f : CK C is holomorphic such that f : RK R. Theorem 5.4 Let K → → ∈ N there exists Then for all κ > 1, r > 0 there exists a constant Cf,r,κ > 0 such that for all n ∈ a ReLU NN ˜fn : [ r, r]K R with\nsup δ∈[−r,r]K | f (δ) − ˜fn(δ) | ≤ Cf,r,κ exp( − κn).\nMoreover, there exists a constant C > 0 which is independent of κ such that depth( ˜fn) C(1 + n log(n)), and size( ˜fn) C(1 + n)K+1 for all n N . ≤\nIn the following we use again the notation\nZ ′(δ) = ZX ′ φ(u) exp( − Φ(u; δ))π0( du), Z(δ) = ZX ′ exp( − Φ(u; δ))π0( du),\nso that Eπδ [φ] = Z ′(δ)/Z(δ). Furthermore, we deﬁne the (ﬁnite, under the made assumptions) constants\nand\nZmin := inf δ∈[−r,r]K Z(δ), Zmax := sup δ∈[−r,r]K Z(δ) Z ′ min := inf δ∈[−r,r]K Z ′(δ), Z ′ max := sup δ∈[−r,r]K Z ′(δ).\nWhile of independent interest, the preceding Theorem 5.4 is a key ingredient in the proof of the following result, which is a principal result of the present paper.\nTheorem 5.5 Suppose the setting of Theorem 2.4 and Section 4.2 (in particular that the ob- servation noise η in (1) is Gaussian). Let r > 0 and K N. Then there holds\n(i) For K = 1, there exist κ > 0 (independent of r, Z and Z ′) and a constant Cκ > 0 N there exists a ReLU NN ˜fn : (depending on κ, r, Z, Z ′, Σ) such that for all n ∈ r, r]K R such that [\nK sup se[—r] [E76] F()] < Cexp — V [0g(Zmax/Zmin) | )\nFurthermore, there exists a constant C > 0 (independent of Z, Z\') such that for every n € N holds\ndepth( ˜fn) size( ˜fn) ≤ ≤ C(1 + n log(n) + log3(n)) , C[1 + n2(log(n) + log( ⌈ log(Zmax/Zmin) ⌉ ))].\np (ii) For K > 1, for every κ > 0 there exists a constant Cκ > 0 (depending on κ, r, Z, Z ′, Σ) N there exists a ReLU NN ˜fn : [ r, r]K R such that such that for all n\nEπδ ˜fn(δ) [φ] sup Cκ exp( κn). − ≤ − δ∈[−r,r]K\nFurthermore, there exists a positive constant C > 0 such that for every n € N holds depth(f, < C(1+n*?log(n)), size(f,) < C(1 4 n)+ .\n(36a)\n(36b)\n(37)\n(38)\n17\nProof. Step 1. We provide the proof for K > 1. Fix κ > 1 (arbitrarily large). Due to the compactness of [ have (cf. (36)) − r, r]K and the continuity (even analyticity) of δ 7→ Z(δ) and δ 7→ Z ′(δ), we\n0 < Zmin, Zmax < ∞ , −∞ < Z ′ min, Z ′ max < ∞ .\nTo approximate Eπδ r, r]K we will combine the following NNs: [ [φ] as a function of δ\n(i) ˜Zn : [ r, r]K εZ, Zmax + εZ]: By Proposition 4.6 and Theorem 5.4 there exists [Zmin − − → a constant Cr,κ, C > 0 and a network ˜Zn such that\nsup δ∈[−r,r]K | Z(δ) − ˜Zn(δ) | ≤ Cr,κ exp( − κn) =: εZ,\nand depth( ˜Zn) C(1 + n log(n)) and size( ˜Zn) C(1 + n)K+1.\n(ii) ˜Z ′ r, r]K [Z ′ εZ′ , Z ′ max + εZ′ ]: By Proposition 4.6 and by Theorem 5.4, there n : [ min − − → exists a constant Cr,κ, C > 0 and a network ˜Z ′ n such that\nsup δ∈[−r,r]K | Z ′(δ) − ˜Z ′ n(δ) | ≤ Cr,κ exp( − κn) =: εZ′ ,\nand depth( ˜Z ′ C(1 + n log(n)) and size( ˜Z ′ C(1 + n)K+1 (without loss of generality n) n) ≤ ≤ we use here the same symbol for the constant Cr,κ as in (i)).\n(iii) ˜dn : [Zmin/2, 2Zmax] [(2Zmax)−1 εd, (Zmin/2)−1 + εd]: The map x 1/x is analytic 7→ − → . Hence, by [25, Theorem 3.7] there exists κ0, C > 0 and a NN ˜dn such that on C 0\n| x−1 − ˜dn(x) | ≤ exp( − κ0n) =: εd ∀ x ∈ [Zmin/2, 2Zmax], with size( ˜dn) C(1 + n)2 and depth( ˜dn) C(1 + n log(n)).\n(iv) ˜mn : [Z ′ min/2, 2Z ′ [(4Zmax)−1, (Zmin/4)−1] R: by [31, Proposition 3.1] (this is a max] × → variation of the original result from [34]), there exists κ0 > 0 and a ReLU NN ˜mn such that\n| xy − ˜mn(x, y) | ≤ exp( − κ0n) ∀ (x, y) ∈ ˜[Z ′ min/2, 2Z ′ max] × [(4Zmax)−1, (Zmin/4)−1]\n(without loss of generality we use here the same symbol for the constant κ0 as in (iii)). Furthermore, there is a constant C > 0 such that for every n and depth( ˜mn) Cn. ∈ N holds size( ˜mn) ≤ Cn\nNow consider\n˜fn(δ) := ˜m⌈n3/2⌉( ˜Z ′ n(δ), ˜d⌈n3/2⌉( ˜Zn(δ))).\nAs a consequence of (39), the terms εZ, ε′ Z and εd tend to 0 as n . Hence there exists → ∞ N (depending on κ, κ0, Z and Z ′), such that for the composition of networks in (40), the n0 ∈ output of each network belongs to the domain of the network it is composed with.\nWe now bound the approximation error. For every δ [ r, r]K and for all n n0\nB (6] = a()] < a (Z,(0), d (Z(8))) = Z(6)d2Z(9))] 2,6) _ 20) + (Z(8)) = Za(8) 1 Z(6) Zn(6) Z()\nwhich implies\n! (@ (6] — fa()] < exp(—ro[n/2]) + exp(ro[n*])| Z,(6)| + n(\n(39a)\n(39b)\n(39c)\n(40)\n(41)\n18\nWe have\n˜Z ′ n(δ) | | ≤ | Z ′(δ) − ˜Z ′ n(δ) | + | Z ′(δ) | ≤ Cr,κ exp( − κn) + Z ′ max.\nHence\nexp( − κ0⌈ n3/2 ⌉ ) + exp( − κ0⌈ n3/2 ⌉ ) | ˜Zn(δ) | ≤ (1 + Cr,κ + Zmax) exp( (1 + Cr,κ + Zmax) exp( − n3/2 κ0⌈ κn), ⌉ )\nn3/2 as long as κn, which is ensured by the conditionn κ0 ≥ ⌈ ⌉ R it holds arbitrary a, b, c, d a/b c/d = and / bd cb ad − | | ∈ − | | | | c d a c , we ﬁnd + b d (κ/κ0)2. Next, using that for ≥ ad cb = b)c c)d + (d (a | − | | − − | ≤\nA ! ) Z4() 7/) + 7 max Zn(6) Z < exp - max = + Zax Zn(6) Z,() ‘min Zmin ‘min Zmin\nIn all\nEπδ | [φ] − ˜fn(δ) | ≤ 1 + Cr,κ + Zmax + Cr,κ ˜Zmax + Z ′ max Zmin ˜Zmin ! exp( − κn),\nprovided that n ˜fn := 0 so that ≥ max { n0, (κ/κ0)2 } . In the complementary case n < max { n0, (κ/κ0)2 } , we set\n| Eπδ [φ] − ˜fn(δ) | ≤ Eπδ supδ∈[−r,r]K [φ] | | n0, (κ/κ0)2 exp( κ max { − } ) exp( − κn).\nHence, with\nCκ := max ( Eπδ supδ∈[−r,r]K [φ] | | n0, (κ/κ0)2 exp( κ max { − } ) , 1 + Cr,κ + Zmax + Cr,κ ˜Zmax + Z ′ max Zmin ˜Zmin )\nEπδ ˜fn(δ) [φ] we have κn) for all n Cκ exp( N.\n− | − | ≤ Next we bound the size of the network. Since K holds for all n N ∈ ≥ 2, there exists a constant such that it\nsize(fn) < C(size(Z) + size(Z) + size(Ms/2)) + ( t /2 F(14 F(1 / < O(+n)\nHere, the positive constant C is independent also of κ. Similarly, one veriﬁes the claimed bound on the depth of ˜fn. This completes the proof for K 2.\n≥ For K = 1, the proof will diﬀer in the approximation of the map [x 1/x]. By Lemma C.1, 7→ there exists a constant C > 0, ¯κ > 0 (C, κ are independent of Zmin, Zmax) such that for every n N there exists a ReLU NN ˜pn such that\nsup 7 = pu()] €[Zmin/2,2Zmax] Kn = < 1(Z Zi)| Zn V108 Z Zi)] Rn Cexp V [0g(Zmax/Zmin) | )\n)\n(42)\n(43)\n19\nMoreover, there exists a constant ¯C > 0 such that depth(˜pn) such that size(˜pn) ¯C[1 + n2(log(n) + log( log(Zmax/Zmin) ¯C(1 + n log(n) + log3(n)) and ≤ ))] . In this case we consider\n˜fn(δ) := ˜mn( ˜Z ′ p n(δ), ˜pn( ˜Zn(δ))).\nBy a version of (41) (obtained by replacing ˜d⌈n3/2⌉ by ˜pn and ˜m⌈n3/2⌉ by ˜mn) and (42), there exists a constant C (independent of n) such that\n| Eπδ [φ] − ˜fn(δ) | ≤ C exp − min κ, κ0, ¯κ n { } log(Zmax/Zmin) !\nFinally, we bound the size of the network. There exists a constant C > 0 such that it holds p size( ˜fn) C(size( ˜Zn) + size( ˜Z ′ n) + size( ˜m⌈n⌉) + size(˜p⌈n⌉))\nt L n 2 t I t L 2 t I t L 2 t I 1 <1 (1 n) (1 n) g log(v/[10g(Zax/Zmin < C(1+ n*(log(n) + log(/[10g(Zmax/Zmin)])))\nSimilarly, one veriﬁes the claimed bound on the depth of ˜fn. p\nThe appearance of the exponent 3/2 in the depth of ReLU NN in Theorem 5.5 (ii) is an artifact of the proof technique. In (40), n3/2 may be replaced by nϕ(n) for any strictly increasing function ϕ : [1, ) ) which tends to inﬁnity as n [1, . Possible choices ∞ ∞ → → ∞ 1 or ϕ(n) = log(n) + 1 (in Theorem 5.5 (ii) ϕ(n) = n1/2 include ϕ(n) = nε for 0 < ε ≪ was used). This would result in depth( ˜f ) C(1 + nϕ(n) log(n)), but would also result in a ≤ potentially larger constant Cκ with (κ/κ0)2 replaced by ϕ−1(κ/κ0) in (43).\n6 Conclusions and Extensions\nWe established the holomorphy of the data-to-(Bayesian) prediction (aka. “data-to-QoI”) map for a ﬁnite-dimensional quantity of interest in PDE-constrained Bayesian inverse problems. It is applicable to general well-posed PDEs with uncertain input from function spaces, for observation data δ subject to additive Gaussian observation noise η. Based on the holomorphy of this map, we inferred exponential bounds on the expression rate of deep ReLU NNs for these maps.\nWe analyzed here in detail only the expression by deep ReLU NNs. As is well-known, this implies also exponential expression rates for DNNs with more regular activation functions (see, e.g. the discussion in [31, Section 3.3]).\nWe also showed that for more general, non-Gaussian observation noise models, the BIP is still well-posed but holomorphy of the data-to-QoI map can, in general, not be expected. In the particular case of ﬁnite-dimensional uncertain input and ﬁnite-dimensional observables, and for observation noise with Lipschitz continuous density, we showed that the data-to-QoI map is likewise Lipschitz continuous. In this case, exponential convergence rates can still be realized by DNNs with more elaborate architectures. Let us give here only one illustrative result indicating possible gains aﬀorded by admitting DNN architectures with activation functions which are more general than ReLU.\nRK in Proposition 6.1 In the setting of Theorem 2.1, and for additive observation noise η ∈ Lip(RK), for every (9) with law admitting a density ρ with respect to λK which is Lipschitz, ρ ∈ N, there exists a DNN 0 < r < , there exist constants c1,K, C > 0 such that, for every W ∞ ∈ ˜fW,r : [ r, r]K R with both ReLU and sinusoidal activation functions and with at most W → − nonzero weights such that\nsup ] - ()] < Cexp ( W) S[—rr\n(44)\n20\nEπδ Proof. The assumptions imply with Proposition 4.2 that the data-to-QoI map [δ 7→ r, r]K). The assertion follows from [35, Theorem 5.1] by scaling and translation from the Lip([ − r, r]K. unit box [0, 1]K (which was studied in [35]) to [ − [φ]] ∈ ✷\nWe note that the proof of [35, Theorem 5.1] allows to infer information about the architecture of the DNN ˜f which appears in Proposition 6.1. In particular, the constant c1,K will, in general, be upper bounded by c/K, where K is the (ﬁnite) dimension of the data space and and c > 0 a generic constant independent of K, cf. [35, p. 7]. Furthermore, analyticity of the data-to-QoI R is not required for Proposition 6.1 to hold. However, the very accurate and map f : Br(0) → stable evaluation of the sinusoidal activations is crucial for the expression rate in Proposition 6.1 to materialize. As all DNNs are operated in ﬁnite ﬂoat point precision, often with rather short mantissas (e.g. when rather strong quantization of NN weights is employed), the scope of Proposition 6.1 could be limited, in practice. Nevertheless, the result does cover, for example, the ﬁnite dimensional setting in Theorem 2.1, where the law Q0 admits merely a Lipschitz density w.r. to the Lebesgue measure λK on the space of observation data δ.\nRemark 6.2 We remark that improved approximation rates when admitting a wider range of activation functions in an otherwise ﬁxed DNN architecture is not surprising. See, e.g., [27, Theorem 7.1]. In practice, however, issues of DNN stability in ﬁnite precision arithmetic, in particular under quantization, of DNNs with these rather intricate activation functions arise.\nIn the present paper, we analyzed the rates of expressive power of deep ReLU NN surrogates for data-to-QoI maps for Bayesian inversion of well-posed PDEs subject to uncertain input data from function spaces. The present analysis substantiates recent numerical evidence (e.g. in [22] and the references there) that even for rather complex PDE models of physical systems, with possible rough/ singular solutions, rather small DNNs can provide highly accurate surrogates for input-to-observable maps in forward UQ and for data-to-QoI maps in Bayesian PDE inversion. The mathematical convergence rate bounds in the present paper are a stepping stone to the analysis of the generalization error, and to the mathematical analysis and the design of multi- level training algorithms, which we shall provide in [11].\nThe constants in the expressive power estimates depend on the covariance Σ of the additive Gaussian observation noise and on the dimension K of the data space. A detailed analysis of the eﬀect when Σ 0 on the expressive power of ReLU NNs to approximate data-to-QoI maps → will be developed elsewhere.\nA Proof of Theorem 5.1\nFix γ > 1. We provide the standard bound on the Taylor coeﬃcients and assume ﬁrst that K = 1, i.e. f : C C. Cauchy’s integral formula gives for every j N0\n(0) 1 SUP¢ ~ d < ! | 2mi y 7\nwhere i = /—1 € C denotes a complex root of —1. Here we used that f is holomorphic on C which in particular contains the ball of radius around 0.\nIf K > 1, we repeatedly apply the estimate (45) in each variable to obtain for every ν NK 0\nsup, f(&1 - €)] .\nFor v € N denote t,, := ¥ f(0)/v!. Since f is holomorphic it admits a convergent multivariate Taylor expansion\nf (δ) = Xν∈Nk 0 tν δν ∀ δ ∈ CK.\n(45)\n(46)\n(47)\n21\nWe point out that with ˜Cf,γ := sup|ξ|=γ | fore f (ξ1, . . . , ξK) | , (46) shows | tν | ≤ ˜Cf,γγ−|ν| and there-\nK ˜Cf,γ γ−|ν| = ˜Cf,γ γ−j tν . < ∞ | | ≤ Xj∈N0 i=1 Xν∈NK Xν∈NK Y 0 0\nThus the order of summation in (47) does not matter, as the series is absolutely convergent for all δ < γ.\n| | Now let us estimate the error of the truncated Taylor expansion. Fix n γ > r. Then ∈ N, r > 0 and\nl sup " f( 0) s€—r] ODY v! = > [e <n veN\\{0 n}K\nNext,\nsup 9" f(0) Se[—rr HOESY v! 5 [llee <n 2l e =n veNF < 5y L ) e NE =}l Y veNK ~ < 5 5y L 5y ( r ( r veN l IN L (49) 1+n)\nwhere ˜Cf,γ,r := ˜Cf,γ ν∈Nk 0 (γ/r)−|ν| is ﬁnite by the same argument as in (48). For a given κ > 1 we can now choose γ > max P κ, r so large that\n{\n}\nγ −n (1 + n)K N. κn) n exp( r ∀ − ∈ ≤\nThe statement now follows by and — ), which completes the proof of h\nB Proof of Theorem 5.4\nWe prove the theorem for the case r = 1. The general case r > 0 follows by setting fr(δ) := f (rδ), and then approximating fr with a neural network ˜fr,n on [ 1, 1]K. Then ˜fn(δ) := − ˜fr,n(δ/r) is a suitable network approximating f , since for every δ r, r]K it holds f (δ) [ − ∈ ˜fn(δ) = fr(δ/r) ˜fr,n(δ/r). Furthermore we assume for now K 2, and discuss the case K = 1 − ≥ in the last step. −\n˜ N and every γ > 0 there exists a network According to [31, Proposition 3.3], for every m ∈ 2, 2]m R of size C(1 + m log(m/γ)) and depth C(1 + log(m) log(m/γ)) such that m,γ : [\n−\n→\nm ~ Ti,m)| < [ <2 sup - my j=1\nStep 1. Using (51), for every ¢ € Ny and v > 0 we define p; , : [1,1] = R by p; ,() := (z,...,x). Then, there exists a constant C\' > 0 such that for every i € Ny and v > 0 holds size(pi) < C(1+ilog(i/v)) and\nsup |x|≤1 | pi,γ(x) − xi | ≤ γ.\n22\n(48)\n(50)\n(51)\nN, κ > 1 and set g(δ) := Fix n ∈ bound (33), i.e. sup|δ|≤r | g(δ) f (δ) − approximation ˜g to g. We deﬁne | ≤ kνkℓ∞ ≤n tν δν , where tν = ∂ν f (0)/ν. We have the Cf,r,κ exp( P − κn). We now construct a neural network\n˜g(δ) := Xkνkℓ∞ ≤n tν ˜ Y K,γ (pν1,γ(δ1), . . . , pνK ,γ(δK)),\nand ﬁx here and throughout the rest of this proof\nγ := exp( κn) 1.\nSince each pνj ,γ(δj) is the realization of a ReLU network, also ˜g is the realization of ReLU network.\nStep 2. We bound the error supδ∈[−1,1]K holds | by (51) K j=1 pνj ,γ(δj) | ≤ 2K since | pνj ,γ(δj) | − g(δ) − δνj j | ≤ ˜g(δ) | γ ≤ . First, note that for any δ 1 for all j. Thus for any δ ∈ ∈ [ [ − − 1, 1]K 1, 1]K\n~ lg( < > t]7 - 11 (01 Puic0x)) <n K IN > t N v+ — lg <n j=1 IN t N > v |TT 67| 167 = p (6)] Pu; (85) v <n i=1 [j=1 j=i+1 A > t|(y + K259) < (1+ K2) 3 |t v <n veN\nSince f is an entire function, it holds tν . This can be shown using bounds < ν∈NK | ∞ 0 | on tν as for instance provided in the proof of Theorem 5.1 (cf. (46)). Hence we have shown P ˜g(δ) g(δ) Cγ = C exp( κn). supδ∈[−1,1]K\n| − | ≤ − Step 3. We estimate the size and depth of (one realization of) the network ˜g.\nWhereas it is easy to see that ˜g is the realization of a neural network, there are diﬀerent ways to construct such a network (i.e. having diﬀerent network architecture). In order to provide one, we ﬁrst consider a network F : RK RKn which takes as input δ RK and computes in → ∈ parallel the Kn dimensional output (pi,γ(δj))i=1,...,n;j=1,...,K. For each i = 1, . . . , n, as stated above,\nsize(pi,γ) ≤ C(1 + i log(i/γ)), depth(pi,γ) ≤ C(1 + log(i) log(i/γ)).\nConcatenating pi,γ with O(log(n) log(n/γ) log(i) log(i/γ)) times the one layer identity network − x = σ(x) 0, x is the ReLU), we ﬁnd that F can be realized by a x) (where σ(x) = max σ( } { − − network of size CKn(1 + n log(n/γ)) and depth C(1 + log(n) log(n/γ)). Using γ = exp( − we get κn)\ndepth(F ) C(1 + n3), size(F ) CKn(1 + n log(n/γ)) ≤ ≤ C(1 + n log(n)), C(1 + log(n) log(n/γ))\nfor a constant C depending on K and κ.\nNext, we concatenate the NN F with a NN G expressing ˜g in (52) given the output (pi,γ(δj))i=1,...,n;j=1,...,K of F . By (52), the size of this second part of the network can be bounded by (n + 1)Ksize( ˜ K,γ) + (n + 1)K, where the last (n + 1)K stems from the summation\n(52)\n(53)\n23\nν over the set { γ = exp( κn)) − ∈ NK 0 : k ν kℓ∞ ≤ n } in (52), which has cardinality (1 + n)K. Hence (since\nsize(G) ≤ (n + 1)Ksize( ˜ K,γ ) + (n + 1)K ≤ C(1 + K log(K/γ))(n + 1)K ≤ C(n + 1)K+1 (54)\nfor constants C, C depending on K and κ. For the depth of G we obtain 1 + depth( ˜ e C(1 + log(K) log(K/γ)) C(n + 1) as an upper bound. In total Q K,γ) ≤\n≤\ne\nsize(˜g) = size(G e ◦ F ) ≤ C(n + 1)K+1 + n3 ≤ C(n + 1)K+1\nF : RK is an upper bound of the complete network G ◦ K 2). Together with the previous two steps we arrive at → R realizing ˜g (here we used that\n(55)\nsup δ∈[−1,1]K | f (δ) − ˜g(δ) | ≤ sup |δ|≤1 | f (δ) − g(δ) | + sup |δ|≤1 | g(δ) − ˜g(δ) | ≤ C exp( − κn) .\nFinally, depth(˜g) 1 + depth(F ) + depth(G) C(1 + n log(n)).\n≤ ≤ N, κ > 0 and for δ Step 4. We show the theorem in case K = 1. Fix again n ∈ ∈ n j=0 tjδj where tj = g(j)(0)/j!. By Theorem 5.1 there exists Cκ,f,1 such that set g(δ) = [ − 1, 1] sup|δ|≤1 | f (δ) g(δ) Cκ,f,1 exp( κn). − | ≤ −\nNow we approximate g by a neural network ˜fn up to the error γ := exp( κn). First, since − C is an entire function, we have C0 := f : C tj| < . By [24, Proposition 4.2], j∈N0 | → ∞ ˜fn(δ) there exists a neural network ˜fn : [ γ, size( ˜fn) R such that sup|δ|≤1 | g(δ) 1, 1] P − − | ≤ → ≤ C(1 + n log(C0/γ) + n log(n)) and depth( ˜fn) C((1 + log(n)) log(C0/δ) + log(n)3) with C ≤ independent of n and γ. With γ = exp( κn) there exists a constant C > 0 such that for every − C(1 + n2) and depth( ˜fn) N holds size( ˜fn) n C(1 + n log(n)). This completes the proof ∈ ≤ ≤ ✷ of Theorem 5.4.\nC ReLU Neural Network Approximation of x 1/x\nThe approximation of rational functions by ReLU NNs is studied in [33]. In the particular approximation of the map [x 1/x], we apply a diﬀerent proof technique. We ﬁrst construct a 7→ sequence of certain variable degree, free-knot continuous splines with exponential convergence rate bounds. We re-express these spline approximations subsequently by a corresponding se- quence of deep ReLU NNs, with exponential error bounds. The following lemma should be compared to [33, Lemma 3.5] in the approximation of the mapping [x 1/x] on an interval 7→ 1/2, where possibly a is close to zero. Speciﬁcally, to achieve an accuracy [a, 1] for 0 < a ≤ 4 0 < ε < 1 required in [33, Lemma 3.5]1 a ReLU NN with size O( 3). In the log(1/ε) log(1/a) ⌈ ⌉ ⌉ ⌈ 2 + 2]) log(1/a) [ log(1/ε) log(1/a) following lemma, we construct a ReLU NN with size O( ⌈ ⌉ ⌉ ⌈ ⌈ ⌉ that achieves an accuracy 0 < ε < 1.\nLemma C.1 Let 0 < a < b < . There exists κ > 0 and constants C, C1 > 0 that are ∞ N, there exists a ReLU NN ˜fn such that independent of a, b such that for every n\nK sup @) T < lot/a)] a z€[a,b] ( V/[log(b/a)] |\n⌈\n⌉\nFurthermore, it holds that depth(f,) < Ci(1 + nlog(n) + log(n)) and ( < l + n?(log(n) + log(/Tlog(b/a)))]\n1The words “size” and “depth” in the statement of [33, Lemma 3.5] should be interchanged. p\n24\nProof: The proof is structured in two steps.\nIn the ﬁrst step, we construct a polynomial spline approximation and in the second step, we apply approximation error bounds of ReLU NNs for splines, see [24].\nL i=1 Ji, where Ji = [a2i−1, a2i], i = The interval J := [a, b] is decomposed into J = 1, and JL = [a2L−1, b] for L = . Note that dist(0, Ji)/ log2(b/a) Ji| 1, . . . , L = 1, i = − ⌈ | ⌉ S 1. JL| ≥ 1, and dist(0, JL)/ 1, . . . , L − |\n− | 1, 1] be the unique aﬃne bijection 1 be arbitrary. Let Fi : Ji 7→ Let i = 1, . . . , L [ − − 3 and F −1 with aﬃne inverse F −1 Ji. Speciﬁcally, Fi(x) = x/(a2i−2) (x) = : [ 1, 1] i i − − 7→ 2i−2). Note that the zero of F −1 a(2i−2x + 3 3 and neither depends on i nor on a. is at x = i · − z+z−1 1/F −1 C, E̺ = ̺ Thus, the map [z (z)] is holomorphic on the ellipse : z for z i 2 { ∈ } | ≤ 7→ | ̺+̺−1 F −1 a2i−2(3 (1, ̺0) with ̺0 := 3 + 2√2. It is easy to see that (z) ) for every any ̺ i 2 − ∈ | ≥ | ∈ E̺. By [25, Theorem 3.5] for every 0 < β < log(̺) < log(3) there exists a constant C > 0 z N, that neither depends on a, b nor on i such that for every p\n−\n|\n1 sup - By(x) a2i2(3 ) Bp) 1, L-1, ze[-1,1 (@)\nwhere P! is the pth order Legendre expansion of the mapping [z + 1/(F ())]; we also used that o > (0 + 07 !)/2. Denote by P! the Langrange interpolant of the mapping [[1,1] > = (x))] using Gauss-Lobatto interpolation points. In particular, P! 1) =1/( ( ) and P (1) = 1/(F 1(1)). Let us denote this Langrange interpolation operator by I,. By [l q (1.14)], h Lebesgue constant of the first p+1 Gauss-Lobatto points in [1, 1] satisfies the bound (suboptimal, see [32], but sufficient for our purposes)\n∀ p ≥ 1, ∀ f ∈ C 0([ − 1, 1]) : k Ipf kC0([−1,1]) ≤ 5(p + 1)2 log(p + 1) k f kC0([−1,1]).\nSince Ip(P i p) = P i p, the estimate of the Lebesgue constant in (57) implies with (56) for every i = 1, . . . , L 1\ni — P z)| < sup 1 () P(z)| + sup - P(x) z€[-1,1] x) z[-1,1] z[-1,1] 5(p +2)2log(p + C exp( Bp a2=2(3 )\nR by The case i = L follows similarly. Deﬁne the spline interpolant Ip,L(x) := Ip,L : [a, b] 7→ ¯P i Ip,L is continuous and restricted to Ji a polynomial p(Fi(x)), x Ji, i = 1, . . . , L. Note that ∈ of degree p and has in total (p + 1)L degrees of freedom. By the estimate (58), for every 0 < β < log(3) there exists a constant C > 0 that neither depends on a nor on b such that for every p N\nsup —I(x) < l sup — P P (] < = exp(~p) z€[a,b] i=1,. Lel-1] F (@)\nIn the second step, we approximate the continuous piecewise polynomial Z,, 1, by a ReLU NN We shall bound the W ([a, b])-norm of Z,, 1. By the Markov inequality, cf. [L3] Equation (1.16) and p. 736], for every polynomial g of degree p\n(56)\n(57)\n(58)\n(59)\nsup x∈Ji | g′(x) | ≤ 4ep2 Ji| | sup x∈Ji | g(x) | .\n25\nThus, by the upper bound on the Lebesgue constant of the Gauss–Lobatto points in (57)\nL kIp,LkW 1,1([a,b]) ≤ ≤ i=1 X | 4ep2 Ji| sup x∈Ji | ( L i=1 sup x∈Ji | Ip,L)′(x) ¯P i p(x) | ≤ | 20ep2(p + 2)2 log(p + 1) ⌈ log(b/a) a ⌉\nContinuous polynomial splines may be approximated by ReLU NNs due to [24, Proposi- tion 5.1]. Speciﬁcally, by [24, Proposition 5.1] and by (60), there exists κ > 0 and constants, C, C1 > 0 (κ, C, C1 are independent of a, b) such that for every n ˜fn such that ∈ N, there exists a ReLU NN\nkIp,L − ˜fnkW 1,1([a,b]) ≤ C ⌈ log(b/a) a ⌉ exp − ⌈ κn log(b/a) ⌉ ! .\nMoreover, depth( ˜fn) C1(1 + p log(p) + n log(p) + log3(p)) and size( ˜fn) p C1( log(b/a) ≤ ≤ ⌈ log(b/a) p log(p)). We recall that by the Sobolev embedding, cf. [3, Theorems 8.6 and 8.8], n ⌉ ⌈ W 1,1([a, b]), there exists a constant C > 0 (independent of a, b) such that for every g ⌉ p2 +\n< € (14 52 ) lbws\nThe asserted error estimate of this lemma follows by (59) and by (61) and by (62) using the triangle inequality, where we choose p = ⌈ C1(1 + n log(n) + log3(n)) and size( ˜fn) depth( ˜fn) ≤ κn/(β ≤ log(b/a) ) . Thus, by this choice, ⌈ ⌉ ⌉ C1[1 + n2(log(n) + log( log(b/a) ))]. p ⌉ ⌈\n(60)\n(61)\n(62)\n✷\nReferences\n[1] K. Abraham and R. Nickl. On statistical Calder´on problems. arXiv e-prints, 2019. arXiv:1906.03486.\n[2] A. Bonito, R. A. DeVore, and R. H. Nochetto. Adaptive ﬁnite element methods for elliptic problems with discontinuous coeﬃcients. SIAM J. Numer. Anal., 51(6):3106–3134, 2013.\n[3] H. Brezis. Functional analysis, Sobolev spaces and partial diﬀerential equations. Universi- text. Springer, New York, 2011.\n[4] M. A. Chkifa. On the Lebesgue constant of Leja sequences for the complex unit disk and of their real projection. J. Approx. Theory, 166:176–200, 2013.\n[5] M. Dashti, S. Harris, and A. Stuart. Besov priors for Bayesian inverse problems. Inverse Probl. Imaging, 6(2):183–200, 2012.\n[6] M. Dashti and A. M. Stuart. The Bayesian approach to inverse problems. In Handbook of uncertainty quantiﬁcation. Vol. 1, 2, 3, pages 311–428. Springer, Cham, 2017.\n[7] J. Dick, R. N. Gantner, Q. T. L. Gia, and C. Schwab. Multilevel higher-order quasi-Monte Carlo Bayesian estimation. Math. Models Methods Appl. Sci., 27(5):953–995, 2017.\n[8] J. Dick, R. N. Gantner, Q. T. L. Gia, and C. Schwab. Higher order quasi-Monte Carlo integration for Bayesian PDE inversion. Comput. Math. Appl., 77(1):144–172, 2019.\n26\n[9] J. Dick, F. Y. Kuo, Q. T. LeGia, and C. Schwab. Multilevel higher order QMC Petrov- Galerkin discretization for aﬃne parametric operator equations. SIAM J. Numer. Anal., 54(4):2541–2568, 2016.\n[10] L. Herrmann, M. Keller, and C. Schwab. Quasi-Monte Carlo Bayesian estimation under Besov priors in elliptic inverse problems. Technical Report 2019-41, Seminar for Applied Mathematics, ETH Z¨urich, Switzerland, 2019.\n[11] L. Herrmann, C. Schwab, and J. Zech. Deep ReLU neural network generalization rates for data-to-QoI maps in Bayesian PDE inversion. Technical report, 2019. in preparation.\n[12] L. Herrmann and Ch. Schwab. Multilevel quasi-Monte Carlo uncertainty quantiﬁcation for advection-diﬀusion-reaction. Technical Report 2019-06, Seminar for Applied Mathematics, ETH Z¨urich, 2019. To appear in Monte Carlo and Quasi-Monte Carlo Methods – MCQMC, Rennes, France, July 2018.\n[13] E. Hille, G. Szeg¨o, and J. D. Tamarkin. On some generalizations of a theorem of A. Markoﬀ. Duke Math. J., 3(4):729–739, 1937.\n[14] V. H. Hoang, J. H. Quek, and C. Schwab. Analysis of multilevel MCMC-FEM for Bayesian inversion of log-normal diﬀusions. Technical Report 2019-05 (revised), Seminar for Applied Mathematics, ETH Z¨urich, 2019. (To appear in Inverse Problems (2020)).\n[15] V. H. Hoang, C. Schwab, and A. Stuart. Complexity analysis of accelerated MCMC methods for Bayesian inversion. Inverse Problems, 29(8):085010, 37, 2013.\n[16] H. Holden and N. H. Risebro. Front tracking for hyperbolic conservation laws, volume 152 of Applied Mathematical Sciences. Springer-Verlag, New York, 2002.\n[17] L. H¨ormander. An introduction to complex analysis in several variables, volume 7 of North- Holland Mathematical Library. North-Holland Publishing Co., Amsterdam, third edition, 1990.\n[18] J. H¨usler, R. Y. Liu, and K. Singh. A formula for the tail probability of a multivariate normal distribution and its applications. J. Multivariate Anal., 82(2):422–430, 2002.\n[19] M. A. Iglesias, Y. Lu, and A. Stuart. A Bayesian level set method for geometric inverse problems. Interfaces Free Bound., 18(2):181–217, 2016.\n[20] D. Jerison and C. E. Kenig. The inhomogeneous Dirichlet problem in Lipschitz domains. J. Funct. Anal., 130(1):161–219, 1995.\n[21] M. Lassas, E. Saksman, and S. Siltanen. Discretization-invariant Bayesian inversion and Besov space priors. Inverse Probl. Imaging, 3(1):87–122, 2009.\n[22] K. Lye, S. Mishra, and R. Molinaro. A multi-level procedure for enhancing accuracy of machine learning algorithms. Technical Report 2019-54, Seminar for Applied Mathematics, ETH Z¨urich, Switzerland, 2019.\n[23] L. Mattner. Complex diﬀerentiation under the integral. Nieuw Arch. Wiskd. (5), 2(1):32– 35, 2001.\n[24] J. A. A. Opschoor, P. C. Petersen, and C. Schwab. Deep ReLU networks and high-order ﬁnite element methods. Technical Report 2019-07, Seminar for Applied Mathematics, ETH Z¨urich, Switzerland, 2019. to appear in Analysis and Applications.\n27\n[25] J. A. A. Opschoor, C. Schwab, and J. Zech. Exponential ReLU DNN expression of holomor- phic maps in high dimension. Technical Report 2019-35, Seminar for Applied Mathematics, ETH Z¨urich, 2019.\n[26] P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep ReLU neural networks. Neural Netw., 108:296 – 330, 2018.\n[27] A. Pinkus. Approximation theory of the MLP model in neural networks. In Acta numerica, 1999, volume 8 of Acta Numer., pages 143–195. Cambridge Univ. Press, Cambridge, 1999.\n[28] W. Rudin. Functional analysis. International Series in Pure and Applied Mathematics. McGraw-Hill, Inc., New York, second edition, 1991.\n[29] C. Schillings and C. Schwab. Sparse, adaptive Smolyak quadratures for Bayesian inverse problems. Inverse Problems, 29(6), 2013.\n[30] C. Schwab and S. Tokareva. High order approximation of probabilistic shock proﬁles in hyperbolic conservation laws with uncertain initial data. ESAIM Math. Model. Numer. Anal., 47(3):807–835, 2013.\n[31] C. Schwab and J. Zech. Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in UQ. Analysis and Applications, Singapore, 17(1):19–55, 2019.\n[32] B. S¨undermann. Lebesgue constants in Lagrangian interpolation at the Fekete points. Mitt. Math. Ges. Hamburg, 11(2):204–211, 1983.\n[33] M. Telgarsky. Neural networks and rational functions. In D. Precup and Y. W. Teh, edi- tors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3387–3393. PMLR, International Con- vention Centre, Sydney, Australia, 06–11 Aug 2017.\n[34] D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Netw., 94:103–114, 2017.\n[35] D. Yarotsky and A. Zhevnerchuk. The phase diagram of approximation rates for deep neural networks, 2019. arXiv:1906.09477.\n[36] K. Yosida. Functional analysis. Classics in Mathematics. Springer-Verlag, Berlin, 1995. Reprint of the sixth (1980) edition.\n[37] J. Zech, D. Dung, and C. Schwab. Multilevel approximation of parametric and stochastic PDEs. Math. Models Methods Appl. Sci., 29(9):1753–1817, 2019.\n28', 'Document 9: Fake Deposition Summary.docx\nJohn Smith Deposition Summary\nCounsel for Plaintiff:\xa0Jane Doe\nCounsel for Defendant:\xa0John Doe\n\nDate of Testimony:\xa003/15/2024\n<table><tr><td>Page/Line</td><td>Examiner</td><td>Summary of Testimony</td><td>Relevance</td></tr><tr><td>3/12-15</td><td>Jane Doe</td><td>Plaintiff is a 47-year-old construction worker with 25 years of experience.</td><td>Background of witness</td></tr><tr><td>5/20-25</td><td>Jane Doe</td><td>Plaintiff had no prior ankle problems before the accident.</td><td>Background of witness</td></tr><tr><td>7/8-12</td><td>Jane Doe</td><td>Plaintiff describes climbing a ladder to get tools when the ladder slipped and he fell.</td><td>Cause and circumstances of incident</td></tr><tr><td>9/15-20</td><td>Jane Doe</td><td>Plaintiff believes the ladder slipped because it was not secured at the top.</td><td>Cause and circumstances of incident</td></tr><tr><td>11/2-5</td><td>Jane Doe</td><td>Plaintiff admits he did not inspect the ladder before climbing it, assuming it was safe.</td><td>Perceptions of fault; potential for contributory negligence</td></tr><tr><td>12/18-22</td><td>John Doe</td><td>Plaintiff broke his left ankle in three places as a result of the fall.</td><td>Damages</td></tr><tr><td>15/6-10</td><td>Jane Doe</td><td>Plaintiff is still undergoing physical therapy and has not been able to return to work since the accident.</td><td>Damages</td></tr><tr><td>16/24-28</td><td>Jane Doe</td><td>Plaintiff is in constant pain and unable to perform daily activities as he used to.</td><td>Damages</td></tr><tr><td>18/11-15</td><td>Jane Doe</td><td>Plaintiff admits to doing light work on a side job while on disability.</td><td>Credibility; potential impact on damages</td></tr><tr><td>20/5-8</td><td>John Doe</td><td>Plaintiff acknowledges a discrepancy between his earlier statement and the incident report regarding ladder inspection, admitting he was mistaken in his testimony.</td><td>Credibility; potential impact on perceptions of fault</td></tr><tr><td>25/19-23</td><td>John Doe</td><td>Plaintiff&#x27;s doctor expects a full recovery but notes it will take time.</td><td>Prognosis</td></tr><tr><td>30/12-16</td><td>Jane Doe</td><td>Plaintiff is hopeful for no long-term complications but acknowledges the possibility of risks.</td><td>Prognosis</td></tr></table>', 'Document 10: Fake Earnings Records Summary.docx\nClaimant’s Earnings Summary\n<table><tr><td>Year</td><td>Source Documents</td><td>Annual Wages</td><td>Health Benefits Value</td><td>Dental Benefits Value</td><td>Vision Benefits Value</td><td>Pension Contribution</td><td>Total Compensation</td></tr><tr><td>2023</td><td>W-2, Union Benefits Summary</td><td>$55,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$64,650</td></tr><tr><td>2022</td><td>W-2, Union Benefits Summary</td><td>$53,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$62,650</td></tr><tr><td>2021</td><td>W-2, Union Benefits Summary</td><td>$50,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$59,650</td></tr><tr><td>2020</td><td>W-2, Union Benefits Summary</td><td>$48,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$57,650</td></tr><tr><td>2019</td><td>W-2, Union Benefits Summary</td><td>$47,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$56,650</td></tr></table>', 'Document 11: Fake Discovery Requests.docx\nInterrogatories to Plaintiff\nDEFINITIONS AND INSTRUCTIONS\nThe term "incident" refers to the trip and fall accident that occurred on January 23, 2024, at the Sierra County Fair.\nThe term "you" or "your" refers to the plaintiff, John Doe.\nThe term "document" refers to all written, printed, recorded, or graphic material, however produced or reproduced, including electronic records and emails.\nINTERROGATORIES\nGeneral Information\nState your full name, date of birth, current address, and any other names you have used or been known by.\nIdentify all addresses where you have resided for the past ten (10) years.\nCircumstances of the Incident\nDescribe in detail the events leading up to, during, and immediately following the incident on January 23, 2024.\nIdentify all persons who witnessed the incident or have knowledge of the circumstances surrounding the incident, including their names, addresses, and relationship to you.\nState the specific location at the Sierra County Fair where the incident occurred, and describe the conditions of the area at the time of the incident.\nCause of the Incident\nDescribe any obstacles, hazards, or other conditions that you believe caused or contributed to the incident.\nState whether you consumed any alcohol, drugs, or medication within 24 hours prior to the incident. If so, identify the substances and the amounts consumed.\nAftermath of the Incident\nDescribe all injuries you sustained as a result of the incident, specifying the nature and extent of each injury.\nIdentify all persons who provided you with medical care or treatment for injuries related to the incident, including their names, addresses, and dates of treatment.\nDescribe all medical treatment you have received for injuries related to the incident, including surgeries, medications, physical therapy, and any other forms of treatment.\nPrior Medical Conditions\nIdentify all medical conditions, injuries, or illnesses you have had in the past ten (10) years, including the names and addresses of the treating physicians or medical providers.\nState whether you have ever suffered any injury to the same body parts involved in the incident. If so, describe the circumstances and treatment received.\nMedical Care in Connection to the Incident\nProvide the names and addresses of all hospitals, clinics, or medical facilities where you have received treatment for injuries related to the incident.\nList all medications prescribed to you as a result of the injuries from the incident, including the name, dosage, and prescribing physician.\nIdentify any medical tests, examinations, or diagnostic procedures you have undergone in connection with the injuries sustained in the incident.\nPrior Earnings through Present\nDescribe your employment history for the past five (5) years, including the names and addresses of employers, job titles, duties, and dates of employment.\nState your average monthly earnings for each year from 2019 to the present, including bonuses, commissions, and other forms of compensation.\nDescribe any loss of earnings or earning capacity you claim to have suffered as a result of the incident.\nCurrent Medical Status\nDescribe your current medical condition, including any ongoing symptoms or limitations resulting from the injuries sustained in the incident.\nIdentify all healthcare providers you are currently seeing for the injuries related to the incident, including their names, addresses, and the nature of the treatment.\nDescribe any future medical treatment you anticipate needing as a result of the injuries sustained in the incident.\nVocational Training and Abilities\nDescribe any vocational training, education, or certifications you have received.\nState whether you have participated in any vocational rehabilitation or job retraining programs since the incident. If so, provide details of the programs.\nImpact of the Incident\nDescribe any limitations or restrictions on your daily activities as a result of the injuries sustained in the incident.\nIdentify all claims for damages you are making as a result of the incident, including general damages, special damages, and any other forms of relief sought.\nThese interrogatories are designed to cover a wide range of relevant topics, from the specifics of the incident and resulting injuries to the plaintiff’s medical history, current condition, and financial impact.\n4o\nCreate 45 requests for production of documents that track with the interrogatories and other relevant topics\nChatGPT\nCertainly! Here are 45 requests for production of documents that align with the interrogatories and other relevant topics in the injury litigation case:\nRequests for Production of Documents\nGeneral Information\nAll documents identifying your full name, date of birth, current address, and any other names you have used or been known by.\nAll documents showing your addresses for the past ten (10) years.\nCircumstances of the Incident\nAll documents, including photographs, diagrams, or videos, that depict the scene of the incident.\nAll documents related to the events leading up to, during, and immediately following the incident.\nAll witness statements or affidavits related to the incident.\nCause of the Incident\nAll documents evidencing any obstacles, hazards, or other conditions that you believe caused or contributed to the incident.\nAll documents related to your consumption of alcohol, drugs, or medication within 24 hours prior to the incident.\nAftermath of the Incident\nAll medical records, reports, or notes documenting the injuries you sustained as a result of the incident.\nAll documents related to any medical treatment you received immediately following the incident.\nAll correspondence or communication with healthcare providers regarding treatment for your injuries from the incident.\nPrior Medical Conditions\nAll medical records from the past ten (10) years documenting any medical conditions, injuries, or illnesses you have had.\nAll documents related to any prior injuries to the same body parts involved in the incident.\nMedical Care in Connection to the Incident\nAll hospital, clinic, or medical facility records where you have received treatment for injuries related to the incident.\nAll prescriptions and pharmacy records for medications prescribed as a result of the injuries from the incident.\nAll documents related to any medical tests, examinations, or diagnostic procedures undergone in connection with the injuries sustained in the incident.\nPrior Earnings through Present\nAll employment records for the past five (5) years, including job titles, duties, and dates of employment.\nAll documents showing your average monthly earnings for each year from 2019 to the present, including bonuses, commissions, and other forms of compensation.\nAll documents related to any loss of earnings or earning capacity claimed as a result of the incident.\nCurrent Medical Status\nAll medical records documenting your current medical condition, including ongoing symptoms or limitations resulting from the injuries sustained in the incident.\nAll documents from healthcare providers currently treating you for injuries related to the incident.\nAll documents related to any future medical treatment anticipated as a result of the injuries sustained in the incident.\nVocational Training and Abilities\nAll documents related to any vocational training, education, or certifications you have received.\nAll documents related to any vocational rehabilitation or job retraining programs participated in since the incident.\nImpact of the Incident\nAll documents evidencing any limitations or restrictions on your daily activities as a result of the injuries sustained in the incident.\nAll documents related to claims for damages, including general damages, special damages, and any other forms of relief sought.\nAll insurance policies that may provide coverage for your claims related to the incident.\nAll correspondence with any insurance company regarding claims related to the incident.\nLegal and Financial Records\nAll documents related to any prior lawsuits or claims for personal injury you have been involved in.\nAll documents related to any settlements or judgments received from prior lawsuits or claims.\nAll tax returns for the past five (5) years.\nAll documents evidencing any financial impact the incident has had on your household.\nCommunication Records\nAll correspondence or communication with any party about the incident.\nAll social media posts, messages, or other electronic communications regarding the incident or your injuries.\nOther Relevant Topics\nAll documents related to any physical therapy or rehabilitation you have undergone as a result of the incident.\nAll bills, invoices, or receipts for medical treatment received due to the incident.\nAll documents related to any assistive devices used as a result of your injuries (e.g., crutches, wheelchairs).\nAll documents related to any modifications made to your home or vehicle due to your injuries.\nAll documents related to any psychological or psychiatric treatment received as a result of the incident.\nAll documents related to any third-party payments for your medical treatment (e.g., insurance, workers’ compensation).\nAll photographs, videos, or recordings of your physical condition before and after the incident.\nAll documents evidencing any employment benefits lost as a result of the incident.\nAll documents related to any complaints or claims filed with government agencies regarding the incident.\nAll documents related to any public benefits applied for or received due to the incident (e.g., disability benefits).\nAll documents evidencing any travel or vacation plans disrupted by the incident.\nAll documents related to any additional expenses incurred as a result of the incident (e.g., childcare, transportation).', 'Document 12: Fake Earnings Records Summary(1).docx\nClaimant’s Earnings Summary\n<table><tr><td>Year</td><td>Source Documents</td><td>Annual Wages</td><td>Health Benefits Value</td><td>Dental Benefits Value</td><td>Vision Benefits Value</td><td>Pension Contribution</td><td>Total Compensation</td></tr><tr><td>2023</td><td>W-2, Union Benefits Summary</td><td>$55,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$64,650</td></tr><tr><td>2022</td><td>W-2, Union Benefits Summary</td><td>$53,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$62,650</td></tr><tr><td>2021</td><td>W-2, Union Benefits Summary</td><td>$50,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$59,650</td></tr><tr><td>2020</td><td>W-2, Union Benefits Summary</td><td>$48,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$57,650</td></tr><tr><td>2019</td><td>W-2, Union Benefits Summary</td><td>$47,000</td><td>$3,000</td><td>$1,500</td><td>$150</td><td>$5,000</td><td>$56,650</td></tr></table>', 'Document 13: Fake Earnings.docx\nYear 2023\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $55,000\nBox 2: Federal income tax withheld: $5,500\nBox 3: Social Security wages: $55,000\nBox 4: Social Security tax withheld: $3,410\nBox 5: Medicare wages and tips: $55,000\nBox 6: Medicare tax withheld: $797.50\nYear 2022\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $53,000\nBox 2: Federal income tax withheld: $5,300\nBox 3: Social Security wages: $53,000\nBox 4: Social Security tax withheld: $3,286\nBox 5: Medicare wages and tips: $53,000\nBox 6: Medicare tax withheld: $768.50\nYear 2021\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $50,000\nBox 2: Federal income tax withheld: $5,000\nBox 3: Social Security wages: $50,000\nBox 4: Social Security tax withheld: $3,100\nBox 5: Medicare wages and tips: $50,000\nBox 6: Medicare tax withheld: $725\nYear 2020\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $48,000\nBox 2: Federal income tax withheld: $4,800\nBox 3: Social Security wages: $48,000\nBox 4: Social Security tax withheld: $2,976\nBox 5: Medicare wages and tips: $48,000\nBox 6: Medicare tax withheld: $696\nYear 2019\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $47,000\nBox 2: Federal income tax withheld: $4,700\nBox 3: Social Security wages: $47,000\nBox 4: Social Security tax withheld: $2,914\nBox 5: Medicare wages and tips: $47,000\nBox 6: Medicare tax withheld: $681.50\nSocial Security Earnings Statement\nSocial Security Earnings Record for John Doe\n<table><tr><td>Year</td><td>Taxed Social Security Earnings</td><td>Taxed Medicare Earnings</td></tr><tr><td>2023</td><td>$55,000</td><td>$55,000</td></tr><tr><td>2022</td><td>$53,000</td><td>$53,000</td></tr><tr><td>2021</td><td>$50,000</td><td>$50,000</td></tr><tr><td>2020</td><td>$48,000</td><td>$48,000</td></tr><tr><td>2019</td><td>$47,000</td><td>$47,000</td></tr></table>\nSummary of Earnings\n2023: $55,000\n2022: $53,000\n2021: $50,000\n2020: $48,000\n2019: $47,000\nTotal Taxed Social Security Earnings: $253,000\nTotal Taxed Medicare Earnings: $253,000\nUnion Benefits Summary for John Doe\nUnion: XYZ Construction Workers Union\nMember Name: John Doe\nMember ID: 123456\nPeriod Covered: 2019 - 2023\nHealth Benefits\nProvider: Union Health Plan\nCoverage Period: 2019 - 2023\nPlan Type: Comprehensive Health Plan\nAnnual Premium: $0 (covered by employer contributions)\nAnnual Deductible: $500 individual / $1,000 family\nCo-Pay: $20 per doctor visit\nOut-of-Pocket Maximum: $3,000 individual / $6,000 family\nServices Covered:\nPrimary Care Visits: 100% after co-pay\nSpecialist Visits: 100% after co-pay\nEmergency Room: 80% after $100 co-pay\nInpatient Hospital: 80% after deductible\nPrescription Drugs: Tiered co-pay ($10 generic, $25 brand-name, $50 specialty)\nDental Benefits\nProvider: Union Dental Plan\nCoverage Period: 2019 - 2023\nAnnual Maximum Benefit: $1,500\nDeductible: $50 individual / $150 family\nPreventive Care: 100% covered (no deductible)\nBasic Services: 80% covered after deductible\nMajor Services: 50% covered after deductible\nOrthodontics: 50% covered, lifetime maximum of $2,000\nVision Benefits\nProvider: Union Vision Plan\nCoverage Period: 2019 - 2023\nAnnual Eye Exam: 100% covered (no co-pay)\nLenses: 100% covered for standard lenses once per year\nFrames: $150 allowance every two years\nContact Lenses: $150 allowance annually (in lieu of glasses)\nPension Benefits\nProvider: Union Pension Fund\nParticipation Period: 2019 - 2023\nVesting Schedule: Fully vested after 5 years\nAnnual Contribution: $5,000 (employer-funded)\nAccrued Benefit: Based on years of service and salary\nRetirement Age: 65\nEarly Retirement: Available from age 55 with reduced benefits\nAccrued Pension Benefit (as of 2023):\nEstimated Monthly Benefit at Retirement: $1,500 per month starting at age 65\nOther Benefits\nDisability Insurance\nShort-Term Disability: 60% of weekly wages up to $1,000 per week, up to 26 weeks\nLong-Term Disability: 50% of monthly wages up to $3,000 per month, after 180 days of disability\nLife Insurance\nCoverage Amount: $50,000 term life insurance policy\nAccidental Death and Dismemberment (AD&D): Additional $50,000 coverage\nAdditional Perks\nLegal Assistance: Free consultations and discounted rates on legal services\nEducation Assistance: Up to $2,000 annually for job-related education and training\nChildcare Assistance: Subsidies available for qualifying families\nSummary of Benefits by Year\n<table><tr><td>Year</td><td>Health Coverage</td><td>Dental Coverage</td><td>Vision Coverage</td><td>Pension Contribution</td><td>Other Benefits</td></tr><tr><td>2019</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr><tr><td>2020</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr><tr><td>2021</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr><tr><td>2022</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr><tr><td>2023</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr></table>', 'Document 14: Fake Injury and Illness Prevention Policy.docx\nFairs Are Us Injury and Illness Prevention Policy (IIPP)\n1. Introduction\nFairs Are Us is committed to providing a safe and healthy environment for all employees, contractors, and visitors. This Injury and Illness Prevention Policy (IIPP) outlines the procedures and responsibilities for maintaining safety standards, preventing injuries and illnesses, and ensuring compliance with applicable safety regulations.\n2. Policy Objectives\nTo identify and mitigate hazards in the workplace.\nTo educate and train employees on recognizing and addressing safety risks.\nTo establish clear procedures for reporting and managing hazards.\nTo promote a culture of safety and health within the organization.\n3. Monitoring for Tripping Hazards\n3.1 Regular Inspections\nFrequency: Conduct daily inspections of all fairgrounds, with particular attention to high-traffic areas such as ride exits, walkways, and entrances.\nResponsibility: Designated safety officers and supervisors are responsible for conducting these inspections.\nChecklist: Use a standardized checklist to ensure all potential tripping hazards, such as protruding spikes, uneven surfaces, and debris, are identified and documented.\n3.2 Immediate Remediation\nAction: Any identified tripping hazards must be marked immediately and remediated as soon as possible.\nReporting: All hazards must be reported to the safety officer and logged in the hazard tracking system.\n4. Warning Against Risks of Harm\n4.1 Signage\nPlacement: Install clear and visible warning signs around areas with known risks, such as construction zones, maintenance areas, and newly identified hazards.\nContent: Signs should include simple language and symbols to effectively communicate the nature of the hazard and the precautions to take.\n4.2 Public Announcements\nFrequency: Regularly broadcast safety announcements over the public address system, especially during peak times and events.\nContent: Announcements should remind visitors to be mindful of their surroundings, follow posted signs, and report any hazards to fair staff.\n5. Training Fair Workers\n5.1 Safety Training Program\nOrientation: All new employees must complete a safety orientation program that includes hazard recognition, safe work practices, and emergency procedures.\nOngoing Training: Conduct regular refresher courses and updates on safety policies and procedures.\n5.2 Specific Training on Hazard Identification and Reporting\nSpotting Hazards: Train workers to identify potential hazards such as tripping hazards, electrical risks, and mechanical failures.\nRemedy Procedures: Provide clear instructions on how to safely remedy minor hazards or secure areas until they can be properly addressed.\nReporting Protocols: Establish a straightforward reporting process for hazards, including the use of hazard report forms and direct communication with supervisors and safety officers.\n5.3 Supervisor and Management Training\nRole Clarity: Ensure that supervisors and management understand their roles in maintaining a safe work environment, including conducting inspections, addressing reported hazards, and supporting safety initiatives.\nIncident Investigation: Train supervisors on how to investigate incidents and near-misses, identify root causes, and implement corrective actions.\n6. Reporting and Communication\n6.1 Hazard Reporting System\nAccessibility: Make hazard report forms easily accessible to all employees.\nSubmission: Encourage prompt reporting of hazards through multiple channels, including direct supervisor contact, email, or a dedicated safety hotline.\n6.2 Feedback and Follow-Up\nAcknowledgment: Acknowledge receipt of hazard reports and provide feedback on the status of the reported issues.\nFollow-Up: Ensure timely follow-up and resolution of reported hazards, and communicate actions taken to the reporting employee.\n7. Continuous Improvement\nReview and Update: Regularly review and update the IIPP to reflect new safety regulations, emerging risks, and feedback from employees.\nSafety Committees: Establish safety committees to review incident reports, suggest improvements, and promote safety awareness.\n8. Conclusion\nFairs Are Us is dedicated to maintaining a safe and healthy environment for all. By following this Injury and Illness Prevention Policy, we can work together to identify and mitigate hazards, promote safety, and ensure a positive experience for all fair attendees and employees.', 'Document 15: Fake Earnings(1).docx\nYear 2023\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $55,000\nBox 2: Federal income tax withheld: $5,500\nBox 3: Social Security wages: $55,000\nBox 4: Social Security tax withheld: $3,410\nBox 5: Medicare wages and tips: $55,000\nBox 6: Medicare tax withheld: $797.50\nYear 2022\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $53,000\nBox 2: Federal income tax withheld: $5,300\nBox 3: Social Security wages: $53,000\nBox 4: Social Security tax withheld: $3,286\nBox 5: Medicare wages and tips: $53,000\nBox 6: Medicare tax withheld: $768.50\nYear 2021\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $50,000\nBox 2: Federal income tax withheld: $5,000\nBox 3: Social Security wages: $50,000\nBox 4: Social Security tax withheld: $3,100\nBox 5: Medicare wages and tips: $50,000\nBox 6: Medicare tax withheld: $725\nYear 2020\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $48,000\nBox 2: Federal income tax withheld: $4,800\nBox 3: Social Security wages: $48,000\nBox 4: Social Security tax withheld: $2,976\nBox 5: Medicare wages and tips: $48,000\nBox 6: Medicare tax withheld: $696\nYear 2019\nForm W-2: Wage and Tax Statement\nEmployee Information:\nName: John Doe\nSocial Security Number: 123-45-6789\nAddress: 123 Main St, Anytown, USA 12345\nEmployer Information:\nEmployer Name: XYZ Construction\nEmployer Identification Number (EIN): 12-3456789\nEmployer Address: 456 Industrial Rd, Anytown, USA 12345\nWages and Taxes:\nBox 1: Wages, tips, other compensation: $47,000\nBox 2: Federal income tax withheld: $4,700\nBox 3: Social Security wages: $47,000\nBox 4: Social Security tax withheld: $2,914\nBox 5: Medicare wages and tips: $47,000\nBox 6: Medicare tax withheld: $681.50\nSocial Security Earnings Statement\nSocial Security Earnings Record for John Doe\n<table><tr><td>Year</td><td>Taxed Social Security Earnings</td><td>Taxed Medicare Earnings</td></tr><tr><td>2023</td><td>$55,000</td><td>$55,000</td></tr><tr><td>2022</td><td>$53,000</td><td>$53,000</td></tr><tr><td>2021</td><td>$50,000</td><td>$50,000</td></tr><tr><td>2020</td><td>$48,000</td><td>$48,000</td></tr><tr><td>2019</td><td>$47,000</td><td>$47,000</td></tr></table>\nSummary of Earnings\n2023: $55,000\n2022: $53,000\n2021: $50,000\n2020: $48,000\n2019: $47,000\nTotal Taxed Social Security Earnings: $253,000\nTotal Taxed Medicare Earnings: $253,000\nUnion Benefits Summary for John Doe\nUnion: XYZ Construction Workers Union\nMember Name: John Doe\nMember ID: 123456\nPeriod Covered: 2019 - 2023\nHealth Benefits\nProvider: Union Health Plan\nCoverage Period: 2019 - 2023\nPlan Type: Comprehensive Health Plan\nAnnual Premium: $0 (covered by employer contributions)\nAnnual Deductible: $500 individual / $1,000 family\nCo-Pay: $20 per doctor visit\nOut-of-Pocket Maximum: $3,000 individual / $6,000 family\nServices Covered:\nPrimary Care Visits: 100% after co-pay\nSpecialist Visits: 100% after co-pay\nEmergency Room: 80% after $100 co-pay\nInpatient Hospital: 80% after deductible\nPrescription Drugs: Tiered co-pay ($10 generic, $25 brand-name, $50 specialty)\nDental Benefits\nProvider: Union Dental Plan\nCoverage Period: 2019 - 2023\nAnnual Maximum Benefit: $1,500\nDeductible: $50 individual / $150 family\nPreventive Care: 100% covered (no deductible)\nBasic Services: 80% covered after deductible\nMajor Services: 50% covered after deductible\nOrthodontics: 50% covered, lifetime maximum of $2,000\nVision Benefits\nProvider: Union Vision Plan\nCoverage Period: 2019 - 2023\nAnnual Eye Exam: 100% covered (no co-pay)\nLenses: 100% covered for standard lenses once per year\nFrames: $150 allowance every two years\nContact Lenses: $150 allowance annually (in lieu of glasses)\nPension Benefits\nProvider: Union Pension Fund\nParticipation Period: 2019 - 2023\nVesting Schedule: Fully vested after 5 years\nAnnual Contribution: $5,000 (employer-funded)\nAccrued Benefit: Based on years of service and salary\nRetirement Age: 65\nEarly Retirement: Available from age 55 with reduced benefits\nAccrued Pension Benefit (as of 2023):\nEstimated Monthly Benefit at Retirement: $1,500 per month starting at age 65\nOther Benefits\nDisability Insurance\nShort-Term Disability: 60% of weekly wages up to $1,000 per week, up to 26 weeks\nLong-Term Disability: 50% of monthly wages up to $3,000 per month, after 180 days of disability\nLife Insurance\nCoverage Amount: $50,000 term life insurance policy\nAccidental Death and Dismemberment (AD&D): Additional $50,000 coverage\nAdditional Perks\nLegal Assistance: Free consultations and discounted rates on legal services\nEducation Assistance: Up to $2,000 annually for job-related education and training\nChildcare Assistance: Subsidies available for qualifying families\nSummary of Benefits by Year\n<table><tr><td>Year</td><td>Health Coverage</td><td>Dental Coverage</td><td>Vision Coverage</td><td>Pension Contribution</td><td>Other Benefits</td></tr><tr><td>2019</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr><tr><td>2020</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr><tr><td>2021</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr><tr><td>2022</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr><tr><td>2023</td><td>Active</td><td>Active</td><td>Active</td><td>$5,000</td><td>Active</td></tr></table>', 'Document 16: Fake Management & Indemnity Agreement.docx\nFAIR MANAGEMENT AND OPERATIONS AGREEMENT\nThis Fair Management and Operations Agreement ("Agreement") is made and entered into as of [Date], by and between Sierra County ("County") and Fairs Are Us, a [State] corporation ("Fair Operator").\nRECITALS\nWHEREAS, the County desires to contract with the Fair Operator to manage and operate the Sierra County Fair ("Fair"); and\nWHEREAS, the Fair Operator has the expertise and experience to manage and operate the Fair; and\nWHEREAS, the parties wish to establish the terms and conditions under which the Fair Operator will manage and operate the Fair.\nNOW, THEREFORE, in consideration of the mutual covenants contained herein, the parties agree as follows:\n1. Term\nThe term of this Agreement shall commence on [Start Date] and continue through [End Date], unless terminated earlier in accordance with the provisions of this Agreement.\n2. Scope of Services\nThe Fair Operator shall manage and operate the Fair, including but not limited to, organizing events, maintaining fairgrounds, providing staffing, and ensuring compliance with all applicable laws and regulations.\n3. Compensation\nThe County shall pay the Fair Operator a management fee of [Amount] for the term of this Agreement, payable in [monthly/quarterly/annual] installments.\n4. Indemnification\nThe Fair Operator agrees to indemnify, defend, and hold harmless the County, its officers, agents, and employees from and against any and all claims, losses, damages, liabilities, costs, and expenses (including reasonable attorney\'s fees) arising out of or related to the Fair, except to the extent such claims or losses are caused by the County\'s gross negligence or willful misconduct.\n5. Insurance\n5.1 Insurance Coverage\nThe Fair Operator shall procure and maintain, at its own expense, the following insurance coverage:\nGeneral Liability Insurance: Coverage with limits of not less than $10,000,000 per occurrence and $10,000,000 aggregate for bodily injury, personal injury, and property damage.\nWorkers\' Compensation Insurance: Coverage as required by the laws of the State of [State].\nAutomobile Liability Insurance: Coverage with limits of not less than $1,000,000 per occurrence for bodily injury and property damage.\nUmbrella/Excess Liability Insurance: Coverage with limits of not less than $10,000,000 per occurrence and aggregate.\n5.2 Additional Insured\nThe County shall be named as an additional insured on all policies, except for Workers\' Compensation Insurance.\n5.3 Waiver of Subrogation\nAll insurance policies required under this Agreement shall include a waiver of subrogation in favor of the County.\n5.4 Proof of Insurance\nThe Fair Operator shall provide the County with certificates of insurance evidencing the required coverage prior to the commencement of the term and upon renewal of any such policies.\n6. Compliance with Laws\nThe Fair Operator shall comply with all applicable federal, state, and local laws, regulations, and ordinances in the performance of its obligations under this Agreement.\n7. Termination\n7.1 Termination for Convenience\nEither party may terminate this Agreement for convenience upon [Number] days\' prior written notice to the other party.\n7.2 Termination for Cause\nEither party may terminate this Agreement for cause if the other party breaches any material term or condition of this Agreement and fails to cure such breach within [Number] days after receipt of written notice thereof.\n8. Miscellaneous\n8.1 Entire Agreement\nThis Agreement constitutes the entire agreement between the parties and supersedes all prior agreements, understandings, and representations, whether written or oral, relating to the subject matter hereof.\n8.2 Amendments\nThis Agreement may only be amended by a written instrument executed by both parties.\n8.3 Governing Law\nThis Agreement shall be governed by and construed in accordance with the laws of the State of [State].\n8.4 Notices\nAll notices required or permitted under this Agreement shall be in writing and shall be deemed given when delivered personally, sent by certified mail, return receipt requested, or sent by a nationally recognized overnight delivery service, to the addresses set forth below, or to such other addresses as may be specified in writing by either party to the other in accordance with this section.\nCounty:\n[Sierra County Address]\n[City, State, Zip]\nAttn: [Contact Person]\nFair Operator:\n[Fairs Are Us Address]\n[City, State, Zip]\nAttn: [Contact Person]\nIN WITNESS WHEREOF, the parties hereto have executed this Agreement as of the day and year first above written.\nCOUNTY:\nBy: ________________________\nName: [Name]\nTitle: [Title]\nDate: _______________________\nFAIR OPERATOR:\nBy: ________________________\nName: [Name]\nTitle: [Title]\nDate: _______________________', 'Document 17: Fake Medical Records.docx\nSierra Vista Medical Center\nPatient Information\nName: John Doe\nDOB: 07/15/1980\nAge: 43\nGender: Male\nEthnicity: Caucasian\nHeight: 6\'0"\nWeight: 227 lbs\nOccupation: Construction Worker\nInitial Visit - Emergency Department\nDate: 05/01/2023\nPhysician: Pedro Velez, MD\nNurse Practitioner: Mary Martin, NP\nChief Complaint:\nPatient presents with severe pain, swelling, and inability to bear weight on the left ankle following a fall at a construction site.\nHistory of Present Illness:\nThe patient reports falling off a ladder approximately 10 feet. Immediate pain and swelling were noted in the left ankle.\nPhysical Examination:\nVital Signs: BP: 130/85, HR: 82 bpm, RR: 18, Temp: 98.6°F\nInspection: Significant swelling and bruising around the left ankle.\nPalpation: Tenderness over the lateral malleolus, crepitus felt.\nRange of Motion: Severely limited due to pain.\nNeurological: Sensation intact distally.\nCirculatory: Pulses present and strong.\nImaging:\nX-ray: Comminuted fracture of the distal fibula with displacement.\nAssessment:\nComminuted fracture of the distal fibula (left ankle).\nPlan:\nReduction and immobilization in a posterior splint.\nPain management: Prescribed hydrocodone/acetaminophen 5/325 mg, 1-2 tablets every 6 hours as needed.\nReferral to orthopedic surgery for operative intervention.\nOperative Report\nDate: 05/02/2023\nSurgeon: Pedro Velez, MD\nPreoperative Diagnosis:\nComminuted fracture of the distal fibula (left ankle).\nPostoperative Diagnosis:\nSame.\nProcedure:\nOpen reduction and internal fixation (ORIF) of the left distal fibula.\nAnesthesia:\nGeneral anesthesia.\nFindings:\nThe fracture was comminuted and displaced. Successful reduction and stabilization achieved with plate and screws.\nComplications:\nNone.\nDischarge Plan:\nWeight-bearing: Non-weight-bearing on the left leg.\nDVT prophylaxis: Prescribed enoxaparin 40 mg subcutaneously daily.\nFollow-up appointment in 1 week.\nFollow-Up Visit 1\nDate: 05/09/2023\nPhysician: Pedro Velez, MD\nNurse Practitioner: Mary Martin, NP\nSubjective:\nPatient reports significant pain improvement, slight swelling persists.\nObjective:\nInspection: Incision site clean, dry, and intact.\nPalpation: Mild tenderness.\nRange of Motion: Limited but improving.\nAssessment:\nPost-operative status is stable with no signs of infection.\nPlan:\nContinue pain management as needed.\nInitiate physical therapy for range of motion exercises.\nFollow-up in 4 weeks.\nFollow-Up Visit 2\nDate: 06/06/2023\nPhysician: Pedro Velez, MD\nNurse Practitioner: Mary Martin, NP\nSubjective:\nPatient reports increased mobility and decreased pain.\nObjective:\nInspection: Incision healed.\nPalpation: No tenderness.\nRange of Motion: Improved significantly.\nStrength: 4/5 in left ankle.\nAssessment:\nHealing progressing well.\nPlan:\nTransition to weight-bearing as tolerated.\nContinue physical therapy.\nFollow-up in 6 weeks for evaluation and potential removal of hardware.\nFinal Follow-Up Visit\nDate: 08/15/2023\nPhysician: Pedro Velez, MD\nNurse Practitioner: Mary Martin, NP\nSubjective:\nPatient reports full return to normal activities with minimal discomfort.\nObjective:\nInspection: No swelling or erythema.\nPalpation: No tenderness.\nRange of Motion: Full range of motion achieved.\nStrength: 5/5 in left ankle.\nAssessment:\nFully healed with no complications.\nPlan:\nDischarge from regular follow-up.\nAdvise patient on gradual return to full activity.\nSchedule hardware removal if symptomatic in the future.', 'Document 18: Fake Medical Records.docx - Google Docs.pdf\nSierra Vista Medical Center\nPatient Information\nName: John Doe\nDOB: 07/15/1980\nAge: 43\nGender: Male\nEthnicity: Caucasian\nHeight: 6\'0"\nWeight: 227 lbs\nOccupation: Construction Worker\nInitial Visit - Emergency Department\nDate: 05/01/2023\nPhysician: Pedro Velez, MD\nNurse Practitioner: Mary Martin, NP\nChief Complaint:\nPatient presents with severe pain, swelling, and inability to bear weight on the left ankle following a fall at a construction site.\nHistory of Present Illness:\nThe patient reports falling off a ladder approximately 10 feet. Immediate pain and swelling were noted in the left ankle.\nPhysical Examination:\no\nVital Signs: BP: 130/85, HR: 82 bpm, RR: 18, Temp: 98.6°F\no\nInspection: Significant swelling and bruising around the left ankle.\no\nPalpation: Tenderness over the lateral malleolus, crepitus felt.\no\nRange of Motion: Severely limited due to pain.\no\nNeurological: Sensation intact distally.\ne\nCirculatory: Pulses present and strong.\nImaging:\nL]\nX-ray: Comminuted fracture of the distal fibula with displacement.\nAssessment:\nL]\nComminuted fracture of the distal fibula (left ankle).\nPlan:\nL]\nReduction and immobilization in a posterior splint.\nSierra Vista Medical Center\nL]\nPain management: Prescribed hydrocodone/acetaminophen 5/325 mg, 1-2 tablets every 6 hours as needed.\nL]\nReferral to orthopedic surgery for operative intervention.\nOperative Report\nDate: 05/02/2023 Surgeon: Pedro Velez, MD\nPreoperative Diagnosis:\nComminuted fracture of the distal fibula (left ankle).\nPostoperative Diagnosis:\nSame.\nProcedure:\nOpen reduction and internal fixation (ORIF) of the left distal fibula.\nAnesthesia:\nGeneral anesthesia.\nFindings:\nThe fracture was comminuted and displaced. Successful reduction and stabilization achieved with plate and screws.\nComplications:\nNone.\nDischarge Plan:\nL]\nWeight-bearing: Non-weight-bearing on the left leg.\nL]\nDVT prophylaxis: Prescribed enoxaparin 40 mg subcutaneously daily.\nL]\nFollow-up appointment in 1 week.\nFollow-Up Visit 1\nDate: 05/09/2023\nPhysician: Pedro Velez, MD\nNurse Practitioner: Mary Martin, NP\nSubjective:\nPatient reports significant pain improvement, slight swelling persists.\nObjective:\nL]\nInspection: Incision site clean, dry, and intact.\n2\nSierra Vista Medical Center\nL]\nPalpation: Mild tenderness.\nL]\nRange of Motion: Limited but improving.\nAssessment:\nL]\nPost-operative status is stable with no signs of infection.\nPlan:\nL]\nContinue pain management as needed.\nL]\nInitiate physical therapy for range of motion exercises.\nL]\nFollow-up in 4 weeks.\nFollow-Up Visit 2\nDate: 06/06/2023\nPhysician: Pedro Velez, MD\nNurse Practitioner: Mary Martin, NP\nSubjective:\nPatient reports increased mobility and decreased pain.\nObjective:\no\nInspection: Incision healed.\no\nPalpation: No tenderness.\no\nRange of Motion: Improved significantly.\ne\nStrength: 4/5 in left ankle.\nAssessment:\nL]\nHealing progressing well.\nPlan:\nL]\nTransition to weight-bearing as tolerated.\nL]\nContinue physical therapy.\nL]\nFollow-up in 6 weeks for evaluation and potential removal of hardware.\nFinal Follow-Up Visit\nDate: 08/15/2023\nPhysician: Pedro Velez, MD\nNurse Practitioner: Mary Martin, NP\n3\nSierra Vista Medical Center\nSubjective:\nPatient reports full return to normal activities with minimal discomfort.\nObjective:\no\nInspection: No swelling or erythema.\no\nPalpation: No tenderness.\no\nRange of Motion: Full range of motion achieved.\ne\nStrength: 5/5 in left ankle.\nAssessment:\nL]\nFully healed with no complications.\nPlan:\nL]\nDischarge from regular follow-up.\nL]\nAdvise patient on gradual return to full activity.\nL]\nSchedule hardware removal if symptomatic in the future.\n4', "Document 19: Fake Plaintiff Deposition Transcript.docx\nDEPOSITION OF JOHN DOE\nSUPERIOR COURT OF CALIFORNIA\nFOR THE COUNTY OF SIERRA\nTaken March 15, 2024\nRecorded by Certified Court Reporters\nJane Green, Certification No. 238876543\nDoe v. Sierra County and Fairs Are Us\nSierra County Superior Court Action No. 24-SUP-659480\n\nPresent:\nPlaintiff's Counsel: Jane Smith, Esq.\nDefendants' Counsel: Robert Brown, Esq.\nOn the Record: 8:35am\nCourt Reporter: Please raise your right hand. Do you swear or affirm to tell the truth, the whole truth, and nothing but the truth?\nJohn Doe: I do.\nCourt Reporter: Please state your name for the record.\nJohn Doe: John Doe.\nDefendants' Counsel: Good morning, Mr. Doe. My name is Robert Brown, and I represent Sierra County and Fairs Are Us in this matter. Could you please state your full name and address for the record?\nJohn Doe: Sure. My name is John Doe, and I live at 123 Elm Street, Anytown, USA.\nDefendants' Counsel: Thank you. Mr. Doe, could you please describe your occupation?\nJohn Doe: I'm a construction worker.\nDefendants' Counsel: And how long have you been working in that field?\nJohn Doe: For about 20 years now.\nDefendants' Counsel: Let's talk about the incident on January 23, 2024. Can you describe what happened that day?\nJohn Doe: Yes. I was at the Sierra County Fair with my two daughters, who are nine years old, and a couple of my co-workers. We were leaving the Tilt-A-Whirl ride when I tripped over a spike that was sticking out of the ground in the exit path.\nDefendants' Counsel: What time did this incident occur?\nJohn Doe: It was around 3:00 PM.\nDefendants' Counsel: Can you describe the spike you tripped over?\nJohn Doe: It was a metal spike, maybe about four inches high, sticking out of the ground. It was right in the path where people were exiting the ride.\nDefendants' Counsel: What happened immediately after you tripped?\nJohn Doe: I fell and landed awkwardly on my left ankle. I felt a sharp pain and couldn't stand up. I also felt pain in my back and neck.\nDefendants' Counsel: Did anyone come to assist you?\nJohn Doe: Yes, my co-workers and a fair staff member came over. They called for medical help, and I was taken to Sierra Vista Medical Center.\nDefendants' Counsel: What kind of treatment did you receive at the medical center?\nJohn Doe: They did some X-rays and found that I had a broken left ankle. I needed immediate surgery. They also noted that I had back spasms and a neck strain.\nDefendants' Counsel: How long were you in the hospital?\nJohn Doe: I was there for three days.\nDefendants' Counsel: Are you currently receiving any treatment?\nJohn Doe: Yes, I'm undergoing physical therapy for my ankle, and I'm also seeing a chiropractor for my back and neck.\nDefendants' Counsel: How has this injury affected your ability to work?\nJohn Doe: I can't return to my job as a construction worker. My doctor hasn't given me a timeline for when I might be able to go back to work.\nDefendants' Counsel: Are you seeking compensation for lost wages?\nJohn Doe: Yes, I am. I'm also seeking compensation for general damages, loss of household services, and other related expenses.\nDefendants' Counsel: Mr. Doe, have you experienced any other impacts on your daily life due to this injury?\nJohn Doe: Yes, I can't do many of the household chores I used to do, and I can't play with my daughters the way I used to. It's been very difficult.\nDefendants' Counsel: Thank you, Mr. Doe. I need to ask you about your activities at the fair. Did you consume any alcohol that day?\nJohn Doe: Yes, I did. I had four beers.\nDefendants' Counsel: Over what period of time did you consume these beers?\nJohn Doe: Over the course of about two hours.\nDefendants' Counsel: Do you believe the alcohol affected your ability to safely navigate the fairgrounds?\nJohn Doe: No, I felt fine and in control.\nDefendants' Counsel: Did you notice the spike or any other unsafe conditions before you tripped?\nJohn Doe: No, I didn't see the spike until I tripped over it. I wasn't looking for hazards specifically, but I didn't notice anything out of the ordinary before the accident.\nDefendants' Counsel: Did you use the Tilt-A-Whirl ride before the incident?\nJohn Doe: Yes, I went on the ride with my daughters.\nDefendants' Counsel: Did you notice the spike or any other unsafe conditions while using the Tilt-A-Whirl or in the exit path before you tripped?\nJohn Doe: No, I did not notice anything unusual when we got off the ride. It wasn't until I tripped that I saw the spike.\nDefendants' Counsel: Thank you, Mr. Doe. I have no further questions at this time.\nPlaintiff's Counsel: I have a few questions, Mr. Doe. Can you describe the condition of the fairgrounds on the day of the incident?\nJohn Doe: The fairgrounds were crowded, and there were a lot of people around. The exit path from the ride was pretty narrow, and I didn't see the spike until it was too late.\nPlaintiff's Counsel: Had you noticed any other hazards at the fair that day?\nJohn Doe: No, I didn't notice anything else out of the ordinary.\nPlaintiff's Counsel: Thank you, Mr. Doe. That's all I have.\nCourt Reporter: The deposition is concluded at 11:00 AM.", 'Document 20: Fake Medical Record Summary.docx\nClaimant’s Medical Record Summary\n<table><tr><td>Source</td><td>Bates #</td><td>Care Provider</td><td>Entry Date</td><td>Visit Date</td><td>Summary</td><td>Service Type</td><td>Charges</td><td>Amount Paid</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>1</td><td>Dr. Smith, PCP</td><td>01/23/24</td><td>1/23/24</td><td>Pt presents after fall from ladder with severe left ankle pain and inability to bear weight. Exam reveals swelling, deformity, and tenderness. X-ray ordered. Diagnosis: Suspected ankle fracture.</td><td>Office Visit</td><td>$250</td><td>$200</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>2</td><td>Radiology, Sierra Vista</td><td>01/23/24</td><td>1/23/24</td><td>X-ray report confirms trimalleolar fracture of the left ankle.</td><td>X-ray Report</td><td>$500</td><td>$500</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>3</td><td>Dr. Jones, Ortho</td><td>01/24/24</td><td>1/24/24</td><td>Surgical consult. Diagnosis confirmed. Pt to undergo ORIF due to unstable fracture. Risks and benefits discussed. Surgery scheduled for 01/25/2024.</td><td>Surgical Consult</td><td>$300</td><td>$250</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>4</td><td>Sierra Vista Hospital Records</td><td>01/25/24</td><td>01/25/24</td><td>Pt undergoes ORIF of left ankle. Post-op diagnosis: Trimalleolar fracture, left ankle. Procedure notes indicate successful fixation.</td><td>ORIF Procedure</td><td>$8,000</td><td>$6,000</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>5</td><td>Sierra Vista Hospital Records</td><td>01/26/24</td><td>01/26/24</td><td>Post-op day 1. Pt reports pain controlled by medication. Incision clean, pulses palpable. Discharge planned for tomorrow with NWB instructions and follow-up with orthopedist in 1 week.</td><td>Hospital Stay</td><td>$1,200</td><td>$900</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>6</td><td>Dr. Jones</td><td>02/01/24</td><td>02/01/24</td><td>Post-op follow-up. Incision healing well, no signs of infection. Pt reports pain improving. Sutures removed. NWB status continued. Referral to physical therapy.</td><td>Office Visit</td><td>$150</td><td>$120</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>7</td><td>PT Notes</td><td>02/08/24</td><td>02/08/24</td><td>Initial PT evaluation. Pt presents with limited ROM, strength, and swelling. Gentle exercises initiated. Home exercise program provided.</td><td>PT Evaluation</td><td>$200</td><td>$160</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>8-9</td><td>PT Notes</td><td>02/15/24</td><td>02/15/24</td><td>Continued PT. Pt demonstrating gradual improvements in ROM and strength. Swelling decreasing. Pt tolerating increased weight-bearing activities.</td><td>Physical Therapy</td><td>$400</td><td>$320</td></tr><tr><td>Plaintiff&#x27;s RFPD Response No. 1</td><td>10</td><td>Dr. Jones</td><td>02/22/24</td><td>02/22/24</td><td>Follow-up. X-rays show good alignment. Pt progressing well in PT. Weight-bearing status advanced to PWB. Discussion about return to work initiated.</td><td>Office VisitX-ray</td><td>$250$150</td><td/></tr></table>', 'Document 21: Fake Request for Production of Documents.doc\nORACLAIM PARTNERS LLP\nEmelia C. Zarzuela (SBN 002456)\nRobert T. Brown (SBN 005778)\n335 Elm Street, Suite 350\nSan Francisco, CA  94111\nTelephone No.: 415-555-4600\nFacsimile No.: 415-555-4601\nAttorneys for Defendants\nFAIRS ARE US, INC. and COUNTY OF SIERRA\nSUPERIOR COURT OF THE STATE OF CALIFORNIA\nCOUNTY OF SIERRA\n<table><tr><td>JOHN DOE Plaintiff, vs. FAIRS ARE US, INC., COUNTY OF SIERRA, and DOES 1 through 25, inclusive, Defendants.</td><td/><td>Case No. CGG-24-555555 Action Filed: January 5, 2024 Trial Date: TBD DEFENDANT FAIRS ARE US INC.’S REQUESTS FOR PRODUCTION, SET ONE</td></tr></table>\nPropounding Party:\tDefendant FAIRS ARE US, INC.\nResponding Party:\tPlaintiff JOHN DOE\nSet No.:\t\t\tONE\nTO PLAINTIFF AND HIS ATTORNEYS OF RECORD:\nPursuant to California Code of Civil Procedure § 2031.010, Defendant FAIRS ARE US, INC. (“Defendant”) serves this Demand for Production of Documents, Set No. One, upon Plaintiff JOHN DOE (“Plaintiff”), and requests that Plaintiff produce and permit Defendant to inspect and copy the documents, writing, and other tangible things designated below.\nThe material requested shall be produced for inspection and copying by the propounding party within thirty (30) days after service at 10:00 a.m. at the law offices of Oraclaim Partners LLP, 335 Elm Street, Suite 350, San Francisco, California, 94111, telephone: (415) 555-4600.  In lieu of permitting the inspection and copying of original documents on that date, copies of the responsive documents and other original prints of photographs may be sent to the attorneys for Defendant to be received by the date of production. The materials produced shall either be (1) produced as they are kept in the usual course of business, or (2) organized and labelled with exhibit numbers which correspond to each request so that said response will accurately show the specific categories of materials produced by Plaintiff or the absence thereof. \nIf the original of a document, writing or other tangible thing is within your possession, custody, or control, produce it; if not, produce such copy of it as is in your possession, custody or control.  Any copy which has any notation, addition, modification, alteration, or change is to be treated as constituting an additional original.\nDEFINITION OF TERMS\n“DOCUMENT” or “DOCUMENTS” means a writing, as defined in Evidence Code § 250, and includes the original or a copy of handwriting, typewriting, printing, photostating, photographing, photocopying, transmitting by electronic mail or facsimile, and every other means of recording upon any tangible thing, any form of communication or representation, including letters, words, pictures, sounds, or symbols, or combinations thereof, and any record thereby created, regardless of the manner in which the record has been stored.\n“YOU” or “YOUR” refers to the plaintiff, his agents, his employees, his insurance companies, their agents, their employees, his attorneys, his accountants, his investigators, and anyone else acting on his behalf.\n“INCIDENT” refers to the circumstances and events described in Plaintiff’s complaint, which give rise to this action.\n“THIS LITIGATION,” “THIS LAWSUIT,” or the “COMPLAINT” all refer to the civil action described in the caption and on this pleading.\nREQUESTS FOR PRODUCTION\nREQUEST FOR PRODUCTION NO. 1:\n\tYOUR Driver’s License or Identification Card. \nREQUEST FOR PRODUCTION NO. 2:\n\tYOUR Social Security Card.\nREQUEST FOR PRODUCTION NO. 3:\n\tAny and all diplomas or certificates of completion for any kind of education YOU have received since 1995.\nREQUEST FOR PRODUCTION NO. 4:\nAny and all DOCUMENTS related to any kind of occupational training YOU have received since 1995.\nREQUEST FOR PRODUCTION NO. 5:\n\tAny and all DOCUMENTS related to any kind of vocational rehabilitation YOU have received since 1995.\nREQUEST FOR PRODUCTION NO. 6:\n\tAny and all DOCUMENTS related to any kind of vocational rehabilitation consultation YOU have had since the date of the INCIDENT.\nREQUEST FOR PRODUCTION NO. 7:\n\tAny and all DOCUMENTS relating to or evidencing YOUR service in any branch of the United States military or the military service of any foreign country.\nREQUEST FOR PRODUCTION NO. 8:\n\tAny and all employment records from any of YOUR past employers.\nREQUEST FOR PRODUCTION NO. 9:\n\tAny and all employment records from YOUR current employer(s).\nREQUEST FOR PRODUCTION NO. 10:\nAny and all disciplinary records for any of YOUR employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. 11:\nAny and all weekly meeting logs for any of YOUR employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. 12:\nAny and all overtime records for any of YOUR employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. 13:\nAny and all incident reports for any of YOUR employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. 14:\nAny and all notices of discharge for any of your employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. 15:\n\tAny and all DOCUMENTS related to employment agreements YOU have entered into since the INCIDENT.\nREQUEST FOR PRODUCTION NO. 16:\n\tAny and all DOCUMENTS concerning unemployment benefits YOU have sought from any entity, including federal or state governments or private companies, from 2007 to present (including but not limited to, unemployment compensation applications, correspondence, statement of benefits, acceptance or rejection of benefits, payments received, or other).\nREQUEST FOR PRODUCTION NO. 17:\n\tAny and all DOCUMENTS concerning disability benefits YOU have sought from any entity, including federal or state governments or private companies, from 2007 to present (including but not limited to, disability compensation applications, correspondence, statement of benefits, acceptance or rejection of benefits, payments received, or other).\nREQUEST FOR PRODUCTION NO. 18:\n\tAny and all DOCUMENTS showing any income you have earned from 2007 to present (including but not limited to, check stubs, W-2 Statements, or other).\nREQUEST FOR PRODUCTION NO. 19:\nAny and all DOCUMENTS showing any income YOU received from any source other than Defendant from 2007 to present (including but not limited to, every financial statement, profit and loss statement, earnings statement, or checking account register).\nREQUEST FOR PRODUCTION NO. 20:\n\tEvery state and/or federal tax return which YOU have filed from 2007 to present.\nREQUEST FOR PRODUCTION NO. 21:\n\tYOUR most recent Social Security Earnings Statement.\nREQUEST FOR PRODUCTION NO. 22:\n\tYOUR Social Security Earnings Statement from the year prior to the INCIDENT.\nREQUEST FOR PRODUCTION NO. 23:\n\tAny and all DOCUMENTS which support YOUR claim of lost earnings.\nREQUEST FOR PRODUCTION NO. 24:\nAny and all DOCUMENTS which support YOUR claim of loss of earning capacity.\nREQUEST FOR PRODUCTION NO. 25:\n\tAny and all DOCUMENTS regarding YOUR membership in each and every union of which YOU were a member at the time of the INCIDENT.\nREQUEST FOR PRODUCTION NO. 26:\n\tAny and all union dispatch slips related to YOUR work for any employer from 2007 to present.\nREQUEST FOR PRODUCTION NO. 27:\nAny and all union discharge slips related to YOUR work for any employer from 2007 to present.\nREQUEST FOR PRODUCTION NO. 28:\n\tAny and all DOCUMENTS which depict the scene of the INCIDENT and/or any locations adjacent thereto (including but not limited to, photographs, videos, films, diagrams, charts or others).\nREQUEST FOR PRODUCTION NO. 29:\n\tAny and all DOCUMENTS which depict any object involved in the INCIDENT (including but not limited to, photographs, videos, films, or others).\nREQUEST FOR PRODUCTION NO. 30:\nAny and all DOCUMENTS which depict any injury involved in the INCIDENT (including but not limited to, photographs, videos, films, or others).\nREQUEST FOR PRODUCTION NO. 31:\n\tAny and all DOCUMENTS which contain a description of the INCIDENT, (including but not limited to, incident reports, doctor’s first report of work injury, or other).\nREQUEST FOR PRODUCTION NO. 32:\n\tAny and all DOCUMENTS which contain a written statement related in any way to the INCIDENT.\nREQUEST FOR PRODUCTION NO. 33:\n\tAny and all DOCUMENTS which contain a recorded statement related in any way to the INCIDENT.\nREQUEST FOR PRODUCTION NO. 34:\n\tAny and all DOCUMENTS which transcribe or record any oral statement made by any person describing the INCIDENT.\nREQUEST FOR PRODUCTION NO. 35:\n\tAny and all DOCUMENTS related to any and all medical or physical exams that YOU have undergone related to YOUR work with any employer from 2007 to present.\nREQUEST FOR PRODUCTION NO. 36:\n\tAny and all DOCUMENTS which pertain to the diagnosis and treatment of the physical injuries which YOU contend that YOU sustained in the INCIDENT (including but not limited to, medical bills or statements, medical reports, other medical or hospital records, or other).\nREQUEST FOR PRODUCTION NO. 37:\n\tAny and all DOCUMENTS reflecting YOUR fitness to work (fit-for-duty-status) since the date of the INCIDENT.\nREQUEST FOR PRODUCTION NO. 38:\n\tAny and all DOCUMENTS that support your claim for future medical expenses in connection with the INCIDENT giving rise to this LITIGATION.\nREQUEST FOR PRODUCTION NO. 39:\n\tAny and all DOCUMENTS that support your claim for past medical expenses in connection with the INCIDENT giving rise to this LITIGATION.\nREQUEST FOR PRODUCTION NO. 40:\n\tAny and all DOCUMENTS pertaining to medical treatment for the part(s) of YOUR body claimed to have been injured in the INCIDENT, whether or not such records were created before or after the date of the INCIDENT (including but not limited to, medical records, bills, reports, x-rays and other radiographic studies).\nREQUEST FOR PRODUCTION NO. 41:\n\tAny and all DOCUMENTS prepared by YOU and pertaining in any way to YOUR injuries alleged in the COMPLAINT (including but not limited to, notes, diary entries, log book entries, calendar entries, daytimer/personal planner entries, or other).\nREQUEST FOR PRODUCTION NO. 42:\n\tAny and all DOCUMENTS reflecting any item of damages claimed in the COMPLAINT (including but not limited to, bills, invoices, statements, or other).\nREQUEST FOR PRODUCTION NO. 43:\n\tAny and all DOCUMENTS that support your claim for general damages in connection with the INCIDENT.\nREQUEST FOR PRODUCTION NO. 44:\n\tAny and all DOCUMENTS which support YOUR claim of Defendant’s negligence as YOU allege in the COMPLAINT.\nREQUEST FOR PRODUCTION NO. 45:\n\tAny and all retirement DOCUMENTS that YOU submitted to any entity (including but not limited to, unions or employers).\nREQUEST FOR PRODUCTION NO. 46:\n\tAny and all DOCUMENTS which describe any pension plans YOU have been enrolled in from 2007 to present.\nREQUEST FOR PRODUCTION NO. 47:\n\tAny and all DOCUMENTS which describe any disaibility plans YOU have been enrolled in from 2007 to present.\nREQUEST FOR PRODUCTION NO. 48:\n\tAny and all DOCUMENTS which describe any retirement plans YOU have been enrolled in from 2007 to present.\nREQUEST FOR PRODUCTION NO. 49:\n\tAny and all medical insurance policies that provide coverage for the injuries alleged in YOUR COMPLAINT.\nREQUEST FOR PRODUCTION NO. 50:\n\tAny and all physical evidence concerning the INCIDENT in your care, custody, or control.\nREQUEST FOR PRODUCTION NO. 51:\nAny and all DOCUMENTS that YOU contend support your claim of Jones Act Seaman status at the time of the INCIDENT.\nREQUEST FOR PRODUCTION NO. 52:\n\tAny and all DOCUMENTS that refer to or reflect YOUR recreational pursuits since the INCIDENT (including but not limited to, emails, tickets, and receipts related to vacation travel, concerts, sports venues, hobbies, or other).\nREQUEST FOR PRODUCTION NO. 53:\nAny and all DOCUMENTS and things you described in your response to Special INTERROGATORY NO. 27.\nREQUEST FOR PRODUCTION NO. 54:\nAny and all DOCUMENTS and things you described in your response to Special INTERROGATORY NO. 30.\nDated:  July 16, 2024\t\t\t\tORACLAIM PARTNERS LLP\nAttorneys for Defendant \nFAIRS ARE US, INC. \n     \tBy:_________________________________\n\t\t\t\t\t\t\tRobert T. Brown\n-8-\nDEFENDANT’S REQUEST FOR PRODUCTION OF DOCUMENTS TO PLAINTIFF, SET ONE', "Document 22: Fake Motion for Summary Judgment.docx\nQ: In this case, if the injured plaintiff had seen the stake he tripped on many times before tripping on it and knew it was there before he got hurt, and the stake was necessary to keep the tilt a whirl secure, do you think Fairs Are Us could prevail on a motion for summary judgment in california state court?\nChatGPT\nA: In California state court, a motion for summary judgment is granted when there is no genuine issue of material fact and the moving party is entitled to judgment as a matter of law (California Code of Civil Procedure § 437c(c)). In evaluating whether Fairs Are Us could prevail on a motion for summary judgment given the plaintiff’s prior knowledge of the stake, several legal principles and defenses could be considered:\nKey Legal Principles\nAssumption of Risk:\nIf the plaintiff had seen the stake many times before and knew it was there, Fairs Are Us could argue that the plaintiff assumed the risk of injury. Assumption of risk can be a complete defense to negligence claims if it is shown that the plaintiff voluntarily and knowingly assumed the risks inherent to a particular activity (Knight v. Jewett (1992) 3 Cal.4th 296, 309).\nOpen and Obvious Doctrine:\nThe open and obvious doctrine posits that a landowner is not liable for injuries caused by a condition that is open and obvious because it serves as a warning itself. If the stake was an open and obvious condition that the plaintiff knew about, Fairs Are Us could argue that they had no duty to warn the plaintiff of the risk (Kinsman v. Unocal Corp. (2005) 37 Cal.4th 659, 673).\nNecessity of the Stake:\nIf the stake was necessary to keep the Tilt-A-Whirl secure, Fairs Are Us might argue that the stake served a legitimate safety purpose, and removing or modifying it could have introduced other risks. They might assert that reasonable measures were in place to mitigate the risk, such as making the stake visible or providing warnings.\nFactors to Consider\nPlaintiff's Knowledge and Conduct:\nEvidence that the plaintiff had seen the stake multiple times and was aware of its presence would support the argument for assumption of risk and the open and obvious doctrine.\nNecessity and Reasonableness:\nIf the stake was crucial for the safety of the ride, and Fairs Are Us can demonstrate that it was positioned and maintained in a reasonable manner, they could argue that they met their duty of care.\nAlternative Measures:\nThe court will consider whether Fairs Are Us took reasonable alternative measures to mitigate the risk, such as using bright paint to mark the stake or placing warning signs.\nCase Law\nKnight v. Jewett (1992) 3 Cal.4th 296: Establishes the doctrine of primary assumption of risk, which may apply if the plaintiff voluntarily engaged in an activity with inherent risks.\nKinsman v. Unocal Corp. (2005) 37 Cal.4th 659: Discusses the open and obvious doctrine, highlighting situations where landowners are not liable for injuries caused by obvious conditions.\nConclusion\nGiven the facts presented:\nThe plaintiff’s prior knowledge of the stake and the necessity of the stake for ride security could provide strong arguments for Fairs Are Us to prevail on a motion for summary judgment.\nIf Fairs Are Us can demonstrate that the risk was open and obvious and/or assumed by the plaintiff, and that they took reasonable measures to ensure safety, the court may find in their favor.\nHowever, whether Fairs Are Us would ultimately prevail depends on the specific details and evidence presented, including the adequacy of warnings and any mitigating actions taken. The court will carefully consider whether there are any genuine issues of material fact that would necessitate a trial.\nSources\nCalifornia Code of Civil Procedure § 437c\nKnight v. Jewett (1992) 3 Cal.4th 296\nKinsman v. Unocal Corp. (2005) 37 Cal.4th 659\nQ: Write a motion for summary judgment that conforms to the SF Superior Court rules based on the above\nA: Below is a draft of a Motion for Summary Judgment that conforms to the San Francisco Superior Court rules, based on the facts provided and the legal arguments discussed.\nLAW OFFICES OF ROBERT BROWN Robert Brown, Esq. (SBN 123456) 123 Main Street San Francisco, CA 94101 Tel: (555) 123-4567 Email: rbrown@lawoffice.com\nAttorney for Defendant Fairs Are Us\nSUPERIOR COURT OF THE STATE OF CALIFORNIA COUNTY OF SAN FRANCISCO\nJohn Doe,\nPlaintiff,\nv.\nSierra County; Fairs Are Us; and DOES 1-10, inclusive,\nDefendants.\nCase No.: [Case Number]\nNOTICE OF MOTION AND MOTION FOR SUMMARY JUDGMENT; MEMORANDUM OF POINTS AND AUTHORITIES\nTO ALL PARTIES AND THEIR ATTORNEYS OF RECORD:\nPLEASE TAKE NOTICE that on [Hearing Date] at [Time], or as soon thereafter as the matter may be heard in Department [Department Number] of the above-entitled court located at 400 McAllister Street, San Francisco, CA 94102, Defendant Fairs Are Us (“Defendant”) will, and hereby does, move the Court for an order granting summary judgment in its favor pursuant to California Code of Civil Procedure section 437c.\nThis motion is based on the ground that there is no triable issue of material fact, and Defendant is entitled to judgment as a matter of law. The motion is made on the grounds that the undisputed facts establish that Plaintiff assumed the risk of injury, the hazard was open and obvious, and Defendant met its duty of care by maintaining the safety of the Tilt-A-Whirl ride.\nThis motion is based on this Notice of Motion, the Memorandum of Points and Authorities, the Separate Statement of Undisputed Material Facts, the Declaration of Robert Brown, Esq., the pleadings and papers on file herein, and such other and further oral and documentary evidence as may be presented at the hearing.\nDated: [Date]\nLAW OFFICES OF ROBERT BROWN\n\nBy: ____________________________\nRobert Brown, Esq.\nAttorney for Defendant\nFairs Are Us\nMEMORANDUM OF POINTS AND AUTHORITIES\nI. INTRODUCTION\nDefendant Fairs Are Us seeks summary judgment on the grounds that Plaintiff John Doe’s claims are barred by the doctrines of assumption of risk and the open and obvious nature of the hazard. Plaintiff was aware of the stake in the ground, which was necessary for the secure operation of the Tilt-A-Whirl ride, and knowingly proceeded despite this awareness. Furthermore, Defendant took reasonable steps to ensure the safety of its patrons.\nII. STATEMENT OF FACTS\nOn January 23, 2024, Plaintiff attended the Sierra County Fair and tripped over a stake near the Tilt-A-Whirl ride, resulting in injuries. The stake was necessary for the structural integrity of the ride and was visible to Plaintiff, who had seen it multiple times before the incident. Despite this knowledge, Plaintiff chose to walk near the stake, resulting in his injuries.\nIII. LEGAL STANDARD\nSummary judgment is appropriate where there is no genuine issue of material fact and the moving party is entitled to judgment as a matter of law (Cal. Civ. Proc. Code § 437c(c)). The moving party bears the initial burden of showing that one or more elements of the cause of action cannot be established or that there is a complete defense to the cause of action (Aguilar v. Atlantic Richfield Co. (2001) 25 Cal.4th 826, 850).\nIV. ARGUMENT\nA. Plaintiff Assumed the Risk of Injury\nUnder the doctrine of primary assumption of risk, a defendant owes no duty of care to protect a plaintiff from risks inherent in a particular activity if the plaintiff voluntarily engages in the activity with knowledge of the risks involved (Knight v. Jewett (1992) 3 Cal.4th 296, 308-309). Here, Plaintiff had seen the stake multiple times and knew of its presence, thereby assuming the risk of injury by walking near it.\nB. The Hazard Was Open and Obvious\nThe open and obvious doctrine provides that a landowner is not liable for injuries caused by an obvious danger because the danger itself serves as a warning (Kinsman v. Unocal Corp. (2005) \n37 Cal.4th 659, 673). The stake was visible and known to Plaintiff, making it an open and obvious hazard.\nC. Defendant Met Its Duty of Care\nDefendant maintained the stake as a necessary safety measure for the Tilt-A-Whirl ride. The stake’s visibility and necessity for the ride’s operation demonstrate that Defendant took reasonable steps to ensure patron safety. There is no evidence that Defendant acted negligently.\nV. CONCLUSION\nFor the foregoing reasons, Defendant Fairs Are Us respectfully requests that the Court grant its motion for summary judgment.\nDated: [Date]\nLAW OFFICES OF ROBERT BROWN\nBy: ____________________________\nRobert Brown, Esq.\nAttorney for Defendant\nFairs Are Us\nSeparate Statement of Undisputed Material Facts\n1. Plaintiff attended the Sierra County Fair on January 23, 2024.\n2. Plaintiff tripped over a stake near the Tilt-A-Whirl ride.\n3. The stake was necessary for the structural integrity of the ride.\n4. Plaintiff had seen the stake multiple times before the incident.\n5. The stake was visible and known to Plaintiff.\n6. Defendant maintained the stake as a safety measure for the ride.\n7. There is no evidence that Defendant acted negligently in maintaining the stake.\nDeclaration of Robert Brown, Esq.\nI, Robert Brown, declare as follows:\nI am an attorney at law duly licensed to practice before all courts in the State of California and am counsel of record for Defendant Fairs Are Us.\nAttached hereto as Exhibit A is a true and correct copy of the incident report regarding Plaintiff’s fall.\nAttached hereto as Exhibit B is a true and correct copy of Plaintiff’s deposition transcript where he admits seeing the stake multiple times before the incident.\nAttached hereto as Exhibit C is a true and correct copy of the safety inspection reports for the Tilt-A-Whirl ride.\nI declare under penalty of perjury under the laws of the State of California that the foregoing is true and correct.\nExecuted on [Date] at San Francisco, California.\nRobert Brown, Esq.", 'Document 23: Fake Request for Production of Documents.docx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\nORACLAIM PARTNERS LLP\nEmelia C. Zarzuela (SBN 002456)\nRobert T. Brown (SBN 005778)\n335 Elm Street, Suite 350\nSan Francisco, CA  94111\nTelephone No.: 415-555-4600\nFacsimile No.: 415-555-4601\nAttorneys for Defendants\nFAIRS ARE US, INC. and COUNTY OF SIERRA\nSUPERIOR COURT OF THE STATE OF CALIFORNIA\nCOUNTY OF SIERRA\n<table><tr><td>JOHN DOE Plaintiff, vs. FAIRS ARE US, INC., COUNTY OF SIERRA, and DOES 1 through 25, inclusive, Defendants.</td><td/><td>Case No. CGG-24-555555 Action Filed: January 5, 2024 Trial Date: TBD DEFENDANT FAIRS ARE US INC.’S REQUESTS FOR PRODUCTION, SET ONE</td></tr></table>\nPropounding Party:\tDefendant FAIRS ARE US, INC.\nResponding Party:\tPlaintiff JOHN DOE\nSet No.:\t\t\tONE\nTO PLAINTIFF AND HIS ATTORNEYS OF RECORD:\nPursuant to California Code of Civil Procedure § 2031.010, Defendant FAIRS ARE US, INC. (“Defendant”) serves this Demand for Production of Documents, Set No. One, upon Plaintiff JOHN DOE (“Plaintiff”), and requests that Plaintiff produce and permit Defendant to inspect and copy the documents, writing, and other tangible things designated below.\nThe material requested shall be produced for inspection and copying by the propounding party within thirty (30) days after service at 10:00 a.m. at the law offices of Oraclaim Partners LLP, 335 Elm Street, Suite 350, San Francisco, California, 94111, telephone: (415) 555-4600.  In lieu of permitting the inspection and copying of original documents on that date, copies of the responsive documents and other original prints of photographs may be sent to the attorneys for Defendant to be received by the date of production. The materials produced shall either be (1) produced as they are kept in the usual course of business, or (2) organized and labelled with exhibit numbers which correspond to each request so that said response will accurately show the specific categories of materials produced by Plaintiff or the absence thereof. \nIf the original of a document, writing or other tangible thing is within your possession, custody, or control, produce it; if not, produce such copy of it as is in your possession, custody or control.  Any copy which has any notation, addition, modification, alteration, or change is to be treated as constituting an additional original.\nDEFINITION OF TERMS\n“DOCUMENT” or “DOCUMENTS” means a writing, as defined in Evidence Code § 250, and includes the original or a copy of handwriting, typewriting, printing, photostating, photographing, photocopying, transmitting by electronic mail or facsimile, and every other means of recording upon any tangible thing, any form of communication or representation, including letters, words, pictures, sounds, or symbols, or combinations thereof, and any record thereby created, regardless of the manner in which the record has been stored.\n“YOU” or “YOUR” refers to the plaintiff, his agents, his employees, his insurance companies, their agents, their employees, his attorneys, his accountants, his investigators, and anyone else acting on his behalf.\n“INCIDENT” refers to the circumstances and events described in Plaintiff’s complaint, which give rise to this action.\n“THIS LITIGATION,” “THIS LAWSUIT,” or the “COMPLAINT” all refer to the civil action described in the caption and on this pleading.\nREQUESTS FOR PRODUCTION\nREQUEST FOR PRODUCTION NO. :\n\tYOUR Driver’s License or Identification Card. \nREQUEST FOR PRODUCTION NO. :\n\tYOUR Social Security Card.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all diplomas or certificates of completion for any kind of education YOU have received since 1995.\nREQUEST FOR PRODUCTION NO. :\nAny and all DOCUMENTS related to any kind of occupational training YOU have received since 1995.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS related to any kind of vocational rehabilitation YOU have received since 1995.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS related to any kind of vocational rehabilitation consultation YOU have had since the date of the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS relating to or evidencing YOUR service in any branch of the United States military or the military service of any foreign country.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all employment records from any of YOUR past employers.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all employment records from YOUR current employer(s).\nREQUEST FOR PRODUCTION NO. :\nAny and all disciplinary records for any of YOUR employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\nAny and all weekly meeting logs for any of YOUR employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\nAny and all overtime records for any of YOUR employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\nAny and all incident reports for any of YOUR employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\nAny and all notices of discharge for any of your employers from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS related to employment agreements YOU have entered into since the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS concerning unemployment benefits YOU have sought from any entity, including federal or state governments or private companies, from 2007 to present (including but not limited to, unemployment compensation applications, correspondence, statement of benefits, acceptance or rejection of benefits, payments received, or other).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS concerning disability benefits YOU have sought from any entity, including federal or state governments or private companies, from 2007 to present (including but not limited to, disability compensation applications, correspondence, statement of benefits, acceptance or rejection of benefits, payments received, or other).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS showing any income you have earned from 2007 to present (including but not limited to, check stubs, W-2 Statements, or other).\nREQUEST FOR PRODUCTION NO. :\nAny and all DOCUMENTS showing any income YOU received from any source other than Defendant from 2007 to present (including but not limited to, every financial statement, profit and loss statement, earnings statement, or checking account register).\nREQUEST FOR PRODUCTION NO. :\n\tEvery state and/or federal tax return which YOU have filed from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\n\tYOUR most recent Social Security Earnings Statement.\nREQUEST FOR PRODUCTION NO. :\n\tYOUR Social Security Earnings Statement from the year prior to the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which support YOUR claim of lost earnings.\nREQUEST FOR PRODUCTION NO. :\nAny and all DOCUMENTS which support YOUR claim of loss of earning capacity.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS regarding YOUR membership in each and every union of which YOU were a member at the time of the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all union dispatch slips related to YOUR work for any employer from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\nAny and all union discharge slips related to YOUR work for any employer from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which depict the scene of the INCIDENT and/or any locations adjacent thereto (including but not limited to, photographs, videos, films, diagrams, charts or others).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which depict any object involved in the INCIDENT (including but not limited to, photographs, videos, films, or others).\nREQUEST FOR PRODUCTION NO. :\nAny and all DOCUMENTS which depict any injury involved in the INCIDENT (including but not limited to, photographs, videos, films, or others).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which contain a description of the INCIDENT, (including but not limited to, incident reports, doctor’s first report of work injury, or other).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which contain a written statement related in any way to the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which contain a recorded statement related in any way to the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which transcribe or record any oral statement made by any person describing the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS related to any and all medical or physical exams that YOU have undergone related to YOUR work with any employer from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which pertain to the diagnosis and treatment of the physical injuries which YOU contend that YOU sustained in the INCIDENT (including but not limited to, medical bills or statements, medical reports, other medical or hospital records, or other).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS reflecting YOUR fitness to work (fit-for-duty-status) since the date of the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS that support your claim for future medical expenses in connection with the INCIDENT giving rise to this LITIGATION.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS that support your claim for past medical expenses in connection with the INCIDENT giving rise to this LITIGATION.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS pertaining to medical treatment for the part(s) of YOUR body claimed to have been injured in the INCIDENT, whether or not such records were created before or after the date of the INCIDENT (including but not limited to, medical records, bills, reports, x-rays and other radiographic studies).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS prepared by YOU and pertaining in any way to YOUR injuries alleged in the COMPLAINT (including but not limited to, notes, diary entries, log book entries, calendar entries, daytimer/personal planner entries, or other).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS reflecting any item of damages claimed in the COMPLAINT (including but not limited to, bills, invoices, statements, or other).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS that support your claim for general damages in connection with the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which support YOUR claim of Defendant’s negligence as YOU allege in the COMPLAINT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all retirement DOCUMENTS that YOU submitted to any entity (including but not limited to, unions or employers).\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which describe any pension plans YOU have been enrolled in from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which describe any disaibility plans YOU have been enrolled in from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS which describe any retirement plans YOU have been enrolled in from 2007 to present.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all medical insurance policies that provide coverage for the injuries alleged in YOUR COMPLAINT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all physical evidence concerning the INCIDENT in your care, custody, or control.\nREQUEST FOR PRODUCTION NO. :\nAny and all DOCUMENTS that YOU contend support your claim of Jones Act Seaman status at the time of the INCIDENT.\nREQUEST FOR PRODUCTION NO. :\n\tAny and all DOCUMENTS that refer to or reflect YOUR recreational pursuits since the INCIDENT (including but not limited to, emails, tickets, and receipts related to vacation travel, concerts, sports venues, hobbies, or other).\nREQUEST FOR PRODUCTION NO. :\nAny and all DOCUMENTS and things you described in your response to Special INTERROGATORY NO. 27.\nREQUEST FOR PRODUCTION NO. :\nAny and all DOCUMENTS and things you described in your response to Special INTERROGATORY NO. 30.\nDated:  July 16, 2024\t\t\t\tORACLAIM PARTNERS LLP\nAttorneys for Defendant \nFAIRS ARE US, INC. \n     \tBy:_________________________________\n\t\t\t\t\t\t\tRobert T. Brown\n-10-\nDEFENDANT’S REQUEST FOR PRODUCTION OF DOCUMENTS TO PLAINTIFF, SET ONE', 'Document 24: Fake Status Report.docx\nSeptember 6, 2024\nPage 5\n[DATE]\nVIA [DELIVERY METHOD]\n[Client Name\nClient Address\nClient Email Address]\n\tRe:\tStatus Report\n\t\t[Case Name and Court Venue]\nDear [Mr./Ms. Last Name]:\nWe write to provide you with this status report and recommendations for further handling.  \n\nPROCEDURAL STATUS\n[A description of the critical procedural events that have taken place since the claim was first received and what procedural events are imminently on deck.]\nWORK PERFORMED\nTo date, we have done the following work to defend the claim:\n[List broad categories of work done to defend the claim.]\nFACTS\nCLAIMANT BACKGROUND\n[Discussion of who the claimant is and any background relevant to liability or damages.]\nCAUSE AND CIRCUMSTANCES OF INJURY OR LOSS\n[Discussion of the events that led to injury or damage, the scope of the injury or damage, and why it occurred.]\nINJURY AND TREATMENT / OR DAMAGE AND REPAIR\n[Discussion of the aftermath of the injury or damage and the measures taken to treat or repair it, including the key players, the time it took, what remains to be resolved, and the cost (if known).]\nPOST-INCIDENT DEVELOPMENTS\n[Discussion of events that have transpired since the incident and aftermath that may affect liability and damages.]\nMITIGATING FACTORS\n[Discussion of most salient facts that may reduce the exposure on the claim.]\nLIABIITY\nAPPLICABLE LEGAL STANDARDS FOR DETERMINING LIABILITY\n[Discuss applicable law to determine liability for the claim.]\nLIABILITY ANALYSIS\n[Discussion of the facts as applied to the legal standards – break out by subheadings that address the major points of legal analysis.]\nCOMPARATIVE FAULT\n[Discuss whether the claimant’s acts or omissions may qualify under applicable legal principles to reduce the claimant’s recovery by claimant’s own proportionate share of fault.]\nCONTRIBUTORY FAULT\n[Discuss whether any third party acts or omissions may qualify under applicable legal principles to reduce Client’s proportionate share of fault by such third party’s proportionate share of fault.]\nDAMAGES\nGENERAL DAMAGES\n[Discuss the range of pain and suffering here based on plaintiff’s verdict values for similar injuries under similar circumstances.]\nECONOMIC LOSS\n[Discuss claimant’s earning loss as a result of the incident and any alternative work the claimant might be able to do to mitigate his or her earnings loss.]\n\tC.  MEDICAL SPECIALS\n[Discuss the extent of past and future medical care required to treat the injury and deal with ongoing symptoms and the cost of the same not paid for by insurance available to the claimant.]\nVERDICT RANGE AND SETTLEMENT VALUE\n[Provide an estimated range of the verdict value based on similar verdicts with similar fact patterns, and estimate the settlement value based on strengths and weaknesses of the case.]\nRECOMMENDATIONS\n[Provide recommendations for next steps of further handling and request authorization for any significant foreseeable expenditures necessary to continue to defend the case.]\nSincerely,\n                                                          /s/ [Attorney Name]\n[Law firm name.]', "Document 25: Fake Witness Statements.docx\nWitness Statement: Jane Smith\nName: Jane Smith\nDate: January 23, 2024\nLocation: Sierra County Fairgrounds\nRelationship to Incident: Fair Attendee, witnessed the accident\nStatement:\nOn January 23, 2024, I was at the Sierra County Fair with my family. Around 3:00 PM, we were near the exit of the Tilt-A-Whirl ride when I saw a man trip and fall. He was with two young girls and a couple of other adults. The man, who I later learned was John Doe, tripped over a metal spike that was sticking out of the ground right in the exit path of the ride.\nWhen he fell, he immediately cried out in pain and couldn't get up. Several people, including what seemed to be fair staff, rushed over to help him. He was holding his left ankle and looked like he was in a lot of pain. The staff called for medical assistance, and an ambulance arrived shortly after to take him to the hospital.\nIt was clear that the spike was a hazard that shouldn't have been there, especially in such a busy area where people were exiting the ride.\nWitness Statement: Mark Johnson\nName: Mark Johnson\nDate: January 23, 2024\nLocation: Sierra County Fairgrounds\nRelationship to Incident: Co-worker of John Doe\nStatement:\nI was at the Sierra County Fair with John Doe and a few other co-workers. We were there to relax and have fun with our families. At around 3:00 PM, we were exiting the Tilt-A-Whirl ride. John was ahead of me with his two daughters. As we were walking out, I saw him suddenly trip and fall hard onto the ground.\nWhen I got closer, I saw that he had tripped over a metal spike that was sticking out of the ground in the exit path. John was in a lot of pain and couldn't move his left ankle. We called for help immediately, and some of the fair staff came over to assist. They called an ambulance, and John was taken to Sierra Vista Medical Center.\nJohn has been having a tough time since the accident. He can't work and is dealing with a lot of pain from his ankle injury and other issues like back spasms and neck strain. It's been really hard for him and his family.\nWitness Statement: Emily Brown\nName: Emily Brown\nDate: January 23, 2024\nLocation: Sierra County Fairgrounds\nRelationship to Incident: Fair Staff Member\nStatement:\nI am a staff member at the Sierra County Fair, and on January 23, 2024, I was working near the Tilt-A-Whirl ride. Around 3:00 PM, I saw a man, later identified as John Doe, trip and fall in the exit area of the ride. He was with his two daughters and some other adults.\nJohn tripped over a metal spike that was protruding from the ground. It was quite shocking because the spike was right in the middle of the exit path, which is heavily trafficked. He fell and immediately appeared to be in a lot of pain, holding his left ankle.\nI, along with a couple of other staff members, rushed over to help him. We called for an ambulance right away, and he was taken to Sierra Vista Medical Center. It was a very unfortunate incident, and it was clear that the spike posed a serious hazard to the fairgoers."]